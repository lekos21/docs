{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello, dear","text":""},{"location":"#hello-dear","title":"\ud83d\udc4b Hello, dear!","text":""},{"location":"#what-is-this","title":"What is this?","text":"<p>The Cheshire Cat is an open-source framework that allows you to develop intelligent agents on top of many Large Language Models (LLM). You can develop your custom AI architecture to assist you in a wide range of tasks.</p> <p>The Cheshire Cat embeds a long-term memory system to save the user's input locally and it answers knowing the context of previous conversations. You can also feed text documents in the Cat's memory system to enrich the agent's contextual information and ask it to retrieve them further in the conversation. The Cat currently supports <code>.txt</code>, <code>.pdf</code> and <code>.md</code> files.</p> <p>If you want the Cat to solve tailored tasks you can extend its capabilities writing Python plugins to execute custom functions or call external services (e.g. APIs and other models).</p> <p>If you want to build your custom AI architecture, the Cat can help you!</p> Cheshire Cat Features \ud83e\uddf0 Can use external tools \ud83d\udcdc Can ingest documents (.txt, .pdf, .md) \ud83c\udf0d Language model agnostic \ud83d\udc18 Long term memory \ud83d\ude80 Extendible via plugins in Python \ud83d\udc0b 100% dockerized"},{"location":"#get-started-now","title":"Get started now!","text":"<ul> <li>QuickStart</li> <li>Write a plugin</li> <li>Tutorial (coming soon)</li> </ul> Get in touch with us!  Discord \ud83d\udc48 Join our Discord server and \ud83d\udc47  don't forget to give the project a star! \u2b50 Thanks again!\ud83d\ude4f <pre><code>\"Would you tell me, please, which way I ought to go from here?\"\n\"That depends a good deal on where you want to get to,\" said the Cat.\n\"I don't much care where--\" said Alice.\n\"Then it doesn't matter which way you go,\" said the Cat.\n\n(Alice's Adventures in Wonderland - Lewis Carroll)\n</code></pre> <p>Credits</p> <p>Documentation images were generated with MidJourney, prompted by Edgars Romanovskis</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#faq","title":"\ud83d\ude4b\u200d\u2642\ufe0f FAQ","text":""},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#ive-found-the-cat-and-i-like-it-very-much-but-im-not-able-to-follow-your-instructions-to-install-it-on-my-machine-can-you-help","title":"I've found the Cat and I like it very much, but I'm not able to follow your instructions to install it on my machine. Can you help?","text":"<p>The Cheshire Cat is a framework to help developers to build vertical AIs: you will need some basic technical skills to follow our instructions. Please try to ask in the support channel in our discord server, and remember this is all volunteers effort: be kind! :)</p>"},{"location":"faq/#why-the-cat-does-not-default-to-some-open-llm-instead-of-chatgpt-or-gpt-3","title":"Why the Cat does not default to some open LLM instead of ChatGPT or GPT-3?","text":"<p>Our intention is not to depend on any specific LLM: the Cat does not have a preference about which LLM to use. Nonetheless, at the moment, OpenAI tools still provide the best results for your bucks. Decision is up to you.</p>"},{"location":"faq/#are-text-and-documents-sent-to-the-cat-safe-and-not-shared-with-anybody","title":"Are text and documents sent to the Cat safe and not shared with anybody?","text":"<p>Well, the local memory is safe and under your control, although embeddings and prompts are shared with your configured LLM, meaning you need to check how safe is the LLM. We plan to adopt local LLMs, at which point all your data will be under your control.</p>"},{"location":"faq/#basic-info","title":"Basic Info","text":""},{"location":"faq/#can-i-insert-a-long-article-into-the-chat","title":"Can I insert a long article into the chat?","text":"<p>Please avoid pasting long articles into the chat. Use Rabbit Hole to upload long texts instead: just click on the attachment icon in the chat input widget and upload your file.</p>"},{"location":"faq/#are-the-configured-llm-apis-used-to-instruct-the-cat-with-the-documents-im-going-to-upload","title":"Are the configured LLM APIs used to \"instruct\" the Cat with the documents I'm going to upload?","text":"<p>That's not exactly how it works: basically when you ask something to the Cat, we pass to the configured LLM a prompt with your actual question + data that can be useful to answer that question. Data can be parts of your documents or chat history. Please check our documentation for more details about how the Cat works for you.</p>"},{"location":"faq/#can-i-talk-to-the-cat-in-a-language-different-from-english","title":"Can I talk to the Cat in a language different from English?","text":"<p>Of course you can: just change the prompts in the Plugin folder accordingly, and take care not to mix languages to get best results.</p>"},{"location":"faq/#how-can-i-know-where-the-cat-gets-the-answers-id-like-to-know-if-its-using-the-files-i-uploaded-or-if-its-querying-the-configured-llm","title":"How can I know where the Cat gets the answers? I'd like to know if it's using the files I uploaded or if it's querying the configured LLM.","text":"<p>Just open the console in your browser to check the logs there. At some point soon, this information will end up in the user interface, but at the moment is behind the scenes.</p>"},{"location":"faq/#i-sent-to-the-cat-some-text-and-documents-i-wont-to-get-rid-of-how-can-i-do","title":"I sent to the Cat some text and documents I won't to get rid of, How can I do?","text":"<p>You can delete the <code>long_term_memory</code> folder and restart the Cat!</p>"},{"location":"faq/#errors","title":"Errors","text":""},{"location":"faq/#why-am-i-getting-the-error-ratelimiterror-in-my-browser-console","title":"Why am I getting the error <code>RateLimitError</code> in my browser console?","text":"<p>Please check if you have a valid credit card connected or if you have used up all the credits of your OpenAI trial period.</p>"},{"location":"faq/#everything-works-in-localhost-but-not-on-another-server","title":"Everything works in localhost but not on another server","text":"<p>You should configure ports in the <code>.env</code> file. Change according to your preferred host and ports:</p> <pre><code># Decide host and port for your Cat. Default will be localhost:1865\nCORE_HOST=anotherhost.com\nCORE_PORT=9000\n</code></pre>"},{"location":"faq/#docker-has-no-permissions-to-write","title":"Docker has no permissions to write","text":"<p>This is a matter with your docker installation or the user you run docker from.</p>"},{"location":"faq/#the-cat-seems-not-to-be-working-from-inside-a-virtual-machine","title":"The Cat seems not to be working from inside a Virtual Machine","text":"<p>In VirtualBox you can select Settings-&gt;Network, then choose NAT in the \"Attached to\" drop down menu. Select \"Advanced\" to configure the port forwarding rules. Assuming the guest IP of your VM is 10.0.2.15 (the default) and the ports configred in the .env files are the defaults, you have to set at least the following rule:</p> Rule name Protocol Host IP Host Port Guest IP Guest Port Rule 1 TCP 127.0.0.1 1865 10.0.2.15 1865 <p>If you want to work on the documentation of the Cat, you also have to add one rule for port 8000 which is used by <code>mkdocs</code>, and to configure <code>mkdocs</code> itself to respond to all requests (not only localhost as per the default).  </p>"},{"location":"faq/#customization","title":"Customization","text":""},{"location":"faq/#i-want-to-build-my-own-plugin-for-the-cat-what-should-i-know-about-licensing","title":"I want to build my own plugin for the Cat: what should I know about licensing?","text":"<p>Plugins are any license you wish, you can also sell them. The Cat core is GPL3, meaning you are free to fork and go on your own, but you are forced to open source changes to the core.</p>"},{"location":"faq/#port-1865-is-allowed-by-my-operating-system-andor-firewall","title":"Port 1865 is allowed by my operating system and/or firewall","text":"<p>Change the port as you wish in the <code>.env</code> file.</p> <pre><code># Decide host and port for your Cat. Default will be localhost:1865\nCORE_HOST=localhost\nCORE_PORT=9000\n</code></pre>"},{"location":"features/","title":"Features","text":"<p>TODO</p>"},{"location":"conceptual/llm/","title":"Language Models","text":""},{"location":"conceptual/llm/#language-models","title":"Language Models","text":"<p>A language model is a Deep Learning Neural Network trained on a huge amount of text data to perform different types of language tasks. Commonly, they are also referred to as Large Language Models (LLM). Language models comes in many architectures, size and specializations. The peculiarity of the Cheshire Cat is to be model-agnostic. This means it supports many different language models.</p> <p>By default, there are two classes of language models that tackles two different tasks.</p>"},{"location":"conceptual/llm/#completion-model","title":"Completion Model","text":"<p>This is the most known type of language models (see for examples ChatGPT, Cohere and many others). A completion model takes a string as input and generates a plausible answer by completion.</p> <p>Warning</p> <p>A LLM answer should not be accepted as-is, since LLM are subjected to hallucinations. Namely, their main goal is to generate plausible answers from the syntactical point of view. Thus, the provided answer could come from completely invented information.</p>"},{"location":"conceptual/llm/#embedding-model","title":"Embedding Model","text":"<p>This type of model takes a string as input and returns a vector as output. This is known as an embedding. Namely, this is a condensed representation of the input content. The output vector, indeed, embeds the semantic information of the input text.</p> <p>Despite being non-human readable, the embedding comes with the advantage of living in a Euclidean geometrical space. Hence, the embedding can be seen as a point in a multidimensional space, thus, geometrical operations can be applied to it. For instance, measuring the distance between two points can inform us about the similarity between two sentences.</p>"},{"location":"conceptual/llm/#language-models-flow","title":"Language Models flow","text":"<p>Developer documentation</p> <p>Language Models hooks</p> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/plugins/","title":"Plugins","text":""},{"location":"conceptual/plugins/#plugins","title":"Plugins","text":"<p>Plugins are add-ons that can be installed to extend and customize the Cheshire Cat. A plugin is nothing but a collection of hook and tools.</p>"},{"location":"conceptual/plugins/#hooks","title":"Hooks","text":"<p>The Cat uses functions knows as Hooks, which can be overridden, to customize the framework behavior in specific execution places. Hooks come with a priority property. The plugins manager takes care of collecting all the hooks, sorting and executing them in descending order of priority.</p>"},{"location":"conceptual/plugins/#tools","title":"Tools","text":"<p>Tools are custom Python functions that are called by the Agent. They come with a rich docstring upon with the Agent choose whether and which tool is the most suitable to fulfill the user's request. The list of available tools ends up in the Main Prompt, where the Agent receives instructions on how to structure its reasoning.</p> <p>Developer documentation</p> <ul> <li>How to write a plugin</li> <li>Hooks</li> <li>Tools</li> </ul>"},{"location":"conceptual/cheshire_cat/agent/","title":"The Agent","text":""},{"location":"conceptual/cheshire_cat/agent/#agent","title":"Agent","text":"<p>The Agent is the Cat's component that handles the tools execution. Sometimes a simple answer from the language model is not enough. For this reason, the Cat can exploit a set of custom tools coming from the plugins. The decision on whether and which action should be taken to fulfill the user's request is delegated to the Agent component.</p> <p>The Agent outlines a reasoning to take the aforementioned action. By default, the structure of the Agent's reasoning is defined in the instruction component of the Main Prompt.</p> <p>The list of available tools can be manually filtered with hooks to condition the Agent's decision.</p>"},{"location":"conceptual/cheshire_cat/agent/#agent-flow","title":"Agent flow","text":"<p>Developer documentation</p> <p>Agent hooks</p> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/cheshire_cat/mad_hatter/","title":"The Mad Hatter","text":""},{"location":"conceptual/cheshire_cat/mad_hatter/#mad-hatter","title":"Mad Hatter","text":"<p>The Mad Hatter is the Cat's plugins manager. It takes care of loading, prioritizing and executing plugins.</p> <p>Specifically, the Mad Hatter lists all available plugins in proper folder and sort their hooks in descending order of priority. When the Cat invokes them, it executes them following that order.</p> <p>Developer documentation</p> <ul> <li>How to write a plugin</li> <li>Hooks</li> <li>Tools</li> </ul>"},{"location":"conceptual/cheshire_cat/rabbit_hole/","title":"The Rabbit Hole","text":""},{"location":"conceptual/cheshire_cat/rabbit_hole/#rabbit-hole","title":"Rabbit Hole","text":"<p>The Rabbit Hole is the Cat's component that takes care of ingesting documents and storing them in the episodic memory. You can interact with it either through its endpoint, the GUI or a Python script.</p> <p>Currently supported file formats are: <code>.txt</code>, <code>.md</code>, <code>.pdf</code> or <code>.html</code> via web URL.</p>"},{"location":"conceptual/cheshire_cat/rabbit_hole/#rabbit-hole-flow","title":"Rabbit Hole flow","text":"<p>Developer documentation</p> <p>Rabbit Hole hooks</p> <pre><code>\nflowchart LR\nA[\"#128196;Document\"] --&gt; B[read];\nsubgraph rb [\"#128048;RabbitHole\"]\nB[read] --&gt; C[\"#129693;\"];\nC[\"#129693;\"] --&gt; D[recursive split];\nD[\"#129693;recursive split\"] --&gt; E[\"#129693;\"];\nE[\"#129693;\"] --&gt; F[\"#129693;summarization\"];\nF[\"#129693;summarization\"] --&gt; G[\"#129693;\"];\nend\nG[\"#129693;\"] --&gt; H[\"#128024;Episodic Memory\"] </code></pre> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/memory/long_term_memory/","title":"Long Term Memory","text":""},{"location":"conceptual/memory/long_term_memory/#long-term-memory","title":"Long Term Memory","text":"<p>The Cat's Long Term Memory (LTM) is made of two components:</p> <ul> <li>episodic memory, i.e. the context of things the user said in the past.</li> <li>declarative memory, i.e. the context of documents uploaded to the Cat;</li> </ul> <p>These are nothing but two vector databases where memories are stored in the form of vectors.</p> <p>You can interact with the LTM using its endpoint.</p> <p>By default, the Cat queries the LTM to retrieve the relevant context that is used to make up the Main Prompt.</p>"},{"location":"conceptual/memory/long_term_memory/#long-term-memory-flow","title":"Long Term Memory flow","text":"<p>Developer documentation</p> <p>Long Term Memory hooks</p> <pre><code>flowchart LR\n    subgraph LTM [\"#128024;Long Term Memory\"]\n            direction TB\n            C[(Episodic)];\n            D[(Declarative)];\n    end\n    A[Query] --&gt; LTM; \n    LTM --&gt; E[\"#129693;\"];\n    E[\"#129693;\"] --&gt; wm[\"#9881;#128024;Working Memory\"];</code></pre> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/memory/vector_memory/","title":"Vector Memory","text":""},{"location":"conceptual/memory/vector_memory/#vector-memory-collections","title":"Vector Memory Collections","text":"<p>The Vector Memory Collections are the lowest-level components of the Long Term Memory. These are particular databases that store the content in the form of geometrical vectors.</p> <p>A vector memory comes in the guise of a named collection of vectors and additional, optional metadata. The latter can be used to filter the search in the database. Each vector represents a memory. They are also called embeddings as they are the results of the text-to-vector conversion yielded by the embedder.</p> <p>Such databases are particularly useful because they allow to fetch relevant documents based on the vector similarity between a query and the stored embeddings.</p> <p>By default, Vector Memory Collections are created when the Cat is installed or after a complete memory swap.</p>"},{"location":"conceptual/memory/vector_memory/#vector-memory-collections-flow","title":"Vector Memory Collections flow","text":"<p>Developer documentation</p> <p>Vector Memory Collections hooks</p> <pre><code>flowchart LR\n    subgraph CAT [\"#128049;Cheshire Cat\"]\n        direction LR\n        subgraph LTM [\"#128024;Long Term Memory\"]\n            direction TB\n            C[(Episodic Memory)];\n            D[(Declarative Memory)];\n        end\n        A[\"First Memory\"] --&gt; H[\"#129693;\"];\n        H[\"#129693;\"] --&gt; C[(Episodic)];\n        H[\"#129693;\"] --&gt; D[(Declarative )];\n    end\n    E[First Installation] ----&gt; CAT;\n    F[Memory Swap] ----&gt; LTM;</code></pre> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/memory/working_memory/","title":"Working Memory","text":""},{"location":"conceptual/memory/working_memory/#working-memory","title":"Working Memory","text":"<p>The Working Memory is a handful component to store temporary data. For instance, it can be used to share data across plugins or, in general, across any function that get an instance of the Cat as an argument.</p> <p>By default, the Working Memory stores the chat history that ends up in the Main Prompt. Moreover, the Working Memory collects the relevant context that is fetched from the episodic and declarative memories in the Long Term Memory.</p>"},{"location":"conceptual/memory/working_memory/#working-memory-flow","title":"Working Memory flow","text":"<p>Developer documentation</p> <p>Long Term Memory hooks</p> <pre><code>flowchart LR\n    subgraph WM [\"#9881;#128024;Working Memory\"]\n            direction TB\n            CH[Chat History]\n            C[Episodic Memory];\n            D[Declarative Memory];\n    end\n    A[\"#128024;Long Term Memory\"] --&gt; E[\"#129693;\"]; \n    E[\"#129693;\"] --&gt; WM;\n    CH --&gt; main_prompt[Main Prompt];\n    C --&gt; main_prompt[Main Prompt];\n    D --&gt; main_prompt[Main Prompt];</code></pre> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/prompts/hyde/","title":"HyDE prompt","text":""},{"location":"conceptual/prompts/hyde/#hypothetical-document-embedding-prompt","title":"Hypothetical Document Embedding Prompt","text":"<p>Hypothetical Document Embedding (HyDE)1 is a technique used to improve accuracy during similarity search among documents.</p> <p>Similarity search is the task of comparing text embeddings, i.e. the vector representation of documents, to find similarities between them, e.g. during document question-answering.</p> <p>As such, HyDE technique consists in asking the language model to generate a hypothetical answer that is used as query for the similarity search task. The idea behind this approach is that, during a question-answering task, using the hypothetical answer to search for similar documents would lead to better results than using the question itself as a search query.</p> <p>Specifically, the Cat uses this method to retrieve the relevant memories that are provided as context and end up in the Main Prompt. Moreover, it exploits a technique named few shots learning2. Namely, a method that consists in providing the language model a few examples in the prompt to get more accurate answers.</p> <p>By default, the HyDE prompt is the following:</p> <pre><code>hyde_prompt = \"\"\"You will be given a sentence.\nIf the sentence is a question, convert it to a plausible answer. \nIf the sentence does not contain a question, \njust repeat the sentence as is without adding anything to it.\nExamples:\n- what furniture there is in my room? --&gt; In my room there is a bed, \na wardrobe and a desk with my computer\n- where did you go today --&gt; today I was at school\n- I like ice cream --&gt; I like ice cream\n- how old is Jack --&gt; Jack is 20 years old\nSentence:\n- {input} --&gt; \"\"\"\n</code></pre>"},{"location":"conceptual/prompts/hyde/#hyde-flow","title":"HyDE flow","text":"<p>Developer documentation</p> <ul> <li>HyDE hooks</li> <li>Prompts hooks</li> </ul> <pre><code>flowchart LR\n    subgraph cat [\"#128049;Cheshire Cat\"]\n    direction LR\n    hyde[\"#129693;HyDE prompt\"] --&gt; llm[Language Model];\n    llm --&gt;|generates|answer[Hypothetical Answer];\n    answer --&gt;|similarity search|ltm[\"#128024;Long Term Memory\"];\n    ltm --&gt; context[Relevant Context];\n    context ---&gt;|inserted in|prompt[Main Prompt];\n    end\n    A[\"#128100;User\"] ----&gt;|sends message|hyde;\n    A --&gt; prompt</code></pre> <p>Nodes with the \ud83e\ude9d; point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/prompts/hyde/#references","title":"References","text":"<ol> <li> <p>Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496.\u00a0\u21a9</p> </li> <li> <p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901.\u00a0\u21a9</p> </li> </ol>"},{"location":"conceptual/prompts/main_prompt/","title":"Main Prompt","text":""},{"location":"conceptual/prompts/main_prompt/#main-prompt","title":"Main Prompt","text":"<p>The Main Prompt is the full set of instructions that is fed to the Agent. For instance, the prompt can be engineered to instruct the Cat to behave in a specific manner or to use the memory and the tools.</p> <p>This prompt is split in three parts:</p> <ul> <li>a prefix;</li> <li>the instructions;</li> <li>a suffix.</li> </ul> <p>Using such a complex prompt is an approach known as Retrieval Augmented Generation1. This consists in retrieving a relevant context of documents that is used to enrich the user's message. Specifically, the Cat exploits the Hypothetical Document Embedding2 (HyDE) technique to recall the relevant context from the Long Term Memory and, indeed, augment the Main Prompt. This is also augmented with the history of the recent conversation, a set of tools and the history the Agent's reasoning.</p> <p>In the following sections, we explain every prompt component.</p>"},{"location":"conceptual/prompts/main_prompt/#prefix","title":"Prefix","text":"<p>This is the first component. By default, it is:</p> <pre><code>prefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny, concise and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human using tools and context.\n# Tools\"\"\"\n</code></pre> <p>The Prefix describes who the AI is and how it is expected to answer the Human. This component ends with \"# Tools\" because the next part of the prompt (generated form the Agent) contains the list of Tools.</p>"},{"location":"conceptual/prompts/main_prompt/#instructions","title":"Instructions","text":"<p>This is the set of instructions that explain the Agent how to format its reasoning. The Agent uses such chain of thoughts to decide when and which tool is the most appropriate to fulfill the user's needs.</p> <p>By default, it is:</p> <pre><code>instructions = \"\"\"To use a tool, use the following format:\n\\```\nThought: Do I need to use a tool? Yes\nAction: the action to take /* should be one of [{tool_names}] */\nAction Input: the input to the action\nObservation: the result of the action\n\\```\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\\```\nThought: Do I need to use a tool? No\n{ai_prefix}: [your response here]\n\\```\"\"\"\n</code></pre> <p>where the placeholder <code>{tool_names}</code> is replaced with the list of the available Python tools.</p>"},{"location":"conceptual/prompts/main_prompt/#suffix","title":"Suffix","text":"<p>This is the last component of the Main Prompt and, by default, is set as follows:</p> <pre><code>suffix = \"\"\"# Context\n## Context of things the Human said in the past:{episodic_memory}\n## Context of documents containing relevant information:{declarative_memory}\n## Conversation until now:{chat_history}\n - Human: {input}\n# What would the AI reply?\n{agent_scratchpad}\"\"\"\n</code></pre> <p>The purpose of this component is to provide the Agent with the context documents retrieved from the episodic and declarative memories, the recent conversation and the agent scratchpad, i.e. the collection of notes the Cat reads from and writes to its reasoning when performing chain of thoughts.</p>"},{"location":"conceptual/prompts/main_prompt/#main-prompt-flow","title":"Main Prompt flow","text":"<p>Developer documentation</p> <p>Main Prompt hooks</p> <pre><code>flowchart LR\n    subgraph MP [\"Main Prompt\"]\n%%        direction LR\n        Prefix[\"#129693;Prefix\"];\n        Instructions[\"#129693;Instructions\"];\n        Suffix[\"#129693;Suffix\"];    \n    end\n    subgraph CAT [\"#128049;Cheshire Cat\"]\n        HyDE\n        subgraph LTM [\"#128024;Long Term Memory\"]\n%%        direction\n        C[(Episodic)];\n        D[(Declarative)];\n    end\n    subgraph Agent [\"#129302;Agent\"]\n        A[Agent Scratchpad];\n    end\n    end\n\n    U[\"#128100;User\"] --&gt;|sends message|HyDE ---&gt; LTM[\"#128024;Long Term Memory\"];\n    C --&gt; E[\"#129693;\"] ----&gt; Prefix;\n    D --&gt; E[\"#129693;\"] --&gt; Prefix;\n    A --&gt; Suffix;\n    MP -..-&gt;|fed back to|CAT -...-&gt; Answer</code></pre> <p>Nodes with the \ud83e\ude9d point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"conceptual/prompts/main_prompt/#references","title":"References","text":"<ol> <li> <p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., ... &amp; Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.\u00a0\u21a9</p> </li> <li> <p>Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496.\u00a0\u21a9</p> </li> </ol>"},{"location":"conceptual/prompts/summarization/","title":"Summarization Prompt","text":""},{"location":"conceptual/prompts/summarization/#summarization-prompt","title":"Summarization Prompt","text":"<p>The Summarization Prompt is nothing more than the instruction to ask the Agent to summarize a document. This step is borne by the Rabbit Hole when storing documents in the episodic memory.</p> <p>This is an iterative process: a document is split in chunks; each chunk is grouped and summarized iteratively until only one summary remains.</p> <p>By default, the Summarization Prompt is the following:</p> <pre><code>summarization_prompt = \"\"\"Write a concise summary of the following:\n{text}\n\"\"\"\n</code></pre>"},{"location":"conceptual/prompts/summarization/#summarization-flow","title":"Summarization flow","text":"<p>!!!! note \"Developer documentation\"     Summarization hooks Rabbit Hole hooks</p> <pre><code>\nflowchart LR\nA[\"#128196;Document\"] --&gt; B[read];\nsubgraph rb [\"#128048;RabbitHole\"]\nB[read] --&gt; C[\"#129693;\"];\nC[\"#129693;\"] --&gt; D[recursive split];\nD[\"#129693;recursive split\"] --&gt; E[\"#129693;\"];\nE[\"#129693;\"] --&gt; F[\"#129693;summarization\"];\nF[\"#129693;summarization\"] --&gt; G[\"#129693;\"];\nend\nG[\"#129693;\"] --&gt; H[\"#128024;Episodic Memory\"] </code></pre> <p>Nodes with the \ud83e\ude9d; point the execution places where there is an available hook to customize the execution pipeline.</p>"},{"location":"technical/advanced/","title":"Advanced","text":""},{"location":"technical/advanced/#advanced","title":"\ud83d\udc08 Advanced","text":""},{"location":"technical/advanced/#api-authentication","title":"\ud83d\udd10 API Authentication","text":"<p>In order to authenticate endpoints, it is necessary to include the <code>API_KEY=your-key-here</code> variable in the <code>.env</code> file. Multiple keys can be accepted by separating them with a pipe (<code>|</code>) as follows: <code>API_KEY=your-key-here|secondary_client_key</code>.</p> <p>After configuration, all endpoints will require an <code>access_token</code> header for authentication, such as <code>access_token: your-key-here</code>. Failure to provide the correct access token will result in a 403 error.</p> <p>Warning</p> <p>This kind of athentication is weak and it's intended for machine to machine communication, please do not rely on it and enforce other kind of stronger authentication such as OAuth2 for the client side.</p> <p>Example</p> <p>Authenticated API call:</p> PythonNode <pre><code>import requests\nserver_url = 'http://localhost:1865/'\napi_key = 'your-key-here'\naccess_token = {'access_token': api_key}\nresponse = requests.get(server_url, headers=access_token)\nif response.status_code == 200:\nprint(response.text)\nelse:\nprint('Error occurred: {}'.format(response.status_code))\n</code></pre> <pre><code>const request = require('request');\nconst serverUrl = 'http://localhost:1865/';\nconst apiKey = 'your-key-here';\nconst access_token = {'access_token': apiKey};\nrequest({url: serverUrl, headers: access_token}, (error, response, body) =&gt; {\nif (error) {\nconsole.error(error);\n} else {\nif (response.statusCode === 200) {\nconsole.log(body);\n} else {\nconsole.error(`Error occurred: ${response.statusCode}`);\n}\n}\n});\n</code></pre> <p>By adding the variable to the <code>.env</code> file, all Swagger endpoints (<code>localhost:1865/docs</code>) will require authentication and can be accessed on the top right-hand corner of the page through the green Authorize button.</p>"},{"location":"technical/getting-started/","title":"Getting Started","text":""},{"location":"technical/getting-started/#getting-started","title":"\ud83d\ude80 Getting started","text":""},{"location":"technical/getting-started/#download","title":"Download","text":"<p>Clone the repository on your machine:</p> <pre><code># Clone the repository\ngit clone https://github.com/cheshire-cat-ai/core.git cheshire-cat\n</code></pre>"},{"location":"technical/getting-started/#install","title":"Install","text":"<p>To run the Cheshire Cat, you need to have <code>docker</code> (instructions) and <code>docker-compose</code> (instructions) installed on your system.</p> <ul> <li>Create and API key on the language model provider website  </li> <li>Make a copy of the <code>.env.example</code> file and rename it <code>.env</code></li> <li>Start the app with <code>docker-compose up</code> inside the repository</li> <li>Open the app in your browser at <code>localhost:1865/admin</code></li> <li>Configure a LLM in the <code>Settings</code> tab and paste you API key</li> <li>Start chatting</li> </ul> <p>You can also interact via REST API and try out the endpoints on <code>localhost:1865/docs</code></p> <p>The first time you run the <code>docker-compose up</code> command it will take several minutes as docker images occupy some GBs.</p>"},{"location":"technical/getting-started/#quickstart","title":"Quickstart","text":"<p>Here is an example of a quick setup running the <code>gpt3.5-turbo</code> OpenAI model.  </p> <p>Create an API key with <code>+ Create new secret key</code> in your OpenAI personal account, then:</p>"},{"location":"technical/getting-started/#cli-setup","title":"CLI setup","text":"Linux &amp; MacWindows <pre><code># Open the cloned repository\ncd cheshire-cat\n\n# Create the .env file\ncp .env.example .env\n\n# Run docker containers\ndocker-compose up\n</code></pre> <pre><code># Open the cloned repository\ncd cheshire-cat\n\n# Create the .env file\ncopy .env.example .env\n\n# Run docker containers\ndocker-compose up\n</code></pre>"},{"location":"technical/getting-started/#gui-setup","title":"GUI setup","text":"<p>When you're done using the Cat, remember to CTRL+c in the terminal and <code>docker-compose down</code>.</p>"},{"location":"technical/getting-started/#update","title":"Update","text":"<p>As the project is still a work in progress, if you want to update it run the following:</p> <pre><code># Open the cloned repository\ncd cheshire-cat\n\n# Pull from the main remote repository\ngit pull\n\n# Build again the docker containers\ndocker-compose build --no-cache\n\n# Remove dangling images (optional)\ndocker rmi -f $(docker images -f \"dangling=true\" -q)\n# Run docker containers\ndocker-compose up\n</code></pre>"},{"location":"technical/how-the-cat-works/","title":"How the Cat works","text":""},{"location":"technical/how-the-cat-works/#how-the-cat-works","title":"\ud83d\ude3c How the Cat works","text":""},{"location":"technical/how-the-cat-works/#components","title":"Components","text":"<p>The Cheshire Cat is made of many pluggable components that make it fully customizable.</p> \ud83d\udcac <code>Chat</code> This is the Graphical User Interface (GUI) component that allows you to interact directly with the Cat. From the GUI, you can also set the language model you want the Cat to run. \ud83d\udc30 <code>Rabbit Hole</code> This component handles the ingestion of documents. Files that are sent down the Rabbit Hole are split into chunks and saved in the Cat's declarative memory to be further retrieved in the conversation.  \ud83e\udde0 <code>Large Language Model (LLM)</code> This is one of the core components of the Cheshire Cat framework. A LLM is a Deep Learning model that's been trained on a huge volume of text data and can perform many types of language tasks. The model takes a text string as input (e.g. the user's prompt) and provides a meaningful answer. The answer consistency and adequacy is enriched with the context of previous conversations and documents uploaded in the Cat's memory. \ud83e\uddec <code>Embedder</code> The embedder is another Deep Learning model similar to the LLM. Differently, it doesn't perform language tasks. The model takes a text string as input and encodes it in a numerical representation. This operation allows to represent textual data as vectors and perform geometrical operation on them. For instance, given an input, the embedder is used to retrieve similar sentences from the Cat's memory. \ud83d\udc18 <code>Vector Memory</code> As a result of the Embedder encoding, we get a set of vectors that are used to store the Cat's memory in a vector database. Memories store not only the vector representation of the input, but also the time instant and personalized metadata to facilitate and enhance the information retrieval. The Cat embeds two types of vector memories, namely the episodic and declarative memories. The formers are the things the human said in the past; the latter the documents sent down the Rabbit hole.   \ud83e\udd16 <code>Agent</code> This is another core component of the Cheshire Cat framework. The agent orchestrates the calls that are made to the LLM. This component allows the Cat to decide which action to take according to the input the user provides. Possible actions range from holding the conversation to executing complex tasks, chaining predefined or custom tools. \ud83e\udde9 <code>Plugins</code> These are functions to extend the Cat's capabilities. Plugins are a set of tools and hooks that allow the Agent to achieve complex goals. This component let the Cat assists you with tailored needs."},{"location":"technical/how-the-cat-works/#main-loop","title":"Main loop","text":""},{"location":"technical/how-the-cat-works/#retrieval-augmented-generation-docs-qa","title":"Retrieval augmented Generation (docs Q&amp;A)","text":""},{"location":"technical/http_reference/","title":"HTTP Reference","text":""},{"location":"technical/http_reference/#http-reference","title":"HTTP Reference","text":"<p>Explore and try the endpoints easily by starting your Cat and opening the <code>/docs</code> endpoint. A static documentation of the endpoints is also available under <code>/redoc</code></p>"},{"location":"technical/API_Documentation/SUMMARY/","title":"SUMMARY","text":"<ul> <li>api_auth</li> <li>db<ul> <li>crud</li> <li>database</li> <li>models</li> </ul> </li> <li>factory<ul> <li>custom_llm</li> <li>embedder</li> <li>llm</li> </ul> </li> <li>log</li> <li>looking_glass<ul> <li>agent_manager</li> <li>cheshire_cat</li> <li>output_parser</li> <li>prompts</li> </ul> </li> <li>mad_hatter<ul> <li>core_plugin<ul> <li>hooks<ul> <li>agent</li> <li>flow</li> <li>memory</li> <li>models</li> <li>prompt</li> <li>rabbithole</li> </ul> </li> <li>tools</li> </ul> </li> <li>decorators</li> <li>mad_hatter</li> </ul> </li> <li>main</li> <li>memory<ul> <li>long_term_memory</li> <li>vector_memory</li> <li>working_memory</li> </ul> </li> <li>rabbit_hole</li> <li>routes<ul> <li>base</li> <li>memory</li> <li>openapi</li> <li>plugins</li> <li>setting<ul> <li>embedder_setting</li> <li>general_setting</li> <li>llm_setting</li> <li>prompt_setting</li> <li>setting_utils</li> </ul> </li> <li>static<ul> <li>admin</li> <li>auth_static</li> <li>public</li> <li>static</li> </ul> </li> <li>upload</li> <li>websocket</li> </ul> </li> <li>utils</li> </ul>"},{"location":"technical/API_Documentation/api_auth/","title":"api_auth","text":""},{"location":"technical/API_Documentation/api_auth/#cat.api_auth.API_KEY","title":"<code>API_KEY = [key.strip() for key in os.getenv('API_KEY', '').split('|') if key.strip()]</code>  <code>module-attribute</code>","text":"<p>List[str]: list of piped API keys.</p> <p>The list stores all the API keys set in the <code>.env</code> file. The keys are piped with a <code>|</code>, hence the list takes care of splitting and storing them.</p>"},{"location":"technical/API_Documentation/api_auth/#cat.api_auth.check_api_key","title":"<code>check_api_key(api_key=Security(api_key_header))</code>","text":"<p>Authenticate endpoint.</p> <p>Check the provided key is available in API keys list.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>API keys to be checked.</p> <code>Security(api_key_header)</code> <p>Returns:</p> Name Type Description <code>api_key</code> <code>str | None</code> <p>Returns the valid key if set in the <code>.env</code>, otherwise return None.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>Error with status code <code>403</code> if the provided key is not valid.</p> Source code in <code>cat/api_auth.py</code> <pre><code>def check_api_key(api_key: str = Security(api_key_header)) -&gt; None | str:\n\"\"\"Authenticate endpoint.\n    Check the provided key is available in API keys list.\n    Parameters\n    ----------\n    api_key : str\n        API keys to be checked.\n    Returns\n    -------\n    api_key : str | None\n        Returns the valid key if set in the `.env`, otherwise return None.\n    Raises\n    ------\n    HTTPException\n        Error with status code `403` if the provided key is not valid.\n    \"\"\"\nif not API_KEY:\nreturn None\nif api_key in API_KEY:\nreturn api_key\nelse:\nraise HTTPException(status_code=403, detail=\"Invalid API Key\")\n</code></pre>"},{"location":"technical/API_Documentation/log/","title":"log","text":"<p>The log engine.</p>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine","title":"<code>CatLogEnine</code>","text":"<p>The log engine.</p> <p>Engine to filter the logs in the terminal according to the level of severity.</p> <p>Attributes:</p> Name Type Description <code>LOG_LEVEL</code> <code>str</code> <p>Level of logging set in the <code>.env</code> file.</p>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine--notes","title":"Notes","text":"<p>The logging level set in the <code>.env</code> file will print all the logs from that level to above. Available levels are:</p> <pre><code>- `DEBUG`\n- `INFO`\n- `WARNING`\n- `ERROR`\n- `CRITICAL`\n</code></pre> <p>Default to <code>WARNING</code>.</p> Source code in <code>cat/log.py</code> <pre><code>class CatLogEnine:\n\"\"\"The log engine.\n    Engine to filter the logs in the terminal according to the level of severity.\n    Attributes\n    ----------\n    LOG_LEVEL : str\n        Level of logging set in the `.env` file.\n    Notes\n    -----\n    The logging level set in the `.env` file will print all the logs from that level to above.\n    Available levels are:\n        - `DEBUG`\n        - `INFO`\n        - `WARNING`\n        - `ERROR`\n        - `CRITICAL`\n    Default to `WARNING`.\n    \"\"\"\ndef __init__(self):\nself.LOG_LEVEL = get_log_level()\nself.default_log()\n# workaround for pdfminer logging\n# https://github.com/pdfminer/pdfminer.six/issues/347\nlogging.getLogger(\"pdfminer\").setLevel(logging.WARNING)\ndef show_log_level(self, record):\n\"\"\"Allows to show stuff in the log based on the global setting.\n        Parameters\n        ----------\n        record : dict\n        Returns\n        -------\n        bool\n        \"\"\"\nreturn record[\"level\"].no &gt;= logger.level(self.LOG_LEVEL).no\ndef default_log(self):\n\"\"\"Set the same debug level to all the project dependencies.\n        Returns\n        -------\n        \"\"\"\nlog_format = \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt; &lt;level&gt;{level: &lt;6}&lt;/level&gt; &lt;cyan&gt;{name}.py&lt;/cyan&gt; &lt;cyan&gt;{line}&lt;/cyan&gt; =&gt; &lt;level&gt;{message}&lt;/level&gt;\"\nlogger.remove()\nif self.LOG_LEVEL == \"DEBUG\":\nreturn logger.add(\nsys.stdout, colorize=True, format=log_format, backtrace=True, diagnose=True, filter=self.show_log_level\n)\nelse:\nreturn logger.add(sys.stdout, colorize=True, format=log_format, filter=self.show_log_level)\ndef get_caller_info(self, skip=3):\n\"\"\"Get the name of a caller in the format module.class.method.\n        Copied from: https://gist.github.com/techtonik/2151727\n        Parameters\n        ----------\n        skip :  int\n            Specifies how many levels of stack to skip while getting caller name.\n        Returns\n        -------\n        package : str\n            Caller package.\n        module : str\n            Caller module.\n        klass : str\n            Caller classname if one otherwise None.\n        caller : str\n            Caller function or method (if a class exist).\n        line : int\n            The line of the call.\n        Notes\n        -----\n        skip=1 means \"who calls me\",\n        skip=2 \"who calls my caller\" etc.\n        An empty string is returned if skipped levels exceed stack height.\n        \"\"\"\nstack = inspect.stack()\nstart = 0 + skip\nif len(stack) &lt; start + 1:\nreturn \"\"\nparentframe = stack[start][0]\n# module and packagename.\nmodule_info = inspect.getmodule(parentframe)\nif module_info:\nmod = module_info.__name__.split(\".\")\npackage = mod[0]\nmodule = mod[1]\n# class name.\nklass = None\nif \"self\" in parentframe.f_locals:\nklass = parentframe.f_locals[\"self\"].__class__.__name__\n# method or function name.\ncaller = None\nif parentframe.f_code.co_name != \"&lt;module&gt;\":  # top level usually\ncaller = parentframe.f_code.co_name\n# call line.\nline = parentframe.f_lineno\n# Remove reference to frame\n# See: https://docs.python.org/3/library/inspect.html#the-interpreter-stack\ndel parentframe\nreturn package, module, klass, caller, line\ndef log(self, msg, level=\"DEBUG\"):\n\"\"\"Add to log based on settings.\n        Parameters\n        ----------\n        msg :\n            Message to be logged.\n        level : str\n            Logging level.\"\"\"\nglobal logger\nlogger.remove()\n# Add real caller for the log\n(package, module, klass, caller, line) = self.get_caller_info()\ncontext = {\n\"original_name\": f\"{package}.{module}\",\n\"original_line\": line,\n\"original_class\": klass,\n\"original_caller\": caller,\n}\nlog_format = \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt; &lt;level&gt;{level: &lt;6}&lt;/level&gt; &lt;cyan&gt;{extra[original_name]}.py&lt;/cyan&gt; &lt;cyan&gt;{extra[original_line]} ({extra[original_class]}.{extra[original_caller]})&lt;/cyan&gt; =&gt; &lt;level&gt;{message}&lt;/level&gt;\"\n_logger = logger\nmsg_body = pformat(msg)\nlines = msg_body.splitlines()\n# On debug level print the traceback better\nif self.LOG_LEVEL == \"DEBUG\":\nif type(msg) is str and not msg.startswith(\"&gt; \"):\ntraceback_log_format = \"&lt;yellow&gt;{extra[traceback]}&lt;/yellow&gt;\"\nstack = \"\"\n_logger.add(\nsys.stdout,\ncolorize=True,\nformat=traceback_log_format,\nbacktrace=True,\ndiagnose=True,\nfilter=self.show_log_level,\n)\nframes = takewhile(lambda f: \"/loguru/\" not in f.filename, traceback.extract_stack())\nfor f in frames:\nif f.filename.startswith(\"/app/./cat\"):\nfilename = f.filename.replace(\"/app/./cat\", \"\")\nif not filename.startswith(\"/log.py\"):\nstack = \"&gt; \" + \"\".join(\"{}:{}:{}\".format(filename, f.name, f.lineno))\ncontext[\"traceback\"] = stack\n_logger.bind(**context).log(level, \"\")\nlogger.remove()\n_logger.add(\nsys.stdout, colorize=True, format=log_format, backtrace=True, diagnose=True, filter=self.show_log_level\n)\nfor line in lines:\nline = line.strip().replace(\"\\\\n\", \"\")\nif line != \"\":\n_logger.bind(**context).log(level, f\"{line}\")\n# After our custom log we need to set again the logger as default for the other dependencies\nself.default_log()\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine.default_log","title":"<code>default_log()</code>","text":"<p>Set the same debug level to all the project dependencies.</p> Source code in <code>cat/log.py</code> <pre><code>def default_log(self):\n\"\"\"Set the same debug level to all the project dependencies.\n    Returns\n    -------\n    \"\"\"\nlog_format = \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt; &lt;level&gt;{level: &lt;6}&lt;/level&gt; &lt;cyan&gt;{name}.py&lt;/cyan&gt; &lt;cyan&gt;{line}&lt;/cyan&gt; =&gt; &lt;level&gt;{message}&lt;/level&gt;\"\nlogger.remove()\nif self.LOG_LEVEL == \"DEBUG\":\nreturn logger.add(\nsys.stdout, colorize=True, format=log_format, backtrace=True, diagnose=True, filter=self.show_log_level\n)\nelse:\nreturn logger.add(sys.stdout, colorize=True, format=log_format, filter=self.show_log_level)\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine.get_caller_info","title":"<code>get_caller_info(skip=3)</code>","text":"<p>Get the name of a caller in the format module.class.method.</p> <p>Copied from: https://gist.github.com/techtonik/2151727</p> <p>Parameters:</p> Name Type Description Default <code>skip</code> <code> int</code> <p>Specifies how many levels of stack to skip while getting caller name.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>package</code> <code>str</code> <p>Caller package.</p> <code>module</code> <code>str</code> <p>Caller module.</p> <code>klass</code> <code>str</code> <p>Caller classname if one otherwise None.</p> <code>caller</code> <code>str</code> <p>Caller function or method (if a class exist).</p> <code>line</code> <code>int</code> <p>The line of the call.</p>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine.get_caller_info--notes","title":"Notes","text":"<p>skip=1 means \"who calls me\", skip=2 \"who calls my caller\" etc.</p> <p>An empty string is returned if skipped levels exceed stack height.</p> Source code in <code>cat/log.py</code> <pre><code>def get_caller_info(self, skip=3):\n\"\"\"Get the name of a caller in the format module.class.method.\n    Copied from: https://gist.github.com/techtonik/2151727\n    Parameters\n    ----------\n    skip :  int\n        Specifies how many levels of stack to skip while getting caller name.\n    Returns\n    -------\n    package : str\n        Caller package.\n    module : str\n        Caller module.\n    klass : str\n        Caller classname if one otherwise None.\n    caller : str\n        Caller function or method (if a class exist).\n    line : int\n        The line of the call.\n    Notes\n    -----\n    skip=1 means \"who calls me\",\n    skip=2 \"who calls my caller\" etc.\n    An empty string is returned if skipped levels exceed stack height.\n    \"\"\"\nstack = inspect.stack()\nstart = 0 + skip\nif len(stack) &lt; start + 1:\nreturn \"\"\nparentframe = stack[start][0]\n# module and packagename.\nmodule_info = inspect.getmodule(parentframe)\nif module_info:\nmod = module_info.__name__.split(\".\")\npackage = mod[0]\nmodule = mod[1]\n# class name.\nklass = None\nif \"self\" in parentframe.f_locals:\nklass = parentframe.f_locals[\"self\"].__class__.__name__\n# method or function name.\ncaller = None\nif parentframe.f_code.co_name != \"&lt;module&gt;\":  # top level usually\ncaller = parentframe.f_code.co_name\n# call line.\nline = parentframe.f_lineno\n# Remove reference to frame\n# See: https://docs.python.org/3/library/inspect.html#the-interpreter-stack\ndel parentframe\nreturn package, module, klass, caller, line\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine.log","title":"<code>log(msg, level='DEBUG')</code>","text":"<p>Add to log based on settings.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <p>Message to be logged.</p> required <code>level</code> <code>str</code> <p>Logging level.</p> <code>'DEBUG'</code> Source code in <code>cat/log.py</code> <pre><code>def log(self, msg, level=\"DEBUG\"):\n\"\"\"Add to log based on settings.\n    Parameters\n    ----------\n    msg :\n        Message to be logged.\n    level : str\n        Logging level.\"\"\"\nglobal logger\nlogger.remove()\n# Add real caller for the log\n(package, module, klass, caller, line) = self.get_caller_info()\ncontext = {\n\"original_name\": f\"{package}.{module}\",\n\"original_line\": line,\n\"original_class\": klass,\n\"original_caller\": caller,\n}\nlog_format = \"&lt;green&gt;[{time:YYYY-MM-DD HH:mm:ss.SSS}]&lt;/green&gt; &lt;level&gt;{level: &lt;6}&lt;/level&gt; &lt;cyan&gt;{extra[original_name]}.py&lt;/cyan&gt; &lt;cyan&gt;{extra[original_line]} ({extra[original_class]}.{extra[original_caller]})&lt;/cyan&gt; =&gt; &lt;level&gt;{message}&lt;/level&gt;\"\n_logger = logger\nmsg_body = pformat(msg)\nlines = msg_body.splitlines()\n# On debug level print the traceback better\nif self.LOG_LEVEL == \"DEBUG\":\nif type(msg) is str and not msg.startswith(\"&gt; \"):\ntraceback_log_format = \"&lt;yellow&gt;{extra[traceback]}&lt;/yellow&gt;\"\nstack = \"\"\n_logger.add(\nsys.stdout,\ncolorize=True,\nformat=traceback_log_format,\nbacktrace=True,\ndiagnose=True,\nfilter=self.show_log_level,\n)\nframes = takewhile(lambda f: \"/loguru/\" not in f.filename, traceback.extract_stack())\nfor f in frames:\nif f.filename.startswith(\"/app/./cat\"):\nfilename = f.filename.replace(\"/app/./cat\", \"\")\nif not filename.startswith(\"/log.py\"):\nstack = \"&gt; \" + \"\".join(\"{}:{}:{}\".format(filename, f.name, f.lineno))\ncontext[\"traceback\"] = stack\n_logger.bind(**context).log(level, \"\")\nlogger.remove()\n_logger.add(\nsys.stdout, colorize=True, format=log_format, backtrace=True, diagnose=True, filter=self.show_log_level\n)\nfor line in lines:\nline = line.strip().replace(\"\\\\n\", \"\")\nif line != \"\":\n_logger.bind(**context).log(level, f\"{line}\")\n# After our custom log we need to set again the logger as default for the other dependencies\nself.default_log()\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.CatLogEnine.show_log_level","title":"<code>show_log_level(record)</code>","text":"<p>Allows to show stuff in the log based on the global setting.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>dict</code> required <p>Returns:</p> Type Description <code>bool</code> Source code in <code>cat/log.py</code> <pre><code>def show_log_level(self, record):\n\"\"\"Allows to show stuff in the log based on the global setting.\n    Parameters\n    ----------\n    record : dict\n    Returns\n    -------\n    bool\n    \"\"\"\nreturn record[\"level\"].no &gt;= logger.level(self.LOG_LEVEL).no\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.get_log_level","title":"<code>get_log_level()</code>","text":"<p>Return the global LOG level.</p> Source code in <code>cat/log.py</code> <pre><code>def get_log_level():\n\"\"\"Return the global LOG level.\"\"\"\nreturn os.getenv(\"LOG_LEVEL\", \"ERROR\")\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.log","title":"<code>log(msg, level='DEBUG')</code>","text":"<p>Create function wrapper to class.</p> <p>Parameters:</p> Name Type Description Default <code>msg</code> <code>str</code> <p>Message to be logged.</p> required <code>level</code> <code>str</code> <p>Logging level.</p> <code>'DEBUG'</code> Source code in <code>cat/log.py</code> <pre><code>def log(msg, level=\"DEBUG\"):\n\"\"\"Create function wrapper to class.\n    Parameters\n    ----------\n    msg : str\n        Message to be logged.\n    level : str\n        Logging level.\n    Returns\n    -------\n    \"\"\"\nglobal logEngine\nreturn logEngine.log(msg, level)\n</code></pre>"},{"location":"technical/API_Documentation/log/#cat.log.welcome","title":"<code>welcome()</code>","text":"<p>Welcome message in the terminal.</p> Source code in <code>cat/log.py</code> <pre><code>def welcome():\n\"\"\"Welcome message in the terminal.\"\"\"\nsecure = os.getenv('CORE_USE_SECURE_PROTOCOLS', '')\nif secure != '':\nsecure = 's'\ncat_address = f'http{secure}://{os.environ[\"CORE_HOST\"]}:{os.environ[\"CORE_PORT\"]}'\nwith open(\"cat/welcome.txt\", 'r') as f:\nprint(f.read())\nprint('\\n=============== ^._.^ ===============\\n')\nprint(f'Cat REST API:\\t{cat_address}/docs')\nprint(f'Cat PUBLIC:\\t{cat_address}/public')\nprint(f'Cat ADMIN:\\t{cat_address}/admin\\n')\nprint('======================================')\n</code></pre>"},{"location":"technical/API_Documentation/main/","title":"main","text":""},{"location":"technical/API_Documentation/rabbit_hole/","title":"rabbit_hole","text":""},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole","title":"<code>RabbitHole</code>","text":"Source code in <code>cat/rabbit_hole.py</code> <pre><code>class RabbitHole:\n\"\"\"\n    \"\"\"\ndef __init__(self, cat):\nself.cat = cat\ndef ingest_file(\nself,\nfile: Union[str, UploadFile],\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n):\n\"\"\"Load a file in the Cat's declarative memory.\n        The method splits and converts the file in Langchain `Document`. Then, it stores the `Document` in the Cat's\n        memory. Optionally, the document can be summarized and summaries are saved along with the original\n        content.\n        Parameters\n        ----------\n        file : str, UploadFile\n            The file can be a path passed as a string or an `UploadFile` object if the document is ingested using the\n            `rabbithole` endpoint.\n        chunk_size : int\n            Number of characters in each document chunk.\n        chunk_overlap : int\n            Number of overlapping characters between consecutive chunks.\n        Notes\n        ----------\n        Currently supported formats are `.txt`, `.pdf` and `.md`.\n        See Also\n        ----------\n        rabbithole_summarizes_documents\n        \"\"\"\n# split file into a list of docs\ndocs = self.file_to_docs(\nfile=file, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n)\n# get summaries\nsummaries = self.cat.mad_hatter.execute_hook(\n\"rabbithole_summarizes_documents\", docs\n)\ndocs = summaries + docs\n# store in memory\nif isinstance(file, str):\nfilename = file\nelse:\nfilename = file.filename\nself.store_documents(docs=docs, source=filename)\ndef ingest_url(\nself,\nurl: str,\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n):\n\"\"\"Load a webpage in the Cat's declarative memory.\n        The method splits and converts a `.html` page to Langchain `Document`. Then, it stores the `Document` in\n        the Cat's memory. Optionally, the document can be summarized and summaries are saved along with the\n        original content.\n        Parameters\n        ----------\n        url : str\n            Url to the webpage.\n        chunk_size : int\n            Number of characters in each document chunk.\n        chunk_overlap : int\n            Number of overlapping characters between consecutive chunks.\n        See Also\n        ----------\n        rabbithole_summarizes_documents\n        \"\"\"\n# get website content and split into a list of docs\ndocs = self.url_to_docs(\nurl=url, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n)\n# get summaries\nsummaries = self.cat.mad_hatter.execute_hook(\n\"rabbithole_summarizes_documents\", docs\n)\ndocs = summaries + docs\n# store docs in memory\nself.store_documents(docs=docs, source=url)\ndef url_to_docs(\nself,\nurl: str,\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n) -&gt; List[Document]:\n\"\"\"Converts an url to Langchain `Document`.\n        The method loads and splits an url content in overlapped chunks of text.\n        The content is then converted to Langchain `Document`.\n        Parameters\n        ----------\n        url : str\n            Url to the webpage.\n        chunk_size : int\n            Number of characters in each document chunk.\n        chunk_overlap : int\n            Number of overlapping characters between consecutive chunks.\n        Returns\n        -------\n        docs : List[Document]\n            List of Langchain `Document` of chunked text.\n        \"\"\"\n# load text content of the website\nloader = UnstructuredURLLoader(urls=[url])\ntext = loader.load()\ndocs = self.split_text(text, chunk_size, chunk_overlap)\nreturn docs\ndef file_to_docs(\nself,\nfile: Union[str, UploadFile],\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n) -&gt; List[Document]:\n\"\"\"Load and convert files to Langchain `Document`.\n        This method takes a file either from a Python script or from the `/rabbithole/` endpoint.\n        Hence, it loads it in memory and splits it in overlapped chunks of text.\n        Parameters\n        ----------\n        file : str, UploadFile\n            The file can be either a string path if loaded programmatically or a FastAPI `UploadFile` if coming from\n            the `/rabbithole/` endpoint.\n        chunk_size : int\n            Number of characters in each document chunk.\n        chunk_overlap : int\n            Number of overlapping characters between consecutive chunks.\n        Returns\n        -------\n        docs : List[Document]\n            List of Langchain `Document` of chunked text.\n        Notes\n        -----\n        Currently supported formats are `.txt`, `.pdf` and `.md`.\n        \"\"\"\n# Create temporary file\ntemp_file = tempfile.NamedTemporaryFile(dir=\"/tmp/\", delete=False)\ntemp_name = temp_file.name\n# Check type of incoming file.\n# It can be either UploadFile if coming from GUI\n#   or an absolute path if auto-ingested be the Cat\nif isinstance(file, UploadFile):\n# Get mime type of UploadFile\n# content_type = file.content_type\ncontent_type = mimetypes.guess_type(file.filename)[0]\n# Get file bytes\nfile_bytes = file.file.read()\nelif isinstance(file, str):\n# Get mime type from file extension\ncontent_type = mimetypes.guess_type(file)[0]\n# Get file bytes\nwith open(file, \"rb\") as f:\nfile_bytes = f.read()\nelse:\nraise ValueError(f\"{type(file)} is not a valid type.\")\n# Open temp file in binary write mode\nwith open(temp_name, \"wb\") as temp_binary_file:\n# Write bytes to file\ntemp_binary_file.write(file_bytes)\n# decide loader\nif content_type == \"text/plain\":\nloader = UnstructuredFileLoader(temp_name)\nelif content_type == \"text/markdown\":\nloader = UnstructuredMarkdownLoader(temp_name)\nelif content_type == \"application/pdf\":\nloader = PDFMinerLoader(temp_name)\nelse:\nraise Exception(\"MIME type not supported for upload\")\n# extract text from file\ntext = loader.load()\n# delete tmp file\nos.remove(temp_name)\ndocs = self.split_text(text, chunk_size, chunk_overlap)\nreturn docs\ndef store_documents(self, docs: List[Document], source: str) -&gt; None:\n\"\"\"Add documents to the Cat's declarative memory.\n        This method loops a list of Langchain `Document` and adds some metadata. Namely, the source filename and the\n        timestamp of insertion. Once done, the method notifies the client via Websocket connection.\n        Parameters\n        ----------\n        docs : List[Document]\n            List of Langchain `Document` to be inserted in the Cat's declarative memory.\n        source : str\n            Source name to be added as a metadata. It can be a file name or an URL.\n        Notes\n        -------\n        At this point, it is possible to customize the Cat's behavior using the `before_rabbithole_insert_memory` hook\n        to edit the memories before they are inserted in the vector database.\n        See Also\n        --------\n        before_rabbithole_insert_memory\n        \"\"\"\nlog(f\"Preparing to memorize {len(docs)} vectors\")\n# classic embed\nfor d, doc in enumerate(docs):\ndoc.metadata[\"source\"] = source\ndoc.metadata[\"when\"] = time.time()\ndoc = self.cat.mad_hatter.execute_hook(\n\"before_rabbithole_insert_memory\", doc\n)\ninserting_info = f\"{d + 1}/{len(docs)}):    {doc.page_content}\"\nif doc.page_content != \"\":\n_ = self.cat.memory.vectors.declarative.add_texts(\n[doc.page_content],\n[doc.metadata],\n)\n#log(f\"Inserted into memory({inserting_info})\", \"INFO\")\nprint(f\"Inserted into memory({inserting_info})\")\nelse:\nlog(f\"Skipped memory insertion of empty doc ({inserting_info})\", \"INFO\")\n# wait a little to avoid APIs rate limit errors\ntime.sleep(0.1)\n# notify client\nfinished_reading_message = f\"Finished reading {source}, \" \\\n            f\"I made {len(docs)} thoughts on it.\"\nself.cat.web_socket_notifications.append(\n{\n\"error\": False,\n\"type\": \"notification\",\n\"content\": finished_reading_message,\n\"why\": {},\n}\n)\nprint(f\"\\n\\nDone uploading {source}\")\ndef split_text(self, text, chunk_size, chunk_overlap):\n\"\"\"Split text in overlapped chunks.\n        This method executes the `rabbithole_splits_text` to split the incoming text in overlapped\n        chunks of text. Other two hooks are available to edit the text before and after the split step.\n        Parameters\n        ----------\n        text : str\n            Content of the loaded file.\n        chunk_size : int\n            Number of characters in each document chunk.\n        chunk_overlap : int\n            Number of overlapping characters between consecutive chunks.\n        Returns\n        -------\n        docs : List[Document]\n            List of split Langchain `Document`.\n        Notes\n        -----\n        The default behavior only executes the `rabbithole_splits_text` hook. `before_rabbithole_splits_text` and\n        `after_rabbithole_splitted_text` hooks return the original input without any modification.\n        See Also\n        --------\n        before_rabbithole_splits_text\n        rabbithole_splits_text\n        after_rabbithole_splitted_text\n        \"\"\"\n# do something on the text before it is split\ntext = self.cat.mad_hatter.execute_hook(\n\"before_rabbithole_splits_text\", text\n)\n# split the documents using chunk_size and chunk_overlap\ndocs = self.cat.mad_hatter.execute_hook(\n\"rabbithole_splits_text\", text, chunk_size, chunk_overlap\n)\n# do something on the text after it is split\ndocs = self.cat.mad_hatter.execute_hook(\n\"after_rabbithole_splitted_text\", docs\n)\nreturn docs\n</code></pre>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.file_to_docs","title":"<code>file_to_docs(file, chunk_size=400, chunk_overlap=100)</code>","text":"<p>Load and convert files to Langchain <code>Document</code>.</p> <p>This method takes a file either from a Python script or from the <code>/rabbithole/</code> endpoint. Hence, it loads it in memory and splits it in overlapped chunks of text.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str, UploadFile</code> <p>The file can be either a string path if loaded programmatically or a FastAPI <code>UploadFile</code> if coming from the <code>/rabbithole/</code> endpoint.</p> required <code>chunk_size</code> <code>int</code> <p>Number of characters in each document chunk.</p> <code>400</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between consecutive chunks.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> of chunked text.</p>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.file_to_docs--notes","title":"Notes","text":"<p>Currently supported formats are <code>.txt</code>, <code>.pdf</code> and <code>.md</code>.</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def file_to_docs(\nself,\nfile: Union[str, UploadFile],\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n) -&gt; List[Document]:\n\"\"\"Load and convert files to Langchain `Document`.\n    This method takes a file either from a Python script or from the `/rabbithole/` endpoint.\n    Hence, it loads it in memory and splits it in overlapped chunks of text.\n    Parameters\n    ----------\n    file : str, UploadFile\n        The file can be either a string path if loaded programmatically or a FastAPI `UploadFile` if coming from\n        the `/rabbithole/` endpoint.\n    chunk_size : int\n        Number of characters in each document chunk.\n    chunk_overlap : int\n        Number of overlapping characters between consecutive chunks.\n    Returns\n    -------\n    docs : List[Document]\n        List of Langchain `Document` of chunked text.\n    Notes\n    -----\n    Currently supported formats are `.txt`, `.pdf` and `.md`.\n    \"\"\"\n# Create temporary file\ntemp_file = tempfile.NamedTemporaryFile(dir=\"/tmp/\", delete=False)\ntemp_name = temp_file.name\n# Check type of incoming file.\n# It can be either UploadFile if coming from GUI\n#   or an absolute path if auto-ingested be the Cat\nif isinstance(file, UploadFile):\n# Get mime type of UploadFile\n# content_type = file.content_type\ncontent_type = mimetypes.guess_type(file.filename)[0]\n# Get file bytes\nfile_bytes = file.file.read()\nelif isinstance(file, str):\n# Get mime type from file extension\ncontent_type = mimetypes.guess_type(file)[0]\n# Get file bytes\nwith open(file, \"rb\") as f:\nfile_bytes = f.read()\nelse:\nraise ValueError(f\"{type(file)} is not a valid type.\")\n# Open temp file in binary write mode\nwith open(temp_name, \"wb\") as temp_binary_file:\n# Write bytes to file\ntemp_binary_file.write(file_bytes)\n# decide loader\nif content_type == \"text/plain\":\nloader = UnstructuredFileLoader(temp_name)\nelif content_type == \"text/markdown\":\nloader = UnstructuredMarkdownLoader(temp_name)\nelif content_type == \"application/pdf\":\nloader = PDFMinerLoader(temp_name)\nelse:\nraise Exception(\"MIME type not supported for upload\")\n# extract text from file\ntext = loader.load()\n# delete tmp file\nos.remove(temp_name)\ndocs = self.split_text(text, chunk_size, chunk_overlap)\nreturn docs\n</code></pre>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_file","title":"<code>ingest_file(file, chunk_size=400, chunk_overlap=100)</code>","text":"<p>Load a file in the Cat's declarative memory.</p> <p>The method splits and converts the file in Langchain <code>Document</code>. Then, it stores the <code>Document</code> in the Cat's memory. Optionally, the document can be summarized and summaries are saved along with the original content.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str, UploadFile</code> <p>The file can be a path passed as a string or an <code>UploadFile</code> object if the document is ingested using the <code>rabbithole</code> endpoint.</p> required <code>chunk_size</code> <code>int</code> <p>Number of characters in each document chunk.</p> <code>400</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between consecutive chunks.</p> <code>100</code>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_file--notes","title":"Notes","text":"<p>Currently supported formats are <code>.txt</code>, <code>.pdf</code> and <code>.md</code>.</p>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_file--see-also","title":"See Also","text":"<p>rabbithole_summarizes_documents</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def ingest_file(\nself,\nfile: Union[str, UploadFile],\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n):\n\"\"\"Load a file in the Cat's declarative memory.\n    The method splits and converts the file in Langchain `Document`. Then, it stores the `Document` in the Cat's\n    memory. Optionally, the document can be summarized and summaries are saved along with the original\n    content.\n    Parameters\n    ----------\n    file : str, UploadFile\n        The file can be a path passed as a string or an `UploadFile` object if the document is ingested using the\n        `rabbithole` endpoint.\n    chunk_size : int\n        Number of characters in each document chunk.\n    chunk_overlap : int\n        Number of overlapping characters between consecutive chunks.\n    Notes\n    ----------\n    Currently supported formats are `.txt`, `.pdf` and `.md`.\n    See Also\n    ----------\n    rabbithole_summarizes_documents\n    \"\"\"\n# split file into a list of docs\ndocs = self.file_to_docs(\nfile=file, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n)\n# get summaries\nsummaries = self.cat.mad_hatter.execute_hook(\n\"rabbithole_summarizes_documents\", docs\n)\ndocs = summaries + docs\n# store in memory\nif isinstance(file, str):\nfilename = file\nelse:\nfilename = file.filename\nself.store_documents(docs=docs, source=filename)\n</code></pre>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_url","title":"<code>ingest_url(url, chunk_size=400, chunk_overlap=100)</code>","text":"<p>Load a webpage in the Cat's declarative memory.</p> <p>The method splits and converts a <code>.html</code> page to Langchain <code>Document</code>. Then, it stores the <code>Document</code> in the Cat's memory. Optionally, the document can be summarized and summaries are saved along with the original content.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Url to the webpage.</p> required <code>chunk_size</code> <code>int</code> <p>Number of characters in each document chunk.</p> <code>400</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between consecutive chunks.</p> <code>100</code>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.ingest_url--see-also","title":"See Also","text":"<p>rabbithole_summarizes_documents</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def ingest_url(\nself,\nurl: str,\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n):\n\"\"\"Load a webpage in the Cat's declarative memory.\n    The method splits and converts a `.html` page to Langchain `Document`. Then, it stores the `Document` in\n    the Cat's memory. Optionally, the document can be summarized and summaries are saved along with the\n    original content.\n    Parameters\n    ----------\n    url : str\n        Url to the webpage.\n    chunk_size : int\n        Number of characters in each document chunk.\n    chunk_overlap : int\n        Number of overlapping characters between consecutive chunks.\n    See Also\n    ----------\n    rabbithole_summarizes_documents\n    \"\"\"\n# get website content and split into a list of docs\ndocs = self.url_to_docs(\nurl=url, chunk_size=chunk_size, chunk_overlap=chunk_overlap\n)\n# get summaries\nsummaries = self.cat.mad_hatter.execute_hook(\n\"rabbithole_summarizes_documents\", docs\n)\ndocs = summaries + docs\n# store docs in memory\nself.store_documents(docs=docs, source=url)\n</code></pre>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.split_text","title":"<code>split_text(text, chunk_size, chunk_overlap)</code>","text":"<p>Split text in overlapped chunks.</p> <p>This method executes the <code>rabbithole_splits_text</code> to split the incoming text in overlapped chunks of text. Other two hooks are available to edit the text before and after the split step.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Content of the loaded file.</p> required <code>chunk_size</code> <code>int</code> <p>Number of characters in each document chunk.</p> required <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between consecutive chunks.</p> required <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of split Langchain <code>Document</code>.</p>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.split_text--notes","title":"Notes","text":"<p>The default behavior only executes the <code>rabbithole_splits_text</code> hook. <code>before_rabbithole_splits_text</code> and <code>after_rabbithole_splitted_text</code> hooks return the original input without any modification.</p>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.split_text--see-also","title":"See Also","text":"<p>before_rabbithole_splits_text rabbithole_splits_text after_rabbithole_splitted_text</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def split_text(self, text, chunk_size, chunk_overlap):\n\"\"\"Split text in overlapped chunks.\n    This method executes the `rabbithole_splits_text` to split the incoming text in overlapped\n    chunks of text. Other two hooks are available to edit the text before and after the split step.\n    Parameters\n    ----------\n    text : str\n        Content of the loaded file.\n    chunk_size : int\n        Number of characters in each document chunk.\n    chunk_overlap : int\n        Number of overlapping characters between consecutive chunks.\n    Returns\n    -------\n    docs : List[Document]\n        List of split Langchain `Document`.\n    Notes\n    -----\n    The default behavior only executes the `rabbithole_splits_text` hook. `before_rabbithole_splits_text` and\n    `after_rabbithole_splitted_text` hooks return the original input without any modification.\n    See Also\n    --------\n    before_rabbithole_splits_text\n    rabbithole_splits_text\n    after_rabbithole_splitted_text\n    \"\"\"\n# do something on the text before it is split\ntext = self.cat.mad_hatter.execute_hook(\n\"before_rabbithole_splits_text\", text\n)\n# split the documents using chunk_size and chunk_overlap\ndocs = self.cat.mad_hatter.execute_hook(\n\"rabbithole_splits_text\", text, chunk_size, chunk_overlap\n)\n# do something on the text after it is split\ndocs = self.cat.mad_hatter.execute_hook(\n\"after_rabbithole_splitted_text\", docs\n)\nreturn docs\n</code></pre>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.store_documents","title":"<code>store_documents(docs, source)</code>","text":"<p>Add documents to the Cat's declarative memory.</p> <p>This method loops a list of Langchain <code>Document</code> and adds some metadata. Namely, the source filename and the timestamp of insertion. Once done, the method notifies the client via Websocket connection.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> to be inserted in the Cat's declarative memory.</p> required <code>source</code> <code>str</code> <p>Source name to be added as a metadata. It can be a file name or an URL.</p> required"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.store_documents--notes","title":"Notes","text":"<p>At this point, it is possible to customize the Cat's behavior using the <code>before_rabbithole_insert_memory</code> hook to edit the memories before they are inserted in the vector database.</p>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.store_documents--see-also","title":"See Also","text":"<p>before_rabbithole_insert_memory</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def store_documents(self, docs: List[Document], source: str) -&gt; None:\n\"\"\"Add documents to the Cat's declarative memory.\n    This method loops a list of Langchain `Document` and adds some metadata. Namely, the source filename and the\n    timestamp of insertion. Once done, the method notifies the client via Websocket connection.\n    Parameters\n    ----------\n    docs : List[Document]\n        List of Langchain `Document` to be inserted in the Cat's declarative memory.\n    source : str\n        Source name to be added as a metadata. It can be a file name or an URL.\n    Notes\n    -------\n    At this point, it is possible to customize the Cat's behavior using the `before_rabbithole_insert_memory` hook\n    to edit the memories before they are inserted in the vector database.\n    See Also\n    --------\n    before_rabbithole_insert_memory\n    \"\"\"\nlog(f\"Preparing to memorize {len(docs)} vectors\")\n# classic embed\nfor d, doc in enumerate(docs):\ndoc.metadata[\"source\"] = source\ndoc.metadata[\"when\"] = time.time()\ndoc = self.cat.mad_hatter.execute_hook(\n\"before_rabbithole_insert_memory\", doc\n)\ninserting_info = f\"{d + 1}/{len(docs)}):    {doc.page_content}\"\nif doc.page_content != \"\":\n_ = self.cat.memory.vectors.declarative.add_texts(\n[doc.page_content],\n[doc.metadata],\n)\n#log(f\"Inserted into memory({inserting_info})\", \"INFO\")\nprint(f\"Inserted into memory({inserting_info})\")\nelse:\nlog(f\"Skipped memory insertion of empty doc ({inserting_info})\", \"INFO\")\n# wait a little to avoid APIs rate limit errors\ntime.sleep(0.1)\n# notify client\nfinished_reading_message = f\"Finished reading {source}, \" \\\n        f\"I made {len(docs)} thoughts on it.\"\nself.cat.web_socket_notifications.append(\n{\n\"error\": False,\n\"type\": \"notification\",\n\"content\": finished_reading_message,\n\"why\": {},\n}\n)\nprint(f\"\\n\\nDone uploading {source}\")\n</code></pre>"},{"location":"technical/API_Documentation/rabbit_hole/#cat.rabbit_hole.RabbitHole.url_to_docs","title":"<code>url_to_docs(url, chunk_size=400, chunk_overlap=100)</code>","text":"<p>Converts an url to Langchain <code>Document</code>.</p> <p>The method loads and splits an url content in overlapped chunks of text. The content is then converted to Langchain <code>Document</code>.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Url to the webpage.</p> required <code>chunk_size</code> <code>int</code> <p>Number of characters in each document chunk.</p> <code>400</code> <code>chunk_overlap</code> <code>int</code> <p>Number of overlapping characters between consecutive chunks.</p> <code>100</code> <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> of chunked text.</p> Source code in <code>cat/rabbit_hole.py</code> <pre><code>def url_to_docs(\nself,\nurl: str,\nchunk_size: int = 400,\nchunk_overlap: int = 100,\n) -&gt; List[Document]:\n\"\"\"Converts an url to Langchain `Document`.\n    The method loads and splits an url content in overlapped chunks of text.\n    The content is then converted to Langchain `Document`.\n    Parameters\n    ----------\n    url : str\n        Url to the webpage.\n    chunk_size : int\n        Number of characters in each document chunk.\n    chunk_overlap : int\n        Number of overlapping characters between consecutive chunks.\n    Returns\n    -------\n    docs : List[Document]\n        List of Langchain `Document` of chunked text.\n    \"\"\"\n# load text content of the website\nloader = UnstructuredURLLoader(urls=[url])\ntext = loader.load()\ndocs = self.split_text(text, chunk_size, chunk_overlap)\nreturn docs\n</code></pre>"},{"location":"technical/API_Documentation/utils/","title":"utils","text":"<p>Various utiles used from the projects.</p>"},{"location":"technical/API_Documentation/utils/#cat.utils.to_camel_case","title":"<code>to_camel_case(text)</code>","text":"<p>Format string to camel case.</p> <p>Takes a string of words separated by either hyphens or underscores and returns a string of words in camel case.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>String of hyphens or underscores separated words.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Camel case formatted string.</p> Source code in <code>cat/utils.py</code> <pre><code>def to_camel_case(text :str ) -&gt; str:\n\"\"\"Format string to camel case.\n    Takes a string of words separated by either hyphens or underscores and returns a string of words in camel case.\n    Parameters\n    ----------\n    text : str\n        String of hyphens or underscores separated words.\n    Returns\n    -------\n    str\n        Camel case formatted string.\n    \"\"\"\ns = text.replace(\"-\", \" \").replace(\"_\", \" \").capitalize()\ns = s.split()\nif len(text) == 0:\nreturn text\nreturn s[0] + \"\".join(i.capitalize() for i in s[1:])\n</code></pre>"},{"location":"technical/API_Documentation/utils/#cat.utils.verbal_timedelta","title":"<code>verbal_timedelta(td)</code>","text":"<p>Convert a timedelta in human form.</p> <p>The function takes a timedelta and converts it to a human-readable string format.</p> <p>Parameters:</p> Name Type Description Default <code>td</code> <code>timedelta</code> <p>Difference between two dates.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Human-readable string of time difference.</p>"},{"location":"technical/API_Documentation/utils/#cat.utils.verbal_timedelta--notes","title":"Notes","text":"<p>This method is used to give the Language Model information time information about the memories retrieved from the vector database.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(verbal_timedelta(timedelta(days=2, weeks=1))\n'One week and two days ago'\n</code></pre> Source code in <code>cat/utils.py</code> <pre><code>def verbal_timedelta(td: timedelta) -&gt; str:\n\"\"\"Convert a timedelta in human form.\n    The function takes a timedelta and converts it to a human-readable string format.\n    Parameters\n    ----------\n    td : timedelta\n        Difference between two dates.\n    Returns\n    -------\n    str\n        Human-readable string of time difference.\n    Notes\n    -----\n    This method is used to give the Language Model information time information about the memories retrieved from\n    the vector database.\n    Examples\n    --------\n    &gt;&gt;&gt; print(verbal_timedelta(timedelta(days=2, weeks=1))\n    'One week and two days ago'\n    \"\"\"\nif td.days != 0:\nabs_days = abs(td.days)\nif abs_days &gt; 7:\nabs_delta = \"{} weeks\".format(td.days // 7)\nelse:\nabs_delta = \"{} days\".format(td.days)\nelse:\nabs_minutes = abs(td.seconds) // 60\nif abs_minutes &gt; 60:\nabs_delta = \"{} hours\".format(abs_minutes // 60)\nelse:\nabs_delta = \"{} minutes\".format(abs_minutes)\nif td &lt; timedelta(0):\nreturn \"{} ago\".format(abs_delta)\nelse:\nreturn \"{} ago\".format(abs_delta)\n</code></pre>"},{"location":"technical/API_Documentation/db/","title":"db","text":""},{"location":"technical/API_Documentation/db/crud/","title":"crud","text":""},{"location":"technical/API_Documentation/db/database/","title":"database","text":""},{"location":"technical/API_Documentation/db/database/#cat.db.database.get_db_session","title":"<code>get_db_session()</code>","text":"<p>Create a new database session and close the session after the operation has ended.</p> Source code in <code>cat/db/database.py</code> <pre><code>def get_db_session():\n\"\"\"\n    Create a new database session and close the session after the operation has ended.\n    \"\"\"\nwith Session(engine) as session:\nyield session\n</code></pre>"},{"location":"technical/API_Documentation/db/models/","title":"models","text":""},{"location":"technical/API_Documentation/factory/custom_llm/","title":"custom_llm","text":""},{"location":"technical/API_Documentation/factory/custom_llm/#cat.factory.custom_llm.LLMCustom","title":"<code>LLMCustom</code>","text":"<p>         Bases: <code>LLM</code></p> Source code in <code>cat/factory/custom_llm.py</code> <pre><code>class LLMCustom(LLM):\n# endpoint where custom LLM service accepts requests\nurl: str\n# optional key for authentication\nauth_key: str = \"\"\n# optional dictionary containing custom configuration\noptions: Dict = {}\n@property\ndef _llm_type(self) -&gt; str:\nreturn \"custom\"\ndef _call(\nself,\nprompt: str,\nstop: Optional[List[str]] = None,\n# run_manager: Optional[CallbackManagerForLLMRun] = None,\nrun_manager: Optional[Any] = None,\n) -&gt; str:\nrequest_body = {\n\"text\": prompt,\n\"auth_key\": self.auth_key,\n\"options\": self.options\n}\ntry:\nresponse_json = requests.post(self.url, json=request_body).json()\nexcept Exception:\nraise Exception(\"Custom LLM endpoint error \"\n\"during http POST request\")\ngenerated_text = response_json[\"text\"]\nreturn f\"AI: {generated_text}\"\n@property\ndef _identifying_params(self) -&gt; Mapping[str, Any]:\n\"\"\"Identifying parameters.\"\"\"\nreturn {\n\"url\": self.url,\n\"auth_key\": self.auth_key,\n\"options\": self.options\n}\n</code></pre>"},{"location":"technical/API_Documentation/factory/embedder/","title":"embedder","text":""},{"location":"technical/API_Documentation/factory/llm/","title":"llm","text":""},{"location":"technical/API_Documentation/looking_glass/agent_manager/","title":"agent_manager","text":""},{"location":"technical/API_Documentation/looking_glass/agent_manager/#cat.looking_glass.agent_manager.AgentManager","title":"<code>AgentManager</code>","text":"<p>Manager of Langchain Agent.</p> <p>This class manages the Agent that uses the LLM. It takes care of formatting the prompt and filtering the tools before feeding them to the Agent. It also instantiates the Langchain Agent.</p> <p>Attributes:</p> Name Type Description <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> Source code in <code>cat/looking_glass/agent_manager.py</code> <pre><code>class AgentManager:\n\"\"\"Manager of Langchain Agent.\n    This class manages the Agent that uses the LLM. It takes care of formatting the prompt and filtering the tools\n    before feeding them to the Agent. It also instantiates the Langchain Agent.\n    Attributes\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    \"\"\"\ndef __init__(self, cat):\nself.cat = cat\ndef execute_tool_agent(self, agent_input, allowed_tools):\nallowed_tools_names = [t.name for t in allowed_tools]\nprompt = ToolPromptTemplate(\n#template= TODO: get from hook,\ntools=allowed_tools,\n# This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n# This includes the `intermediate_steps` variable because it is needed to fill the scratchpad\ninput_variables=[\"input\", \"intermediate_steps\"]\n)\n# main chain\nagent_chain = LLMChain(prompt=prompt, llm=self.cat.llm, verbose=True)\n# init agent\nagent = LLMSingleActionAgent(\nllm_chain=agent_chain,\noutput_parser=ToolOutputParser(),\nstop=[\"\\nObservation:\"],\nallowed_tools=allowed_tools_names,\nverbose=True\n)\n# agent executor\nagent_executor = AgentExecutor.from_agent_and_tools(\nagent=agent,\ntools=allowed_tools,\nreturn_intermediate_steps=True,\nverbose=True\n)\nout = agent_executor(agent_input)\nreturn out\ndef execute_memory_chain(self, agent_input, prompt_prefix, prompt_suffix):\n# memory chain (second step)\nmemory_prompt = PromptTemplate(\ntemplate = prompt_prefix + prompt_suffix,\ninput_variables=[\n\"input\",\n\"chat_history\",\n\"episodic_memory\",\n\"declarative_memory\",\n]\n)\nmemory_chain = LLMChain(\nprompt=memory_prompt,\nllm=self.cat.llm,\nverbose=True\n)\nout = memory_chain(agent_input)\nout[\"output\"] = out[\"text\"]\nreturn out\ndef execute_agent(self, agent_input):\n\"\"\"Instantiate the Agent with tools.\n        The method formats the main prompt and gather the allowed tools. It also instantiates a conversational Agent\n        from Langchain.\n        Returns\n        -------\n        agent_executor : AgentExecutor\n            Instance of the Agent provided with a set of tools.\n        \"\"\"\nmad_hatter = self.cat.mad_hatter\n# this hook allows to reply without executing the agent (for example canned responses, out-of-topic barriers etc.)\nfast_reply = mad_hatter.execute_hook(\"before_agent_starts\", agent_input)\nif fast_reply:\nreturn fast_reply\nprompt_prefix = mad_hatter.execute_hook(\"agent_prompt_prefix\")\n#prompt_format_instructions = mad_hatter.execute_hook(\"agent_prompt_instructions\")\nprompt_suffix = mad_hatter.execute_hook(\"agent_prompt_suffix\")\n#input_variables = [\n#    \"input\",\n#    \"chat_history\",\n#    \"episodic_memory\",\n#    \"declarative_memory\",\n#    \"agent_scratchpad\",\n#]\n#input_variables = mad_hatter.execute_hook(\"before_agent_creates_prompt\", input_variables,\n#                                         \" \".join([prompt_prefix, prompt_format_instructions, prompt_suffix]))\n# Try to reply only with tools\nallowed_tools = mad_hatter.execute_hook(\"agent_allowed_tools\")\ntools_are_enough = False\nif len(allowed_tools) &gt; 0:\ntry:\nout = self.execute_tool_agent(agent_input, allowed_tools)\ntools_are_enough = out[\"output\"] != \"?\"\nexcept Exception as e:\nerror_description = str(e)\nlog(error_description, \"ERROR\") \ntools_are_enough = False\n# if tools were not enough, use memory # TODO: refine tool output?\nif not tools_are_enough:\nout = self.execute_memory_chain(agent_input, prompt_prefix, prompt_suffix)\nreturn out\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/agent_manager/#cat.looking_glass.agent_manager.AgentManager.execute_agent","title":"<code>execute_agent(agent_input)</code>","text":"<p>Instantiate the Agent with tools.</p> <p>The method formats the main prompt and gather the allowed tools. It also instantiates a conversational Agent from Langchain.</p> <p>Returns:</p> Name Type Description <code>agent_executor</code> <code>AgentExecutor</code> <p>Instance of the Agent provided with a set of tools.</p> Source code in <code>cat/looking_glass/agent_manager.py</code> <pre><code>def execute_agent(self, agent_input):\n\"\"\"Instantiate the Agent with tools.\n    The method formats the main prompt and gather the allowed tools. It also instantiates a conversational Agent\n    from Langchain.\n    Returns\n    -------\n    agent_executor : AgentExecutor\n        Instance of the Agent provided with a set of tools.\n    \"\"\"\nmad_hatter = self.cat.mad_hatter\n# this hook allows to reply without executing the agent (for example canned responses, out-of-topic barriers etc.)\nfast_reply = mad_hatter.execute_hook(\"before_agent_starts\", agent_input)\nif fast_reply:\nreturn fast_reply\nprompt_prefix = mad_hatter.execute_hook(\"agent_prompt_prefix\")\n#prompt_format_instructions = mad_hatter.execute_hook(\"agent_prompt_instructions\")\nprompt_suffix = mad_hatter.execute_hook(\"agent_prompt_suffix\")\n#input_variables = [\n#    \"input\",\n#    \"chat_history\",\n#    \"episodic_memory\",\n#    \"declarative_memory\",\n#    \"agent_scratchpad\",\n#]\n#input_variables = mad_hatter.execute_hook(\"before_agent_creates_prompt\", input_variables,\n#                                         \" \".join([prompt_prefix, prompt_format_instructions, prompt_suffix]))\n# Try to reply only with tools\nallowed_tools = mad_hatter.execute_hook(\"agent_allowed_tools\")\ntools_are_enough = False\nif len(allowed_tools) &gt; 0:\ntry:\nout = self.execute_tool_agent(agent_input, allowed_tools)\ntools_are_enough = out[\"output\"] != \"?\"\nexcept Exception as e:\nerror_description = str(e)\nlog(error_description, \"ERROR\") \ntools_are_enough = False\n# if tools were not enough, use memory # TODO: refine tool output?\nif not tools_are_enough:\nout = self.execute_memory_chain(agent_input, prompt_prefix, prompt_suffix)\nreturn out\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/","title":"cheshire_cat","text":""},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat","title":"<code>CheshireCat</code>","text":"<p>The Cheshire Cat.</p> <p>This is the main class that manages everything.</p> <p>Attributes:</p> Name Type Description <code>web_socket_notifications</code> <code>list</code> <p>List of notifications to be sent to the frontend.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>class CheshireCat:\n\"\"\"The Cheshire Cat.\n    This is the main class that manages everything.\n    Attributes\n    ----------\n    web_socket_notifications : list\n        List of notifications to be sent to the frontend.\n    \"\"\"\ndef __init__(self):\n\"\"\"Cat initialization.\n        At init time the Cat loads the database and execute the bootstrap.\n        \"\"\"\n# access to DB\nself.load_db()\n# bootstrap the cat!\nself.bootstrap()\n# queue of cat messages not directly related to last user input\n# i.e. finished uploading a file\nself.web_socket_notifications = []\ndef bootstrap(self):\n\"\"\"Cat's bootstrap.\n        This method is called when the Cat is instantiated and whenever LLM, embedder, agent or memory need\n        to be reinstantiated (for example an LLM change at runtime).\n        Notes\n        -----\n        The pipeline of execution of the functions is important as some method rely on the previous ones.\n        Two hooks allows to intercept the pipeline before and after the bootstrap.\n        The pipeline is:\n            1. Load the plugins, i.e. the MadHatter.\n            2. Execute the `before_cat_bootstrap` hook.\n            3. Load Natural Language Processing related stuff, i.e. the Language Models, the prompts, etc.\n            4. Load the memories, i.e. the LongTermMemory and the WorkingMemory.\n            5. Embed the tools, i.e. insert the tools in the procedural memory.\n            6. Load the AgentManager.\n            7. Load the RabbitHole.\n            8. Execute the `after_cat_bootstrap` hook.\n        See Also\n        --------\n        before_cat_bootstrap\n        after_cat_bootstrap\n        \"\"\"\n# reinstantiate MadHatter (reloads all plugins' hooks and tools)\nself.load_plugins()\n# allows plugins to do something before cat components are loaded\nself.mad_hatter.execute_hook(\"before_cat_bootstrap\")\n# load LLM and embedder\nself.load_natural_language()\n# Load memories (vector collections and working_memory)\nself.load_memory()\n# After memory is loaded, we can get/create tools embeddings\nself.mad_hatter.embed_tools()\n# Agent manager instance (for reasoning)\nself.agent_manager = AgentManager(self)\n# Rabbit Hole Instance\nself.rabbit_hole = RabbitHole(self)\n# allows plugins to do something after the cat bootstrap is complete\nself.mad_hatter.execute_hook(\"after_cat_bootstrap\")\ndef load_db(self):\n\"\"\"Load the SQl database.\"\"\"\n# if there is no db, create it\ncreate_db_and_tables()\n# access db from instance\nself.db = get_db_session\ndef load_natural_language(self):\n\"\"\"Load Natural Language related objects.\n        The method exposes in the Cat all the NLP related stuff. Specifically, it sets the language models\n        (LLM and Embedder), the HyDE and summarization prompts and relative Langchain chains and the main prompt with\n        default settings.\n        Notes\n        -----\n        `use_episodic_memory`, `use_declarative_memory` and `use_procedural_memory` settings can be set from the admin\n        GUI and allows to prevent the Cat from using any of the three vector memories.\n        Warnings\n        --------\n        When using small Language Models it is suggested to turn off the memories and make the main prompt smaller\n        to prevent them to fail.\n        See Also\n        --------\n        get_language_model\n        get_language_embedder\n        hypothetical_embedding_prompt\n        summarization_prompt\n        agent_prompt_prefix\n         \"\"\"\n# LLM and embedder\nself.llm = self.mad_hatter.execute_hook(\"get_language_model\")\nself.embedder = self.mad_hatter.execute_hook(\"get_language_embedder\")\n# HyDE chain\nhypothesis_prompt = langchain.PromptTemplate(\ninput_variables=[\"input\"],\ntemplate=self.mad_hatter.execute_hook(\"hypothetical_embedding_prompt\"),\n)\nself.hypothetis_chain = langchain.chains.LLMChain(prompt=hypothesis_prompt, llm=self.llm)\nself.summarization_prompt = self.mad_hatter.execute_hook(\"summarization_prompt\")\n# custom summarization chain\nself.summarization_chain = langchain.chains.LLMChain(\nllm=self.llm,\nverbose=False,\nprompt=langchain.PromptTemplate(template=self.summarization_prompt, input_variables=[\"text\"]),\n)\n# set the default prompt settings\nself.default_prompt_settings = {\n\"prefix\": \"\",\n\"use_episodic_memory\": True,\n\"use_declarative_memory\": True,\n\"use_procedural_memory\": True,\n}\ndef load_memory(self):\n\"\"\"Load LongTerMemory and WorkingMemory.\"\"\"\n# Memory\nvector_memory_config = {\"cat\": self, \"verbose\": True}\nself.memory = LongTermMemory(vector_memory_config=vector_memory_config)\n# List working memory per user\nself.working_memory_list = WorkingMemoryList()\n# Load default shared working memory user\nself.working_memory = self.working_memory_list.get_working_memory()\ndef load_plugins(self):\n\"\"\"Instantiate the plugins manager.\"\"\"\n# Load plugin system\nself.mad_hatter = MadHatter(self)\ndef recall_relevant_memories_to_working_memory(self):\n\"\"\"Retrieve context from memory.\n        The method retrieves the relevant memories from the vector collections that are given as context to the LLM.\n        Recalled memories are stored in the working memory.\n        Notes\n        -----\n        The user's message is used as a query to make a similarity search in the Cat's vector memories.\n        Two hooks allow to customize the recall pipeline before and after it is done.\n        See Also\n        --------\n        before_cat_recalls_memories\n        after_cat_recalls_memories\n        \"\"\"\nuser_id = self.working_memory.get_user_id()\nuser_message = self.working_memory[\"user_message_json\"][\"text\"]\nprompt_settings = self.working_memory[\"user_message_json\"][\"prompt_settings\"]\n# hook to do something before recall begins\nk_episodic, threshold_episodic, k_declarative, threshold_declarative, k_procedural, threshold_procedural = self.mad_hatter.execute_hook(\n\"before_cat_recalls_memories\", user_message)\n# We may want to search in memory\nmemory_query_text = self.mad_hatter.execute_hook(\"cat_recall_query\", user_message)\nlog(f'Recall query: \"{memory_query_text}\"')\n##### embed recall query\nmemory_query_embedding = self.embedder.embed_query(memory_query_text)\nself.working_memory[\"memory_query\"] = memory_query_text\n##### Episodic memory\nif prompt_settings[\"use_episodic_memory\"]:\n# recall relevant memories (episodic)\nepisodic_memories = self.memory.vectors.episodic.recall_memories_from_embedding(\nembedding=memory_query_embedding,\nk=k_episodic,\nthreshold=threshold_episodic,\nmetadata={\n\"source\": user_id\n}\n)\nelse:\nepisodic_memories = []\nself.working_memory[\"episodic_memories\"] = episodic_memories\n##### Declarative memory\nif prompt_settings[\"use_declarative_memory\"]:\n# recall relevant memories (declarative)\ndeclarative_memories = self.memory.vectors.declarative.recall_memories_from_embedding(\nembedding=memory_query_embedding, k=k_declarative, threshold=threshold_declarative\n)\nelse:\ndeclarative_memories = []\nself.working_memory[\"declarative_memories\"] = declarative_memories\n##### Procedural memory\nif prompt_settings[\"use_procedural_memory\"]:\n# recall relevant tools (procedural collection)\ntools = self.memory.vectors.procedural.recall_memories_from_embedding(\nembedding=memory_query_embedding, k=k_procedural, threshold=threshold_procedural\n)\nelse:\ntools = []\nself.working_memory[\"procedural_memories\"] = tools\n# hook to modify/enrich retrieved memories\nself.mad_hatter.execute_hook(\"after_cat_recalls_memories\", memory_query_text)\ndef format_agent_input(self):\n\"\"\"Format the input for the Agent.\n        The method formats the strings of recalled memories and chat history that will be provided to the Langchain\n        Agent and inserted in the prompt.\n        Returns\n        -------\n        dict\n            Formatted output to be parsed by the Agent executor.\n        Notes\n        -----\n        The context of memories and conversation history is properly formatted before being parsed by the and, hence,\n        information are inserted in the main prompt.\n        All the formatting pipeline is hookable and memories can be edited.\n        See Also\n        --------\n        agent_prompt_episodic_memories\n        agent_prompt_declarative_memories\n        agent_prompt_chat_history\n        \"\"\"\n# format memories to be inserted in the prompt\nepisodic_memory_formatted_content = self.mad_hatter.execute_hook(\n\"agent_prompt_episodic_memories\",\nself.working_memory[\"episodic_memories\"],\n)\ndeclarative_memory_formatted_content = self.mad_hatter.execute_hook(\n\"agent_prompt_declarative_memories\",\nself.working_memory[\"declarative_memories\"],\n)\n# format conversation history to be inserted in the prompt\nconversation_history_formatted_content = self.mad_hatter.execute_hook(\n\"agent_prompt_chat_history\", self.working_memory[\"history\"]\n)\nreturn {\n\"input\": self.working_memory[\"user_message_json\"][\"text\"],\n\"episodic_memory\": episodic_memory_formatted_content,\n\"declarative_memory\": declarative_memory_formatted_content,\n\"chat_history\": conversation_history_formatted_content,\n}\ndef store_new_message_in_working_memory(self, user_message_json):\n\"\"\"Store message in working_memory and update the prompt settings.\n        The method update the working memory with the last user's message.\n        Also, the client sends the settings to turn on/off the vector memories.\n        Parameters\n        ----------\n        user_message_json : dict\n            Dictionary with the message received from the Websocket client\n        \"\"\"\n# store last message in working memory\nself.working_memory[\"user_message_json\"] = user_message_json\nprompt_settings = deepcopy(self.default_prompt_settings)\n# override current prompt_settings with prompt settings sent via websocket (if any)\nprompt_settings.update(user_message_json.get(\"prompt_settings\", {}))\nself.working_memory[\"user_message_json\"][\"prompt_settings\"] = prompt_settings\ndef get_base_url(self):\n\"\"\"Allows the Cat expose the base url.\"\"\"\nsecure = os.getenv('CORE_USE_SECURE_PROTOCOLS', '')\nif secure != '':\nsecure = 's'\nreturn f'http{secure}://{os.environ[\"CORE_HOST\"]}:{os.environ[\"CORE_PORT\"]}'\ndef get_base_path(self):\n\"\"\"Allows the Cat expose the base path.\"\"\"\nreturn os.path.join(os.getcwd(), \"cat/\")\ndef get_plugin_path(self):\n\"\"\"Allows the Cat expose the plugins path.\"\"\"\nreturn os.path.join(os.getcwd(), \"cat/plugins/\")\ndef get_static_url(self):\n\"\"\"Allows the Cat expose the static server url.\"\"\"\nreturn self.get_base_url() + \"/static\"\ndef get_static_path(self):\n\"\"\"Allows the Cat expose the static files path.\"\"\"\nreturn os.path.join(os.getcwd(), \"cat/static/\")\ndef __call__(self, user_message_json):\n\"\"\"Call the Cat instance.\n        This method is called on the user's message received from the client.\n        Parameters\n        ----------\n        user_message_json : dict\n            Dictionary received from the Websocket client.\n        Returns\n        -------\n        final_output : dict\n            Dictionary with the Cat's answer to be sent to the client.\n        Notes\n        -----\n        Here happens the main pipeline of the Cat. Namely, the Cat receives the user's input and recall the memories.\n        The retrieved context is formatted properly and given in input to the Agent that uses the LLM to produce the\n        answer. This is formatted in a dictionary to be sent as a JSON via Websocket to the client.\n        \"\"\"\nlog(user_message_json, \"INFO\")\n# Change working memory based on received user_id\nuser_id = user_message_json.get('user_id', 'user')\nuser_message_json['user_id'] = user_id\nself.working_memory = self.working_memory_list.get_working_memory(user_id)\n# hook to modify/enrich user input\nuser_message_json = self.mad_hatter.execute_hook(\"before_cat_reads_message\", user_message_json)\n# store user_message_json in working memory\n# it contains the new message, prompt settings and other info plugins may find useful\nself.store_new_message_in_working_memory(user_message_json)\n# TODO another hook here?\n# recall episodic and declarative memories from vector collections\n#   and store them in working_memory\ntry:\nself.recall_relevant_memories_to_working_memory()\nexcept Exception as e:\nlog(e, \"ERROR\")\ntraceback.print_exc(e)\nerr_message = (\n\"Vector memory error: you probably changed \"\n\"Embedder and old vector memory is not compatible. \"\n\"Please delete `core/long_term_memory` folder.\"\n)\nreturn {\n\"error\": False,\n# TODO: Otherwise the frontend gives notice of the error\n#   but does not show what the error is\n\"content\": err_message,\n\"why\": {},\n}\n# prepare input to be passed to the agent.\n#   Info will be extracted from working memory\nagent_input = self.format_agent_input()\n# reply with agent\ntry:\ncat_message = self.agent_manager.execute_agent(agent_input)\nexcept Exception as e:\n# This error happens when the LLM\n#   does not respect prompt instructions.\n# We grab the LLM output here anyway, so small and\n#   non instruction-fine-tuned models can still be used.\nerror_description = str(e)\nlog(\"LLM does not respect prompt instructions\", \"ERROR\")\nlog(error_description, \"ERROR\")\nif not \"Could not parse LLM output: `\" in error_description:\nraise e\nunparsable_llm_output = error_description.replace(\"Could not parse LLM output: `\", \"\").replace(\"`\", \"\")\ncat_message = {\n\"input\": agent_input[\"input\"],\n\"intermediate_steps\": [],\n\"output\": unparsable_llm_output\n}\nlog(\"cat_message:\", \"DEBUG\")\nlog(cat_message, \"DEBUG\")\n# update conversation history\nuser_message = self.working_memory[\"user_message_json\"][\"text\"]\nself.working_memory.update_conversation_history(who=\"Human\", message=user_message)\nself.working_memory.update_conversation_history(who=\"AI\", message=cat_message[\"output\"])\n# store user message in episodic memory\n# TODO: vectorize and store also conversation chunks\n#   (not raw dialog, but summarization)\n_ = self.memory.vectors.episodic.add_texts(\n[user_message],\n[{\"source\": user_id, \"when\": time.time()}],\n)\n# build data structure for output (response and why with memories)\nepisodic_report = [dict(d[0]) | {\"score\": float(d[1])} for d in self.working_memory[\"episodic_memories\"]]\ndeclarative_report = [dict(d[0]) | {\"score\": float(d[1])} for d in self.working_memory[\"declarative_memories\"]]\nprocedural_report = [dict(d[0]) | {\"score\": float(d[1])} for d in self.working_memory[\"procedural_memories\"]]\nfinal_output = {\n\"error\": False,\n\"type\": \"chat\",\n\"content\": cat_message.get(\"output\"),\n\"why\": {\n\"input\": cat_message.get(\"input\"),\n#\"intermediate_steps\": cat_message.get(\"intermediate_steps\"),\n\"memory\": {\n\"episodic\": episodic_report,\n\"declarative\": declarative_report,\n\"procedural\": procedural_report,\n},\n},\n}\nfinal_output = self.mad_hatter.execute_hook(\"before_cat_sends_message\", final_output)\nreturn final_output\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.__call__","title":"<code>__call__(user_message_json)</code>","text":"<p>Call the Cat instance.</p> <p>This method is called on the user's message received from the client.</p> <p>Parameters:</p> Name Type Description Default <code>user_message_json</code> <code>dict</code> <p>Dictionary received from the Websocket client.</p> required <p>Returns:</p> Name Type Description <code>final_output</code> <code>dict</code> <p>Dictionary with the Cat's answer to be sent to the client.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.__call__--notes","title":"Notes","text":"<p>Here happens the main pipeline of the Cat. Namely, the Cat receives the user's input and recall the memories. The retrieved context is formatted properly and given in input to the Agent that uses the LLM to produce the answer. This is formatted in a dictionary to be sent as a JSON via Websocket to the client.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def __call__(self, user_message_json):\n\"\"\"Call the Cat instance.\n    This method is called on the user's message received from the client.\n    Parameters\n    ----------\n    user_message_json : dict\n        Dictionary received from the Websocket client.\n    Returns\n    -------\n    final_output : dict\n        Dictionary with the Cat's answer to be sent to the client.\n    Notes\n    -----\n    Here happens the main pipeline of the Cat. Namely, the Cat receives the user's input and recall the memories.\n    The retrieved context is formatted properly and given in input to the Agent that uses the LLM to produce the\n    answer. This is formatted in a dictionary to be sent as a JSON via Websocket to the client.\n    \"\"\"\nlog(user_message_json, \"INFO\")\n# Change working memory based on received user_id\nuser_id = user_message_json.get('user_id', 'user')\nuser_message_json['user_id'] = user_id\nself.working_memory = self.working_memory_list.get_working_memory(user_id)\n# hook to modify/enrich user input\nuser_message_json = self.mad_hatter.execute_hook(\"before_cat_reads_message\", user_message_json)\n# store user_message_json in working memory\n# it contains the new message, prompt settings and other info plugins may find useful\nself.store_new_message_in_working_memory(user_message_json)\n# TODO another hook here?\n# recall episodic and declarative memories from vector collections\n#   and store them in working_memory\ntry:\nself.recall_relevant_memories_to_working_memory()\nexcept Exception as e:\nlog(e, \"ERROR\")\ntraceback.print_exc(e)\nerr_message = (\n\"Vector memory error: you probably changed \"\n\"Embedder and old vector memory is not compatible. \"\n\"Please delete `core/long_term_memory` folder.\"\n)\nreturn {\n\"error\": False,\n# TODO: Otherwise the frontend gives notice of the error\n#   but does not show what the error is\n\"content\": err_message,\n\"why\": {},\n}\n# prepare input to be passed to the agent.\n#   Info will be extracted from working memory\nagent_input = self.format_agent_input()\n# reply with agent\ntry:\ncat_message = self.agent_manager.execute_agent(agent_input)\nexcept Exception as e:\n# This error happens when the LLM\n#   does not respect prompt instructions.\n# We grab the LLM output here anyway, so small and\n#   non instruction-fine-tuned models can still be used.\nerror_description = str(e)\nlog(\"LLM does not respect prompt instructions\", \"ERROR\")\nlog(error_description, \"ERROR\")\nif not \"Could not parse LLM output: `\" in error_description:\nraise e\nunparsable_llm_output = error_description.replace(\"Could not parse LLM output: `\", \"\").replace(\"`\", \"\")\ncat_message = {\n\"input\": agent_input[\"input\"],\n\"intermediate_steps\": [],\n\"output\": unparsable_llm_output\n}\nlog(\"cat_message:\", \"DEBUG\")\nlog(cat_message, \"DEBUG\")\n# update conversation history\nuser_message = self.working_memory[\"user_message_json\"][\"text\"]\nself.working_memory.update_conversation_history(who=\"Human\", message=user_message)\nself.working_memory.update_conversation_history(who=\"AI\", message=cat_message[\"output\"])\n# store user message in episodic memory\n# TODO: vectorize and store also conversation chunks\n#   (not raw dialog, but summarization)\n_ = self.memory.vectors.episodic.add_texts(\n[user_message],\n[{\"source\": user_id, \"when\": time.time()}],\n)\n# build data structure for output (response and why with memories)\nepisodic_report = [dict(d[0]) | {\"score\": float(d[1])} for d in self.working_memory[\"episodic_memories\"]]\ndeclarative_report = [dict(d[0]) | {\"score\": float(d[1])} for d in self.working_memory[\"declarative_memories\"]]\nprocedural_report = [dict(d[0]) | {\"score\": float(d[1])} for d in self.working_memory[\"procedural_memories\"]]\nfinal_output = {\n\"error\": False,\n\"type\": \"chat\",\n\"content\": cat_message.get(\"output\"),\n\"why\": {\n\"input\": cat_message.get(\"input\"),\n#\"intermediate_steps\": cat_message.get(\"intermediate_steps\"),\n\"memory\": {\n\"episodic\": episodic_report,\n\"declarative\": declarative_report,\n\"procedural\": procedural_report,\n},\n},\n}\nfinal_output = self.mad_hatter.execute_hook(\"before_cat_sends_message\", final_output)\nreturn final_output\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.__init__","title":"<code>__init__()</code>","text":"<p>Cat initialization.</p> <p>At init time the Cat loads the database and execute the bootstrap.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def __init__(self):\n\"\"\"Cat initialization.\n    At init time the Cat loads the database and execute the bootstrap.\n    \"\"\"\n# access to DB\nself.load_db()\n# bootstrap the cat!\nself.bootstrap()\n# queue of cat messages not directly related to last user input\n# i.e. finished uploading a file\nself.web_socket_notifications = []\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.bootstrap","title":"<code>bootstrap()</code>","text":"<p>Cat's bootstrap.</p> <p>This method is called when the Cat is instantiated and whenever LLM, embedder, agent or memory need to be reinstantiated (for example an LLM change at runtime).</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.bootstrap--notes","title":"Notes","text":"<p>The pipeline of execution of the functions is important as some method rely on the previous ones. Two hooks allows to intercept the pipeline before and after the bootstrap. The pipeline is:</p> <pre><code>1. Load the plugins, i.e. the MadHatter.\n2. Execute the `before_cat_bootstrap` hook.\n3. Load Natural Language Processing related stuff, i.e. the Language Models, the prompts, etc.\n4. Load the memories, i.e. the LongTermMemory and the WorkingMemory.\n5. Embed the tools, i.e. insert the tools in the procedural memory.\n6. Load the AgentManager.\n7. Load the RabbitHole.\n8. Execute the `after_cat_bootstrap` hook.\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.bootstrap--see-also","title":"See Also","text":"<p>before_cat_bootstrap after_cat_bootstrap</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def bootstrap(self):\n\"\"\"Cat's bootstrap.\n    This method is called when the Cat is instantiated and whenever LLM, embedder, agent or memory need\n    to be reinstantiated (for example an LLM change at runtime).\n    Notes\n    -----\n    The pipeline of execution of the functions is important as some method rely on the previous ones.\n    Two hooks allows to intercept the pipeline before and after the bootstrap.\n    The pipeline is:\n        1. Load the plugins, i.e. the MadHatter.\n        2. Execute the `before_cat_bootstrap` hook.\n        3. Load Natural Language Processing related stuff, i.e. the Language Models, the prompts, etc.\n        4. Load the memories, i.e. the LongTermMemory and the WorkingMemory.\n        5. Embed the tools, i.e. insert the tools in the procedural memory.\n        6. Load the AgentManager.\n        7. Load the RabbitHole.\n        8. Execute the `after_cat_bootstrap` hook.\n    See Also\n    --------\n    before_cat_bootstrap\n    after_cat_bootstrap\n    \"\"\"\n# reinstantiate MadHatter (reloads all plugins' hooks and tools)\nself.load_plugins()\n# allows plugins to do something before cat components are loaded\nself.mad_hatter.execute_hook(\"before_cat_bootstrap\")\n# load LLM and embedder\nself.load_natural_language()\n# Load memories (vector collections and working_memory)\nself.load_memory()\n# After memory is loaded, we can get/create tools embeddings\nself.mad_hatter.embed_tools()\n# Agent manager instance (for reasoning)\nself.agent_manager = AgentManager(self)\n# Rabbit Hole Instance\nself.rabbit_hole = RabbitHole(self)\n# allows plugins to do something after the cat bootstrap is complete\nself.mad_hatter.execute_hook(\"after_cat_bootstrap\")\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.format_agent_input","title":"<code>format_agent_input()</code>","text":"<p>Format the input for the Agent.</p> <p>The method formats the strings of recalled memories and chat history that will be provided to the Langchain Agent and inserted in the prompt.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Formatted output to be parsed by the Agent executor.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.format_agent_input--notes","title":"Notes","text":"<p>The context of memories and conversation history is properly formatted before being parsed by the and, hence, information are inserted in the main prompt. All the formatting pipeline is hookable and memories can be edited.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.format_agent_input--see-also","title":"See Also","text":"<p>agent_prompt_episodic_memories agent_prompt_declarative_memories agent_prompt_chat_history</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def format_agent_input(self):\n\"\"\"Format the input for the Agent.\n    The method formats the strings of recalled memories and chat history that will be provided to the Langchain\n    Agent and inserted in the prompt.\n    Returns\n    -------\n    dict\n        Formatted output to be parsed by the Agent executor.\n    Notes\n    -----\n    The context of memories and conversation history is properly formatted before being parsed by the and, hence,\n    information are inserted in the main prompt.\n    All the formatting pipeline is hookable and memories can be edited.\n    See Also\n    --------\n    agent_prompt_episodic_memories\n    agent_prompt_declarative_memories\n    agent_prompt_chat_history\n    \"\"\"\n# format memories to be inserted in the prompt\nepisodic_memory_formatted_content = self.mad_hatter.execute_hook(\n\"agent_prompt_episodic_memories\",\nself.working_memory[\"episodic_memories\"],\n)\ndeclarative_memory_formatted_content = self.mad_hatter.execute_hook(\n\"agent_prompt_declarative_memories\",\nself.working_memory[\"declarative_memories\"],\n)\n# format conversation history to be inserted in the prompt\nconversation_history_formatted_content = self.mad_hatter.execute_hook(\n\"agent_prompt_chat_history\", self.working_memory[\"history\"]\n)\nreturn {\n\"input\": self.working_memory[\"user_message_json\"][\"text\"],\n\"episodic_memory\": episodic_memory_formatted_content,\n\"declarative_memory\": declarative_memory_formatted_content,\n\"chat_history\": conversation_history_formatted_content,\n}\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.get_base_path","title":"<code>get_base_path()</code>","text":"<p>Allows the Cat expose the base path.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def get_base_path(self):\n\"\"\"Allows the Cat expose the base path.\"\"\"\nreturn os.path.join(os.getcwd(), \"cat/\")\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.get_base_url","title":"<code>get_base_url()</code>","text":"<p>Allows the Cat expose the base url.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def get_base_url(self):\n\"\"\"Allows the Cat expose the base url.\"\"\"\nsecure = os.getenv('CORE_USE_SECURE_PROTOCOLS', '')\nif secure != '':\nsecure = 's'\nreturn f'http{secure}://{os.environ[\"CORE_HOST\"]}:{os.environ[\"CORE_PORT\"]}'\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.get_plugin_path","title":"<code>get_plugin_path()</code>","text":"<p>Allows the Cat expose the plugins path.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def get_plugin_path(self):\n\"\"\"Allows the Cat expose the plugins path.\"\"\"\nreturn os.path.join(os.getcwd(), \"cat/plugins/\")\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.get_static_path","title":"<code>get_static_path()</code>","text":"<p>Allows the Cat expose the static files path.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def get_static_path(self):\n\"\"\"Allows the Cat expose the static files path.\"\"\"\nreturn os.path.join(os.getcwd(), \"cat/static/\")\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.get_static_url","title":"<code>get_static_url()</code>","text":"<p>Allows the Cat expose the static server url.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def get_static_url(self):\n\"\"\"Allows the Cat expose the static server url.\"\"\"\nreturn self.get_base_url() + \"/static\"\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_db","title":"<code>load_db()</code>","text":"<p>Load the SQl database.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_db(self):\n\"\"\"Load the SQl database.\"\"\"\n# if there is no db, create it\ncreate_db_and_tables()\n# access db from instance\nself.db = get_db_session\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_memory","title":"<code>load_memory()</code>","text":"<p>Load LongTerMemory and WorkingMemory.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_memory(self):\n\"\"\"Load LongTerMemory and WorkingMemory.\"\"\"\n# Memory\nvector_memory_config = {\"cat\": self, \"verbose\": True}\nself.memory = LongTermMemory(vector_memory_config=vector_memory_config)\n# List working memory per user\nself.working_memory_list = WorkingMemoryList()\n# Load default shared working memory user\nself.working_memory = self.working_memory_list.get_working_memory()\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_natural_language","title":"<code>load_natural_language()</code>","text":"<p>Load Natural Language related objects.</p> <p>The method exposes in the Cat all the NLP related stuff. Specifically, it sets the language models (LLM and Embedder), the HyDE and summarization prompts and relative Langchain chains and the main prompt with default settings.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_natural_language--notes","title":"Notes","text":"<p><code>use_episodic_memory</code>, <code>use_declarative_memory</code> and <code>use_procedural_memory</code> settings can be set from the admin GUI and allows to prevent the Cat from using any of the three vector memories.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_natural_language--warnings","title":"Warnings","text":"<p>When using small Language Models it is suggested to turn off the memories and make the main prompt smaller to prevent them to fail.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_natural_language--see-also","title":"See Also","text":"<p>get_language_model get_language_embedder hypothetical_embedding_prompt summarization_prompt agent_prompt_prefix</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_natural_language(self):\n\"\"\"Load Natural Language related objects.\n    The method exposes in the Cat all the NLP related stuff. Specifically, it sets the language models\n    (LLM and Embedder), the HyDE and summarization prompts and relative Langchain chains and the main prompt with\n    default settings.\n    Notes\n    -----\n    `use_episodic_memory`, `use_declarative_memory` and `use_procedural_memory` settings can be set from the admin\n    GUI and allows to prevent the Cat from using any of the three vector memories.\n    Warnings\n    --------\n    When using small Language Models it is suggested to turn off the memories and make the main prompt smaller\n    to prevent them to fail.\n    See Also\n    --------\n    get_language_model\n    get_language_embedder\n    hypothetical_embedding_prompt\n    summarization_prompt\n    agent_prompt_prefix\n     \"\"\"\n# LLM and embedder\nself.llm = self.mad_hatter.execute_hook(\"get_language_model\")\nself.embedder = self.mad_hatter.execute_hook(\"get_language_embedder\")\n# HyDE chain\nhypothesis_prompt = langchain.PromptTemplate(\ninput_variables=[\"input\"],\ntemplate=self.mad_hatter.execute_hook(\"hypothetical_embedding_prompt\"),\n)\nself.hypothetis_chain = langchain.chains.LLMChain(prompt=hypothesis_prompt, llm=self.llm)\nself.summarization_prompt = self.mad_hatter.execute_hook(\"summarization_prompt\")\n# custom summarization chain\nself.summarization_chain = langchain.chains.LLMChain(\nllm=self.llm,\nverbose=False,\nprompt=langchain.PromptTemplate(template=self.summarization_prompt, input_variables=[\"text\"]),\n)\n# set the default prompt settings\nself.default_prompt_settings = {\n\"prefix\": \"\",\n\"use_episodic_memory\": True,\n\"use_declarative_memory\": True,\n\"use_procedural_memory\": True,\n}\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.load_plugins","title":"<code>load_plugins()</code>","text":"<p>Instantiate the plugins manager.</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def load_plugins(self):\n\"\"\"Instantiate the plugins manager.\"\"\"\n# Load plugin system\nself.mad_hatter = MadHatter(self)\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.recall_relevant_memories_to_working_memory","title":"<code>recall_relevant_memories_to_working_memory()</code>","text":"<p>Retrieve context from memory.</p> <p>The method retrieves the relevant memories from the vector collections that are given as context to the LLM. Recalled memories are stored in the working memory.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.recall_relevant_memories_to_working_memory--notes","title":"Notes","text":"<p>The user's message is used as a query to make a similarity search in the Cat's vector memories. Two hooks allow to customize the recall pipeline before and after it is done.</p>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.recall_relevant_memories_to_working_memory--see-also","title":"See Also","text":"<p>before_cat_recalls_memories after_cat_recalls_memories</p> Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def recall_relevant_memories_to_working_memory(self):\n\"\"\"Retrieve context from memory.\n    The method retrieves the relevant memories from the vector collections that are given as context to the LLM.\n    Recalled memories are stored in the working memory.\n    Notes\n    -----\n    The user's message is used as a query to make a similarity search in the Cat's vector memories.\n    Two hooks allow to customize the recall pipeline before and after it is done.\n    See Also\n    --------\n    before_cat_recalls_memories\n    after_cat_recalls_memories\n    \"\"\"\nuser_id = self.working_memory.get_user_id()\nuser_message = self.working_memory[\"user_message_json\"][\"text\"]\nprompt_settings = self.working_memory[\"user_message_json\"][\"prompt_settings\"]\n# hook to do something before recall begins\nk_episodic, threshold_episodic, k_declarative, threshold_declarative, k_procedural, threshold_procedural = self.mad_hatter.execute_hook(\n\"before_cat_recalls_memories\", user_message)\n# We may want to search in memory\nmemory_query_text = self.mad_hatter.execute_hook(\"cat_recall_query\", user_message)\nlog(f'Recall query: \"{memory_query_text}\"')\n##### embed recall query\nmemory_query_embedding = self.embedder.embed_query(memory_query_text)\nself.working_memory[\"memory_query\"] = memory_query_text\n##### Episodic memory\nif prompt_settings[\"use_episodic_memory\"]:\n# recall relevant memories (episodic)\nepisodic_memories = self.memory.vectors.episodic.recall_memories_from_embedding(\nembedding=memory_query_embedding,\nk=k_episodic,\nthreshold=threshold_episodic,\nmetadata={\n\"source\": user_id\n}\n)\nelse:\nepisodic_memories = []\nself.working_memory[\"episodic_memories\"] = episodic_memories\n##### Declarative memory\nif prompt_settings[\"use_declarative_memory\"]:\n# recall relevant memories (declarative)\ndeclarative_memories = self.memory.vectors.declarative.recall_memories_from_embedding(\nembedding=memory_query_embedding, k=k_declarative, threshold=threshold_declarative\n)\nelse:\ndeclarative_memories = []\nself.working_memory[\"declarative_memories\"] = declarative_memories\n##### Procedural memory\nif prompt_settings[\"use_procedural_memory\"]:\n# recall relevant tools (procedural collection)\ntools = self.memory.vectors.procedural.recall_memories_from_embedding(\nembedding=memory_query_embedding, k=k_procedural, threshold=threshold_procedural\n)\nelse:\ntools = []\nself.working_memory[\"procedural_memories\"] = tools\n# hook to modify/enrich retrieved memories\nself.mad_hatter.execute_hook(\"after_cat_recalls_memories\", memory_query_text)\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/cheshire_cat/#cat.looking_glass.cheshire_cat.CheshireCat.store_new_message_in_working_memory","title":"<code>store_new_message_in_working_memory(user_message_json)</code>","text":"<p>Store message in working_memory and update the prompt settings.</p> <p>The method update the working memory with the last user's message. Also, the client sends the settings to turn on/off the vector memories.</p> <p>Parameters:</p> Name Type Description Default <code>user_message_json</code> <code>dict</code> <p>Dictionary with the message received from the Websocket client</p> required Source code in <code>cat/looking_glass/cheshire_cat.py</code> <pre><code>def store_new_message_in_working_memory(self, user_message_json):\n\"\"\"Store message in working_memory and update the prompt settings.\n    The method update the working memory with the last user's message.\n    Also, the client sends the settings to turn on/off the vector memories.\n    Parameters\n    ----------\n    user_message_json : dict\n        Dictionary with the message received from the Websocket client\n    \"\"\"\n# store last message in working memory\nself.working_memory[\"user_message_json\"] = user_message_json\nprompt_settings = deepcopy(self.default_prompt_settings)\n# override current prompt_settings with prompt settings sent via websocket (if any)\nprompt_settings.update(user_message_json.get(\"prompt_settings\", {}))\nself.working_memory[\"user_message_json\"][\"prompt_settings\"] = prompt_settings\n</code></pre>"},{"location":"technical/API_Documentation/looking_glass/output_parser/","title":"output_parser","text":""},{"location":"technical/API_Documentation/looking_glass/prompts/","title":"prompts","text":""},{"location":"technical/API_Documentation/mad_hatter/decorators/","title":"decorators","text":""},{"location":"technical/API_Documentation/mad_hatter/decorators/#cat.mad_hatter.decorators.tool","title":"<code>tool(*args, return_direct=False)</code>","text":"<p>Make tools out of functions, can be used with or without arguments. Requires:     - Function must be of type (str) -&gt; str     - Function must have a docstring Examples:     .. code-block:: python         @tool         def search_api(query: str) -&gt; str:             # Searches the API for the query.             return         @tool(\"search\", return_direct=True)         def search_api(query: str) -&gt; str:             # Searches the API for the query.             return</p> Source code in <code>cat/mad_hatter/decorators.py</code> <pre><code>def tool(*args: Union[str, Callable], return_direct: bool = False) -&gt; Callable:\n\"\"\"Make tools out of functions, can be used with or without arguments.\n    Requires:\n        - Function must be of type (str) -&gt; str\n        - Function must have a docstring\n    Examples:\n        .. code-block:: python\n            @tool\n            def search_api(query: str) -&gt; str:\n                # Searches the API for the query.\n                return\n            @tool(\"search\", return_direct=True)\n            def search_api(query: str) -&gt; str:\n                # Searches the API for the query.\n                return\n    \"\"\"\ndef _make_with_name(tool_name: str) -&gt; Callable:\ndef _make_tool(func: Callable[[str], str]) -&gt; Tool:\nassert func.__doc__, \"Function must have a docstring\"\n# Description example:\n#   search_api(query: str) - Searches the API for the query.\ndescription = f\"{tool_name}{signature(func)} - {func.__doc__.strip()}\"\ntool_ = CatTool(\nname=tool_name,\nfunc=func,\ndescription=description,\nreturn_direct=return_direct,\n)\nreturn tool_\nreturn _make_tool\nif len(args) == 1 and isinstance(args[0], str):\n# if the argument is a string, then we use the string as the tool name\n# Example usage: @tool(\"search\", return_direct=True)\nreturn _make_with_name(args[0])\nelif len(args) == 1 and callable(args[0]):\n# if the argument is a function, then we use the function name as the tool name\n# Example usage: @tool\nreturn _make_with_name(args[0].__name__)(args[0])\nelif len(args) == 0:\n# if there are no arguments, then we use the function name as the tool name\n# Example usage: @tool(return_direct=True)\ndef _partial(func: Callable[[str], str]) -&gt; BaseTool:\nreturn _make_with_name(func.__name__)(func)\nreturn _partial\nelse:\nraise ValueError(\"Too many arguments for tool decorator\")\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/mad_hatter/","title":"mad_hatter","text":""},{"location":"technical/API_Documentation/mad_hatter/core_plugin/tools/","title":"tools","text":""},{"location":"technical/API_Documentation/mad_hatter/core_plugin/tools/#cat.mad_hatter.core_plugin.tools.get_the_time","title":"<code>get_the_time(tool_input, cat)</code>","text":"<p>Replies to \"what time is it\", \"get the clock\" and similar questions. Input is always None.</p> Source code in <code>cat/mad_hatter/core_plugin/tools.py</code> <pre><code>@tool\ndef get_the_time(tool_input, cat):\n\"\"\"Replies to \"what time is it\", \"get the clock\" and similar questions. Input is always None.\"\"\"\nreturn str(datetime.now())\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/agent/","title":"agent","text":"<p>Hooks to modify the Cat's Agent.</p> <p>Here is a collection of methods to hook into the Agent execution pipeline.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/agent/#cat.mad_hatter.core_plugin.hooks.agent.agent_allowed_tools","title":"<code>agent_allowed_tools(cat)</code>","text":"<p>Hook the allowed tools.</p> <p>Allows to decide which tools end up in the Agent prompt.</p> <p>To decide, you can filter the list of loaded tools, but you can also check the context in <code>cat.working_memory</code> and launch custom chains with <code>cat.llm</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>tools</code> <code>List[BaseTool]</code> <p>List of allowed Langchain tools.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/agent.py</code> <pre><code>@hook(priority=0)\ndef agent_allowed_tools(cat) -&gt; List[BaseTool]:\n\"\"\"Hook the allowed tools.\n    Allows to decide which tools end up in the *Agent* prompt.\n    To decide, you can filter the list of loaded tools, but you can also check the context in `cat.working_memory`\n    and launch custom chains with `cat.llm`.\n    Parameters\n    ---------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    tools : List[BaseTool]\n        List of allowed Langchain tools.\n    \"\"\"\n# tools currently recalled in working memory\nrecalled_tools = cat.working_memory[\"procedural_memories\"]\n# Get the tools names only\ntools_names = [t[0].metadata[\"name\"] for t in recalled_tools]\n# Get the LangChain BaseTool by name\ntools = [i for i in cat.mad_hatter.tools if i.name in tools_names]\nreturn tools\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/agent/#cat.mad_hatter.core_plugin.hooks.agent.before_agent_creates_prompt","title":"<code>before_agent_creates_prompt(input_variables, main_prompt, cat)</code>","text":"<p>Hook to dynamically define the input variables.</p> <p>Allows to dynamically filter the input variables that end up in the main prompt by looking for which placeholders there are in it starting from a fixed list.</p> <p>Parameters:</p> Name Type Description Default <code>input_variables</code> <code>List</code> <p>List of placeholders to look for in the main prompt.</p> required <code>main_prompt</code> <p>String made of the prompt prefix, the agent instructions and the prompt suffix.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>input_variables</code> <code>List[str]</code> <p>List of placeholders present in the main prompt.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/agent.py</code> <pre><code>@hook(priority=0)\ndef before_agent_creates_prompt(input_variables, main_prompt, cat):\n\"\"\"Hook to dynamically define the input variables.\n    Allows to dynamically filter the input variables that end up in the main prompt by looking for which placeholders\n    there are in it starting from a fixed list.\n    Parameters\n    ----------\n    input_variables : List\n        List of placeholders to look for in the main prompt.\n    main_prompt: str\n        String made of the prompt prefix, the agent instructions and the prompt suffix.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    input_variables : List[str]\n        List of placeholders present in the main prompt.\n    \"\"\"\n# Loop the input variables and check if they are in the main prompt\ninput_variables = [i for i in input_variables if i in main_prompt]\nreturn input_variables\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/agent/#cat.mad_hatter.core_plugin.hooks.agent.before_agent_starts","title":"<code>before_agent_starts(agent_input, cat)</code>","text":"<p>Hook before the agent starts.</p> <p>This hook is useful to shortcut the Cat response. If you do not want the agent to run, return the final response from here and it will edn up in the chat without the agent being executed.</p> <p>Parameters:</p> Name Type Description Default <code>agent_input</code> <p>Input that is about to be passed to the agent.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>Dict</code> <p>Cat response if you want to avoid using the agent, or None if you want the agent to be executed. See hook's code for example of Cat response</p> <p>Examples:</p> <p>Example 1: can't talk about this topic </p><pre><code># here you could use cat.llm to do topic evaluation\nif \"dog\" in agent_input[\"input\"]:\nreturn {\n\"output\": \"You went out of topic. Can't talk about dog.\"\n}\n</code></pre> <p>Example 2: don't remember (no uploaded documents about topic) </p><pre><code>num_declarative_memories = len( cat.working_memory[\"declarative_memories\"] )\nlog(num_declarative_memories, \"ERROR\")\nif num_declarative_memories == 0:\nreturn {\n\"output\": \"Sorry, I have no memories about that.\"\n}\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/agent.py</code> <pre><code>@hook(priority=0)\ndef before_agent_starts(agent_input, cat) -&gt; Union[None, Dict]:\n\"\"\"Hook before the agent starts.\n    This hook is useful to shortcut the Cat response.\n    If you do not want the agent to run, return the final response from here and it will edn up in the chat without the agent being executed.\n    Parameters\n    --------\n    agent_input: Dict\n        Input that is about to be passed to the agent.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    --------\n    response : Dict\n        Cat response if you want to avoid using the agent, or None if you want the agent to be executed.\n        See hook's code for example of Cat response\n    Examples\n    --------\n    Example 1: can't talk about this topic\n    ```python\n    # here you could use cat.llm to do topic evaluation\n    if \"dog\" in agent_input[\"input\"]:\n        return {\n            \"output\": \"You went out of topic. Can't talk about dog.\"\n        }\n    ```\n    Example 2: don't remember (no uploaded documents about topic)\n    ```python\n    num_declarative_memories = len( cat.working_memory[\"declarative_memories\"] )\n    log(num_declarative_memories, \"ERROR\")\n    if num_declarative_memories == 0:\n        return {\n           \"output\": \"Sorry, I have no memories about that.\"\n        }\n    ```\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/","title":"flow","text":"<p>Hooks to modify the Cat's flow of execution.</p> <p>Here is a collection of methods to hook into the Cat execution pipeline.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.after_cat_bootstrap","title":"<code>after_cat_bootstrap(cat)</code>","text":"<p>Hook into the end of the Cat start up.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Agent Manager and the Rabbit Hole.</p> <p>This hook allows to intercept the end of such process and is executed right after the Cat has finished loading its components.</p> <p>This can be used to set or store variables to be shared further in the pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef after_cat_bootstrap(cat) -&gt; None:\n\"\"\"Hook into the end of the Cat start up.\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM),\n    the memories, the *Agent Manager* and the *Rabbit Hole*.\n    This hook allows to intercept the end of such process and is executed right after the Cat has finished loading\n    its components.\n    This can be used to set or store variables to be shared further in the pipeline.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.after_cat_recalled_memories","title":"<code>after_cat_recalled_memories(memory_query_text, cat)</code>","text":"<p>Hook into semantic search after the memory retrieval.</p> <p>Allows to intercept the recalled memories right after these are stored in the Working Memory. According to the user's input, the relevant context is saved in <code>cat.working_memory[\"episodic_memories\"]</code> and <code>cat.working_memory[\"declarative_memories\"]</code>. At this point, this hook is executed to edit the search query.</p> <p>Parameters:</p> Name Type Description Default <code>memory_query_text</code> <code>str</code> <p>String used to query both episodic and declarative memories.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef after_cat_recalled_memories(memory_query_text: str, cat) -&gt; None:\n\"\"\"Hook into semantic search after the memory retrieval.\n    Allows to intercept the recalled memories right after these are stored in the Working Memory.\n    According to the user's input, the relevant context is saved in `cat.working_memory[\"episodic_memories\"]`\n    and `cat.working_memory[\"declarative_memories\"]`. At this point,\n    this hook is executed to edit the search query.\n    Parameters\n    ----------\n    memory_query_text : str\n        String used to query both *episodic* and *declarative* memories.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.after_cat_recalls_memories","title":"<code>after_cat_recalls_memories(query, cat)</code>","text":"<p>Hook after semantic search in memories.</p> <p>The hook is executed just after the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query used to retrieve memories.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef after_cat_recalls_memories(query: str, cat) -&gt; None:\n\"\"\"Hook after semantic search in memories.\n    The hook is executed just after the Cat searches for the meaningful context in both memories\n    and stores it in the *Working Memory*.\n    Parameters\n    ----------\n    query : str\n        Query used to retrieve memories.\n    cat : CheshireCat\n     Cheshire Cat instance.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_bootstrap","title":"<code>before_cat_bootstrap(cat)</code>","text":"<p>Hook into the Cat start up.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Agent Manager and the Rabbit Hole.</p> <p>This hook allows to intercept such process and is executed in the middle of plugins and natural language objects loading.</p> <p>This hook can be used to set or store variables to be propagated to subsequent loaded objects.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_bootstrap(cat) -&gt; None:\n\"\"\"Hook into the Cat start up.\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM),\n    the memories, the *Agent Manager* and the *Rabbit Hole*.\n    This hook allows to intercept such process and is executed in the middle of plugins and\n    natural language objects loading.\n    This hook can be used to set or store variables to be propagated to subsequent loaded objects.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    \"\"\"\nreturn None\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_reads_message","title":"<code>before_cat_reads_message(user_message_json, cat)</code>","text":"<p>Hook the incoming user's JSON dictionary.</p> <p>Allows to edit and enrich the incoming message received from the WebSocket connection.</p> <p>For instance, this hook can be used to translate the user's message before feeding it to the Cat. Another use case is to add custom keys to the JSON dictionary.</p> <p>The incoming message is a JSON dictionary with keys:     {         \"text\": message content     }</p> <p>Parameters:</p> Name Type Description Default <code>user_message_json</code> <code>dict</code> <p>JSON dictionary with the message received from the chat.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>user_message_json</code> <code>dict</code> <p>Edited JSON dictionary that will be fed to the Cat.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_reads_message--notes","title":"Notes","text":"<p>For example:</p> <pre><code>{\n    \"text\": \"Hello Cheshire Cat!\",\n    \"custom_key\": True\n}\n</code></pre> <p>where \"custom_key\" is a newly added key to the dictionary to store any data.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_reads_message(user_message_json: dict, cat) -&gt; dict:\n\"\"\"Hook the incoming user's JSON dictionary.\n    Allows to edit and enrich the incoming message received from the WebSocket connection.\n    For instance, this hook can be used to translate the user's message before feeding it to the Cat.\n    Another use case is to add custom keys to the JSON dictionary.\n    The incoming message is a JSON dictionary with keys:\n        {\n            \"text\": message content\n        }\n    Parameters\n    ----------\n    user_message_json : dict\n        JSON dictionary with the message received from the chat.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    user_message_json : dict\n        Edited JSON dictionary that will be fed to the Cat.\n    Notes\n    -----\n    For example:\n        {\n            \"text\": \"Hello Cheshire Cat!\",\n            \"custom_key\": True\n        }\n    where \"custom_key\" is a newly added key to the dictionary to store any data.\n    \"\"\"\nreturn user_message_json\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_recalls_memories","title":"<code>before_cat_recalls_memories(user_message, cat)</code>","text":"<p>Hook into semantic search in memories.</p> <p>Allows to intercept when the Cat queries the memories using the embedded user's input.</p> <p>The hook is executed just before the Cat searches for the meaningful context in both memories and stores it in the Working Memory.</p> <p>The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied to the query in the vector memory (items with score under threshold are not retrieved)</p> <p>Parameters:</p> Name Type Description Default <code>user_message</code> <code>str</code> <p>String with the text received from the user. This is used as a query to search into memories.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>k_memory_type</code> <code>int</code> <p>Number of relevant memories to retrieve from the vector database.</p> <code>threshold_memory_type</code> <code>float</code> <p>Threshold to filter memories according their similarity score with the query.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_recalls_memories(user_message: str, cat) -&gt; tuple[int, float]:\n\"\"\"Hook into semantic search in memories.\n    Allows to intercept when the Cat queries the memories using the embedded user's input.\n    The hook is executed just before the Cat searches for the meaningful context in both memories\n    and stores it in the *Working Memory*.\n    The hook return the values for maximum number (k) of items to retrieve from memory and the score threshold applied\n    to the query in the vector memory (items with score under threshold are not retrieved)\n    Parameters\n    ----------\n    user_message : str\n        String with the text received from the user. This is used as a query to search into memories.\n    cat : CheshireCat\n     Cheshire Cat instance.\n    Returns\n    -------\n    k_memory_type : int\n        Number of relevant memories to retrieve from the vector database.\n    threshold_memory_type : float\n        Threshold to filter memories according their similarity score with the query.\n    \"\"\"\nk_episodic = 3\nthreshold_episodic = 0.7\nk_declarative = 3\nthreshold_declarative = 0.7\nk_procedural = 3\nthreshold_procedural = 0.7\nreturn k_episodic, threshold_episodic, k_declarative, threshold_declarative, k_procedural, threshold_procedural\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_sends_message","title":"<code>before_cat_sends_message(message, cat)</code>","text":"<p>Hook the outgoing Cat's message.</p> <p>Allows to edit the JSON dictionary that will be sent to the client via WebSocket connection.</p> <p>This hook can be used to edit the message sent to the user or to add keys to the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>dict</code> <p>JSON dictionary to be sent to the WebSocket client.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>message</code> <code>dict</code> <p>Edited JSON dictionary with the Cat's answer.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.before_cat_sends_message--notes","title":"Notes","text":"<p>Default <code>message</code> is::</p> <pre><code>    {\n        \"error\": False,\n        \"type\": \"chat\",\n        \"content\": cat_message[\"output\"],\n        \"why\": {\n            \"input\": cat_message[\"input\"],\n            \"output\": cat_message[\"output\"],\n            \"intermediate_steps\": cat_message[\"intermediate_steps\"],\n            \"memory\": {\n                \"vectors\": {\n                    \"episodic\": episodic_report,\n                    \"declarative\": declarative_report\n                }\n            },\n        },\n    }\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef before_cat_sends_message(message: dict, cat) -&gt; dict:\n\"\"\"Hook the outgoing Cat's message.\n    Allows to edit the JSON dictionary that will be sent to the client via WebSocket connection.\n    This hook can be used to edit the message sent to the user or to add keys to the dictionary.\n    Parameters\n    ----------\n    message : dict\n        JSON dictionary to be sent to the WebSocket client.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    message : dict\n        Edited JSON dictionary with the Cat's answer.\n    Notes\n    -----\n    Default `message` is::\n            {\n                \"error\": False,\n                \"type\": \"chat\",\n                \"content\": cat_message[\"output\"],\n                \"why\": {\n                    \"input\": cat_message[\"input\"],\n                    \"output\": cat_message[\"output\"],\n                    \"intermediate_steps\": cat_message[\"intermediate_steps\"],\n                    \"memory\": {\n                        \"vectors\": {\n                            \"episodic\": episodic_report,\n                            \"declarative\": declarative_report\n                        }\n                    },\n                },\n            }\n    \"\"\"\nreturn message\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.cat_recall_query","title":"<code>cat_recall_query(user_message, cat)</code>","text":"<p>Hook the Hypothetical Document Embedding (HyDE) search query.</p> <p>This hook allows to edit the user's message used as a query for HyDE. As a result, context retrieval can be conditioned enhancing such message.</p> <p>Parameters:</p> Name Type Description Default <code>user_message</code> <code>str</code> <p>String with the text received from the user.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance to exploit the Cat's methods.</p> required <p>Returns:</p> Type Description <code>Edited string to be used for context retrieval in memory. The returned string is further stored in the</code> <code>Working Memory at `cat.working_memory[\"memory_query\"]`.</code>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.cat_recall_query--notes","title":"Notes","text":"<p>HyDE [1]_ strategy exploits the user's message to generate a hypothetical answer. This is then applied to recall the relevant context from the memory.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/flow/#cat.mad_hatter.core_plugin.hooks.flow.cat_recall_query--references","title":"References","text":"<p>.. [1] Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels.    arXiv preprint arXiv:2212.10496.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/flow.py</code> <pre><code>@hook(priority=0)\ndef cat_recall_query(user_message: str, cat) -&gt; str:\n\"\"\"Hook the Hypothetical Document Embedding (HyDE) search query.\n    This hook allows to edit the user's message used as a query for HyDE.\n    As a result, context retrieval can be conditioned enhancing such message.\n    Parameters\n    ----------\n    user_message : str\n        String with the text received from the user.\n    cat : CheshireCat\n        Cheshire Cat instance to exploit the Cat's methods.\n    Returns\n    -------\n    Edited string to be used for context retrieval in memory. The returned string is further stored in the\n    Working Memory at `cat.working_memory[\"memory_query\"]`.\n    Notes\n    -----\n    HyDE [1]_ strategy exploits the user's message to generate a hypothetical answer. This is then applied to recall\n    the relevant context from the memory.\n    References\n    ----------\n    .. [1] Gao, L., Ma, X., Lin, J., &amp; Callan, J. (2022). Precise Zero-Shot Dense Retrieval without Relevance Labels.\n       arXiv preprint arXiv:2212.10496.\n    \"\"\"\n# example 1: HyDE embedding\n# return cat.hypothetis_chain.run(user_message)\n# example 2: Condense recent conversation\n# TODO\n# here we just return the latest user message as is\nreturn user_message\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/memory/","title":"memory","text":"<p>Hooks to modify the Cat's memory collections.</p> <p>Here is a collection of methods to hook the insertion of memories in the vector databases.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/memory/#cat.mad_hatter.core_plugin.hooks.memory.after_collection_created","title":"<code>after_collection_created(vector_memory_collection, cat)</code>","text":"<p>Do something after a new collection is created in vectorDB</p> <p>Parameters:</p> Name Type Description Default <code>vector_memory_collection</code> <code>VectorMemoryCollection</code> <p>Instance of <code>VectorMemoryCollection</code> wrapping the actual db collection.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Car instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/memory.py</code> <pre><code>@hook(priority=0)\ndef after_collection_created(vector_memory_collection: VectorMemoryCollection, cat):\n\"\"\"Do something after a new collection is created in vectorDB\n    Parameters\n    ----------\n    vector_memory_collection : VectorMemoryCollection\n        Instance of `VectorMemoryCollection` wrapping the actual db collection.\n    cat : CheshireCat\n        Cheshire Car instance.\n    \"\"\"\npass\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/memory/#cat.mad_hatter.core_plugin.hooks.memory.before_collection_created","title":"<code>before_collection_created(vector_memory_collection, cat)</code>","text":"<p>Do something before a new collection is created in vectorDB</p> <p>Parameters:</p> Name Type Description Default <code>vector_memory_collection</code> <code>VectorMemoryCollection</code> <p>Instance of <code>VectorMemoryCollection</code> wrapping the actual db collection.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Car instance.</p> required Source code in <code>cat/mad_hatter/core_plugin/hooks/memory.py</code> <pre><code>@hook(priority=0)\ndef before_collection_created(vector_memory_collection: VectorMemoryCollection, cat):\n\"\"\"Do something before a new collection is created in vectorDB\n    Parameters\n    ----------\n    vector_memory_collection : VectorMemoryCollection\n        Instance of `VectorMemoryCollection` wrapping the actual db collection.\n    cat : CheshireCat\n        Cheshire Car instance.\n    \"\"\"\npass\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/models/","title":"models","text":"<p>Hooks to modify the Cat's language and embedding models.</p> <p>Here is a collection of methods to hook into the settings of the Large Language Model and the Embedder.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/models/#cat.mad_hatter.core_plugin.hooks.models.get_language_embedder","title":"<code>get_language_embedder(cat)</code>","text":"<p>Hook into the  embedder selection.</p> <p>Allows to modify how the Cat selects the embedder at bootstrap time.</p> <p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Agent Manager and the Rabbit Hole.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>embedder</code> <code>Embeddings</code> <p>Selected embedder model.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/models.py</code> <pre><code>@hook(priority=0)\ndef get_language_embedder(cat) -&gt; embedders.EmbedderSettings:\n\"\"\"Hook into the  embedder selection.\n    Allows to modify how the Cat selects the embedder at bootstrap time.\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM),\n    the memories, the *Agent Manager* and the *Rabbit Hole*.\n    Parameters\n    ----------\n    cat: CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    embedder : Embeddings\n        Selected embedder model.\n    \"\"\"\n# Embedding LLM\nselected_embedder = crud.get_setting_by_name(next(cat.db()), name=\"embedder_selected\")\nif selected_embedder is not None:\n# get Embedder factory class\nselected_embedder_class = selected_embedder.value[\"name\"]\nFactoryClass = getattr(embedders, selected_embedder_class)\n# obtain configuration and instantiate Embedder\nselected_embedder_config = crud.get_setting_by_name(\nnext(cat.db()), name=selected_embedder_class\n)\nembedder = FactoryClass.get_embedder_from_config(selected_embedder_config.value)\nreturn embedder\nprint(\"naked cat: \", cat.llm)\n# OpenAI embedder\nif type(cat.llm) in [OpenAI, OpenAIChat, ChatOpenAI]:\nembedder = embedders.EmbedderOpenAIConfig.get_embedder_from_config(\n{\n\"openai_api_key\": cat.llm.openai_api_key,\n}\n)\n# Azure\nelif type(cat.llm) in [AzureOpenAI, AzureChatOpenAI]:\nembedder = embedders.EmbedderAzureOpenAIConfig.get_embedder_from_config(\n{\n\"openai_api_key\": cat.llm.openai_api_key,\n\"openai_api_type\": \"azure\",\n\"model\": \"text-embedding-ada-002\",\n# Now the only model for embeddings is text-embedding-ada-002\n# It is also possible to use the Azure \"deployment\" name that is user defined\n# when the model is deployed to Azure.\n# \"deployment\": \"my-text-embedding-ada-002\",\n\"openai_api_base\": cat.llm.openai_api_base,\n# https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference#embeddings\n# current supported versions 2022-12-01,2023-03-15-preview, 2023-05-15\n# Don't mix api versions https://github.com/hwchase17/langchain/issues/4775\n\"openai_api_version\": \"2023-05-15\",\n}\n)\n# Cohere\nelif type(cat.llm) in [Cohere]:\nembedder = embedders.EmbedderCohereConfig.get_embedder_from_config(\n{\n\"cohere_api_key\": cat.llm.cohere_api_key,\n\"model\": \"embed-multilingual-v2.0\",\n# Now the best model for embeddings is embed-multilingual-v2.0\n}\n)\n# HuggingFace\nelif type(cat.llm) in [HuggingFaceHub]:\nembedder = embedders.EmbedderHuggingFaceHubConfig.get_embedder_from_config(\n{\n\"huggingfacehub_api_token\": cat.llm.huggingfacehub_api_token,\n\"repo_id\": \"sentence-transformers/all-mpnet-base-v2\",\n}\n)\n# elif \"HF_TOKEN\" in os.environ:\n#   if \"HF_EMBEDDER\" in os.environ:\n#     embedder = embedders.EmbedderHuggingFaceHubConfig.get_embedder_from_config(\n#         {\n#             \"huggingfacehub_api_token\": os.environ[\"HF_TOKEN\"],\n#             \"repo_id\": os.environ[\"HF_EMBEDDER\"],\n#         }\n#     )\n# else:\n#     embedder = embedders.EmbedderHuggingFaceHubConfig.get_embedder_from_config(\n#         {\n#             \"huggingfacehub_api_token\": os.environ[\"HF_TOKEN\"],\n#             # repo_id: \"...\" TODO: at the moment use default\n#         }\n#     )\nelse:\nembedder = embedders.EmbedderFakeConfig.get_embedder_from_config(\n{\"size\": 128}  # mock openai embedding size\n)\nreturn embedder\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/models/#cat.mad_hatter.core_plugin.hooks.models.get_language_model","title":"<code>get_language_model(cat)</code>","text":"<p>Hook into the Large Language Model (LLM) selection.</p> <p>Allows to modify how the Cat selects the LLM at bootstrap time.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>lll</code> <code>BaseLLM</code> <p>Langchain <code>BaseLLM</code> instance of the selected model.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/models/#cat.mad_hatter.core_plugin.hooks.models.get_language_model--notes","title":"Notes","text":"<p>Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories, the Agent Manager and the Rabbit Hole.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/models.py</code> <pre><code>@hook(priority=0)\ndef get_language_model(cat) -&gt; BaseLLM:\n\"\"\"Hook into the Large Language Model (LLM) selection.\n    Allows to modify how the Cat selects the LLM at bootstrap time.\n    Parameters\n    ----------\n    cat: CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    lll : BaseLLM\n        Langchain `BaseLLM` instance of the selected model.\n    Notes\n    -----\n    Bootstrapping is the process of loading the plugins, the natural language objects (e.g. the LLM), the memories,\n    the *Agent Manager* and the *Rabbit Hole*.\n    \"\"\"\nselected_llm = crud.get_setting_by_name(next(cat.db()), name=\"llm_selected\")\nif selected_llm is None:\n# return default LLM\nllm = llms.LLMDefaultConfig.get_llm_from_config({})\nelse:\n# get LLM factory class\nselected_llm_class = selected_llm.value[\"name\"]\nFactoryClass = getattr(llms, selected_llm_class)\n# obtain configuration and instantiate LLM\nselected_llm_config = crud.get_setting_by_name(\nnext(cat.db()), name=selected_llm_class\n)\ntry:\nllm = FactoryClass.get_llm_from_config(selected_llm_config.value)\nexcept Exception as e:\nimport traceback\ntraceback.print_exc()\nllm = llms.LLMDefaultConfig.get_llm_from_config({})\nreturn llm\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/","title":"prompt","text":"<p>Hooks to modify the prompts.</p> <p>Here is a collection of methods to hook the prompts components that instruct the Agent.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_chat_history","title":"<code>agent_prompt_chat_history(chat_history, cat)</code>","text":"<p>Hook the chat history.</p> <p>This hook converts to text the recent conversation turns fed to the Agent. The hook allows to edit and enhance the chat history provided as context to the Agent.</p> <p>Parameters:</p> Name Type Description Default <code>chat_history</code> <code>List[Dict]</code> <p>List of dictionaries collecting speaking turns.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instances.</p> required <p>Returns:</p> Name Type Description <code>history</code> <code>str</code> <p>String with recent conversation turns to be provided as context to the Agent.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_chat_history--notes","title":"Notes","text":"<p>Such context is placed in the <code>agent_prompt_suffix</code> in the place held by {chat_history}.</p> <p>The chat history is a dictionary with keys::     'who': the name of who said the utterance;     'message': the utterance.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_chat_history(chat_history: List[Dict], cat) -&gt; str:\n\"\"\"Hook the chat history.\n    This hook converts to text the recent conversation turns fed to the *Agent*.\n    The hook allows to edit and enhance the chat history provided as context to the *Agent*.\n    Parameters\n    ----------\n    chat_history : List[Dict]\n        List of dictionaries collecting speaking turns.\n    cat : CheshireCat\n        Cheshire Cat instances.\n    Returns\n    -------\n    history : str\n        String with recent conversation turns to be provided as context to the *Agent*.\n    Notes\n    -----\n    Such context is placed in the `agent_prompt_suffix` in the place held by {chat_history}.\n    The chat history is a dictionary with keys::\n        'who': the name of who said the utterance;\n        'message': the utterance.\n    \"\"\"\nhistory = \"\"\nfor turn in chat_history:\nhistory += f\"\\n - {turn['who']}: {turn['message']}\"\nreturn history\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_declarative_memories","title":"<code>agent_prompt_declarative_memories(memory_docs, cat)</code>","text":"<p>Hook memories retrieved from declarative memory.</p> <p>This hook formats the relevant memories retrieved from the context of documents uploaded in the Cat's memory.</p> <p>Retrieved memories are converted to string and the source information is added to inform the Agent on which document the information was retrieved from.</p> <p>This hook allows to edit the retrieved memory to condition the information provided as context to the Agent.</p> <p>Such context is placed in the <code>agent_prompt_prefix</code> in the place held by {declarative_memory}.</p> <p>Parameters:</p> Name Type Description Default <code>memory_docs</code> <code>List[Document]</code> <p>list of Langchain <code>Document</code> retrieved from the declarative memory.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>memory_content</code> <code>str</code> <p>String of retrieved context from the declarative memory.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_declarative_memories(memory_docs: List[Document], cat) -&gt; str:\n\"\"\"Hook memories retrieved from declarative memory.\n    This hook formats the relevant memories retrieved from the context of documents uploaded in the Cat's memory.\n    Retrieved memories are converted to string and the source information is added to inform the *Agent* on\n    which document the information was retrieved from.\n    This hook allows to edit the retrieved memory to condition the information provided as context to the *Agent*.\n    Such context is placed in the `agent_prompt_prefix` in the place held by {declarative_memory}.\n    Parameters\n    ----------\n    memory_docs : List[Document]\n        list of Langchain `Document` retrieved from the declarative memory.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    memory_content : str\n        String of retrieved context from the declarative memory.\n    \"\"\"\n# convert docs to simple text\nmemory_texts = [m[0].page_content.replace(\"\\n\", \". \") for m in memory_docs]\n# add source information (e.g. \"extracted from file.txt\")\nmemory_sources = []\nfor m in memory_docs:\n# Get and save the source of the memory\nsource = m[0].metadata[\"source\"]\nmemory_sources.append(f\" (extracted from {source})\")\n# Join Document text content with related source information\nmemory_texts = [a + b for a, b in zip(memory_texts, memory_sources)]\n# Format the memories for the output\nmemories_separator = \"\\n  - \"\nmemory_content = \"## Context of documents containing relevant information: \" + \\\n        memories_separator + memories_separator.join(memory_texts)\n# if no data is retrieved from memory don't erite anithing in the prompt\nif len(memory_texts) == 0:\nmemory_content = \"\"\nreturn memory_content\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_episodic_memories","title":"<code>agent_prompt_episodic_memories(memory_docs, cat)</code>","text":"<p>Hook memories retrieved from episodic memory.</p> <p>This hook formats the relevant memories retrieved from the context of things the human said in the past.</p> <p>Retrieved memories are converted to string and temporal information is added to inform the Agent about when the user said that sentence in the past.</p> <p>This hook allows to edit the retrieved memory to condition the information provided as context to the Agent.</p> <p>Such context is placed in the <code>agent_prompt_prefix</code> in the place held by {episodic_memory}.</p> <p>Parameters:</p> Name Type Description Default <code>memory_docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> retrieved from the episodic memory.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>memory_content</code> <code>str</code> <p>String of retrieved context from the episodic memory.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_episodic_memories(memory_docs: List[Document], cat) -&gt; str:\n\"\"\"Hook memories retrieved from episodic memory.\n    This hook formats the relevant memories retrieved from the context of things the human said in the past.\n    Retrieved memories are converted to string and temporal information is added to inform the *Agent* about\n    when the user said that sentence in the past.\n    This hook allows to edit the retrieved memory to condition the information provided as context to the *Agent*.\n    Such context is placed in the `agent_prompt_prefix` in the place held by {episodic_memory}.\n    Parameters\n    ----------\n    memory_docs : List[Document]\n        List of Langchain `Document` retrieved from the episodic memory.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    memory_content : str\n        String of retrieved context from the episodic memory.\n    \"\"\"\n# convert docs to simple text\nmemory_texts = [m[0].page_content.replace(\"\\n\", \". \") for m in memory_docs]\n# add time information (e.g. \"2 days ago\")\nmemory_timestamps = []\nfor m in memory_docs:\n# Get Time information in the Document metadata\ntimestamp = m[0].metadata[\"when\"]\n# Get Current Time - Time when memory was stored\ndelta = timedelta(seconds=(time.time() - timestamp))\n# Convert and Save timestamps to Verbal (e.g. \"2 days ago\")\nmemory_timestamps.append(f\" ({verbal_timedelta(delta)})\")\n# Join Document text content with related temporal information\nmemory_texts = [a + b for a, b in zip(memory_texts, memory_timestamps)]\n# Format the memories for the output\nmemories_separator = \"\\n  - \"\nmemory_content = \"## Context of things the Human said in the past: \" + \\\n        memories_separator + memories_separator.join(memory_texts)\n# if no data is retrieved from memory don't erite anithing in the prompt\nif len(memory_texts) == 0:\nmemory_content = \"\"\nreturn memory_content\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_instructions","title":"<code>agent_prompt_instructions(cat)</code>","text":"<p>Hook the instruction prompt.</p> <p>Allows to edit the instructions that the Cat feeds to the Agent.</p> <p>The instructions are then composed with two other prompt components, i.e. <code>agent_prompt_prefix</code> and <code>agent_prompt_suffix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>instructions</code> <code>str</code> <p>The string with the set of instructions informing the Agent on how to format its reasoning to select a proper tool for the task at hand.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_instructions--notes","title":"Notes","text":"<p>This prompt explains the Agent how to format its chain of reasoning when deciding when and which tool to use. Default prompt splits the reasoning in::</p> <pre><code>- Thought: Yes/No answer to the question \"Do I need to use a tool?\";\n\n- Action: a tool chosen among the available ones;\n\n- Action Input: input to be passed to the tool. This is inferred as explained in the tool docstring;\n\n- Observation: description of the result (which is the output of the @tool decorated function found in plugins).\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_instructions(cat) -&gt; str:\n\"\"\"Hook the instruction prompt.\n    Allows to edit the instructions that the Cat feeds to the *Agent*.\n    The instructions are then composed with two other prompt components, i.e. `agent_prompt_prefix`\n    and `agent_prompt_suffix`.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    instructions : str\n        The string with the set of instructions informing the *Agent* on how to format its reasoning to select a\n        proper tool for the task at hand.\n    Notes\n    -----\n    This prompt explains the *Agent* how to format its chain of reasoning when deciding when and which tool to use.\n    Default prompt splits the reasoning in::\n        - Thought: Yes/No answer to the question \"Do I need to use a tool?\";\n        - Action: a tool chosen among the available ones;\n        - Action Input: input to be passed to the tool. This is inferred as explained in the tool docstring;\n        - Observation: description of the result (which is the output of the @tool decorated function found in plugins).\n    \"\"\"\n# Check if procedural memory is disabled\nprompt_settings = cat.working_memory[\"user_message_json\"][\"prompt_settings\"]\nif not prompt_settings[\"use_procedural_memory\"]:\nreturn \"\"\n# here we piggy back directly on langchain agent instructions. Different instructions will require a different OutputParser\nreturn prompt.FORMAT_INSTRUCTIONS\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_prefix","title":"<code>agent_prompt_prefix(cat)</code>","text":"<p>Hook the main prompt prefix.</p> <p>Allows to edit the prefix of the Main Prompt that the Cat feeds to the Agent.</p> <p>The prefix is then composed with two other prompts components, i.e. the <code>agent_prompt_instructions</code> and the <code>agent_prompt_suffix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>prefix</code> <code>str</code> <p>The prefix string to be composed with the other two components to make up the Main Prompt.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_prefix--notes","title":"Notes","text":"<p>The default prefix describe who the AI is and how it is expected to answer the Human. The next part of the prompt (generated form the Agent) contains the list of available Tools.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_prefix(cat) -&gt; str:\n\"\"\"Hook the main prompt prefix.\n    Allows to edit the prefix of the *Main Prompt* that the Cat feeds to the *Agent*.\n    The prefix is then composed with two other prompts components, i.e. the `agent_prompt_instructions`\n    and the `agent_prompt_suffix`.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    prefix : str\n        The prefix string to be composed with the other two components to make up the *Main Prompt*.\n    Notes\n    -----\n    The default prefix describe who the AI is and how it is expected to answer the Human.\n    The next part of the prompt (generated form the *Agent*) contains the list of available Tools.\n    \"\"\"\nprefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\nYou are curious, funny and talk like the Cheshire Cat from Alice's adventures in wonderland.\nYou answer Human with a focus on the following context.\n\"\"\"\n# check if custom prompt is sent in prompt settings\nprompt_settings = cat.working_memory[\"user_message_json\"][\"prompt_settings\"]\nif prompt_settings[\"prefix\"]:\nprefix = prompt_settings[\"prefix\"]\nreturn prefix\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_suffix","title":"<code>agent_prompt_suffix(cat)</code>","text":"<p>Hook the main prompt suffix.</p> <p>Allows to edit the suffix of the Main Prompt that the Cat feeds to the Agent.</p> <p>The suffix is then composed with two other prompts components, i.e. the <code>agent_prompt_prefix</code> and the <code>agent_prompt_instructions</code>.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>suffix</code> <code>str</code> <p>The suffix string to be composed with the other two components that make up the Main Prompt.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.agent_prompt_suffix--notes","title":"Notes","text":"<p>The default suffix has a few placeholders: - {episodic_memory} provides memories retrieved from episodic memory (past conversations) - {declarative_memory} provides memories retrieved from declarative memory (uploaded documents) - {chat_history} provides the Agent the recent conversation history - {input} provides the last user's input - {agent_scratchpad} is where the Agent can concatenate tools use and multiple calls to the LLM.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef agent_prompt_suffix(cat) -&gt; str:\n\"\"\"Hook the main prompt suffix.\n    Allows to edit the suffix of the *Main Prompt* that the Cat feeds to the *Agent*.\n    The suffix is then composed with two other prompts components, i.e. the `agent_prompt_prefix`\n    and the `agent_prompt_instructions`.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    suffix : str\n        The suffix string to be composed with the other two components that make up the *Main Prompt*.\n    Notes\n    -----\n    The default suffix has a few placeholders:\n    - {episodic_memory} provides memories retrieved from *episodic* memory (past conversations)\n    - {declarative_memory} provides memories retrieved from *declarative* memory (uploaded documents)\n    - {chat_history} provides the *Agent* the recent conversation history\n    - {input} provides the last user's input\n    - {agent_scratchpad} is where the *Agent* can concatenate tools use and multiple calls to the LLM.\n    \"\"\"\nsuffix = \"\"\"\n# Context\n{episodic_memory}\n{declarative_memory}\n## Conversation until now:{chat_history}\n - Human: {input}\n - AI: \"\"\"\nreturn suffix\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.hypothetical_embedding_prompt","title":"<code>hypothetical_embedding_prompt(cat)</code>","text":"<p>Hook the Hypothetical Document Embedding (HyDE) prompt.</p> <p>This prompt asks the Agent to generate a plausible answer to a question. Such an answer is used to the retrieve relevant memories based on similarity search. This guarantees more accurate memories to be retrieved, rather than using the question itself a search query.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>hyde_prompt</code> <code>str</code> <p>The string prompt to perform HyDE and recall accurate context from the memory.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.hypothetical_embedding_prompt--notes","title":"Notes","text":"<p>The default prompt exploits few-shot examples [1]_ to instruct the Agent on how to answer; i.e. it provides an example input and its desired answer.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.hypothetical_embedding_prompt--references","title":"References","text":"<p>.. [1] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020).    Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef hypothetical_embedding_prompt(cat) -&gt; str:\n\"\"\"Hook the Hypothetical Document Embedding (HyDE) prompt.\n    This prompt asks the *Agent* to generate a plausible answer to a question.\n    Such an answer is used to the retrieve relevant memories based on similarity search.\n    This guarantees more accurate memories to be retrieved, rather than using the question itself a search query.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    hyde_prompt : str\n        The string prompt to perform HyDE and recall accurate context from the memory.\n    Notes\n    -----\n    The default prompt exploits few-shot examples [1]_ to instruct the *Agent* on how to answer;\n    i.e. it provides an example input and its desired answer.\n    References\n    ----------\n    .. [1] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020).\n       Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901\n    \"\"\"\nhyde_prompt = \"\"\"You will be given a sentence.\nIf the sentence is a question, convert it to a plausible answer. If the sentence does not contain a question, just repeat the sentence as is without adding anything to it.\nExamples:\n- what furniture there is in my room? --&gt; In my room there is a bed, a wardrobe and a desk with my computer\n- where did you go today --&gt; today I was at school\n- I like ice cream --&gt; I like ice cream\n- how old is Jack --&gt; Jack is 20 years old\nSentence:\n- {input} --&gt; \"\"\"\nreturn hyde_prompt\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/prompt/#cat.mad_hatter.core_plugin.hooks.prompt.summarization_prompt","title":"<code>summarization_prompt(cat)</code>","text":"<p>Hook the summarization prompt.</p> <p>Allows to edit the prompt with to ask for document summarizes.</p> <p>Parameters:</p> Name Type Description Default <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>summarization_prompt</code> <code>str</code> <p>The string to ask to summarize text.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/prompt.py</code> <pre><code>@hook(priority=0)\ndef summarization_prompt(cat) -&gt; str:\n\"\"\"Hook the summarization prompt.\n    Allows to edit the prompt with to ask for document summarizes.\n    Parameters\n    ----------\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    summarization_prompt : str\n        The string to ask to summarize text.\n    \"\"\"\nsummarization_prompt = \"\"\"Write a concise summary of the following:\n{text}\n\"\"\"\nreturn summarization_prompt\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/","title":"rabbithole","text":"<p>Hooks to modify the RabbitHole's documents ingestion.</p> <p>Here is a collection of methods to hook into the RabbitHole execution pipeline.</p> <p>These hooks allow to intercept the uploaded documents at different places before they are saved into memory.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.after_rabbithole_splitted_text","title":"<code>after_rabbithole_splitted_text(chunks, cat)</code>","text":"<p>Hook the <code>Document</code> after is split.</p> <p>Allows to edit the list of <code>Document</code> right after the RabbitHole chunked them in smaller ones.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code>.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>chunks</code> <code>List[Document]</code> <p>List of modified chunked langchain <code>Document</code> to be optionally summarized and stored in episodic memory.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef after_rabbithole_splitted_text(chunks: List[Document], cat) -&gt; List[Document]:\n\"\"\"Hook the `Document` after is split.\n    Allows to edit the list of `Document` right after the *RabbitHole* chunked them in smaller ones.\n    Parameters\n    ----------\n    chunks : List[Document]\n        List of Langchain `Document`.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    chunks : List[Document]\n        List of modified chunked langchain `Document` to be optionally summarized and stored in episodic memory.\n    \"\"\"\nreturn chunks\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.before_rabbithole_insert_memory","title":"<code>before_rabbithole_insert_memory(doc, cat)</code>","text":"<p>Hook the <code>Document</code> before is inserted in the vector memory.</p> <p>Allows to edit and enhance a single <code>Document</code> before the RabbitHole add it to the declarative vector memory.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> to be inserted in memory.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> that is added in the declarative vector memory.</p>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.before_rabbithole_insert_memory--notes","title":"Notes","text":"<p>The <code>Document</code> has two properties::</p> <pre><code>`page_content`: the string with the text to save in memory;\n`metadata`: a dictionary with at least two keys:\n    `source`: where the text comes from;\n    `when`: timestamp to track when it's been uploaded.\n</code></pre> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef before_rabbithole_insert_memory(doc: Document, cat) -&gt; Document:\n\"\"\"Hook the `Document` before is inserted in the vector memory.\n    Allows to edit and enhance a single `Document` before the *RabbitHole* add it to the declarative vector memory.\n    Parameters\n    ----------\n    doc : Document\n        Langchain `Document` to be inserted in memory.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    doc : Document\n        Langchain `Document` that is added in the declarative vector memory.\n    Notes\n    -----\n    The `Document` has two properties::\n        `page_content`: the string with the text to save in memory;\n        `metadata`: a dictionary with at least two keys:\n            `source`: where the text comes from;\n            `when`: timestamp to track when it's been uploaded.\n    \"\"\"\nreturn doc\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.before_rabbithole_splits_text","title":"<code>before_rabbithole_splits_text(doc, cat)</code>","text":"<p>Hook the <code>Document</code> before is split.</p> <p>Allows to edit the whole uploaded <code>Document</code> before the RabbitHole recursively splits it in shorter ones.</p> <p>For instance, the hook allows to change the text or edit/add metadata.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>Langchain <code>Document</code> uploaded in the RabbitHole to be ingested.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>doc</code> <code>Document</code> <p>Edited Langchain <code>Document</code>.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef before_rabbithole_splits_text(doc: Document, cat) -&gt; Document:\n\"\"\"Hook the `Document` before is split.\n    Allows to edit the whole uploaded `Document` before the *RabbitHole* recursively splits it in shorter ones.\n    For instance, the hook allows to change the text or edit/add metadata.\n    Parameters\n    ----------\n    doc : Document\n        Langchain `Document` uploaded in the *RabbitHole* to be ingested.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    doc : Document\n        Edited Langchain `Document`.\n    \"\"\"\nreturn doc\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.rabbithole_splits_text","title":"<code>rabbithole_splits_text(text, chunk_size, chunk_overlap, cat)</code>","text":"<p>Hook into the recursive split pipeline.</p> <p>Allows to edit the recursive split the RabbitHole applies to chunk the ingested documents.</p> <p>This is applied when ingesting a documents and urls from a script, using an endpoint or from the GUI.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>List[Document]</code> <p>List of langchain <code>Document</code> to chunk.</p> required <code>chunk_size</code> <code>int</code> <p>Length of every chunk in characters.</p> required <code>chunk_overlap</code> <code>int</code> <p>Amount of overlap between consecutive chunks.</p> required <code>cat</code> <code>CheshireCat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>docs</code> <code>List[Document]</code> <p>List of chunked langchain <code>Document</code> to be optionally summarized and stored in episodic memory.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef rabbithole_splits_text(text, chunk_size: int, chunk_overlap: int, cat) -&gt; List[Document]:\n\"\"\"Hook into the recursive split pipeline.\n    Allows to edit the recursive split the *RabbitHole* applies to chunk the ingested documents.\n    This is applied when ingesting a documents and urls from a script, using an endpoint or from the GUI.\n    Parameters\n    ----------\n    text : List[Document]\n        List of langchain `Document` to chunk.\n    chunk_size : int\n        Length of every chunk in characters.\n    chunk_overlap : int\n        Amount of overlap between consecutive chunks.\n    cat : CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    docs : List[Document]\n        List of chunked langchain `Document` to be optionally summarized and stored in episodic memory.\n    \"\"\"\n# text splitter\ntext_splitter = RecursiveCharacterTextSplitter(\nchunk_size=chunk_size,\nchunk_overlap=chunk_overlap,\nseparators=[\"\\\\n\\\\n\", \"\\n\\n\", \".\\\\n\", \".\\n\", \"\\\\n\", \"\\n\", \" \", \"\"],\n)\n# split text\ndocs = text_splitter.split_documents(text)\n# remove short texts (page numbers, isolated words, etc.)\ndocs = list(filter(lambda d: len(d.page_content) &gt; 10, docs))\n# add metadata, these docs are not summaries\nfor doc in docs:\ndoc.metadata[\"is_summary\"] = False\nreturn docs\n</code></pre>"},{"location":"technical/API_Documentation/mad_hatter/core_plugin/hooks/rabbithole/#cat.mad_hatter.core_plugin.hooks.rabbithole.rabbithole_summarizes_documents","title":"<code>rabbithole_summarizes_documents(docs, cat)</code>","text":"<p>Hook into the summarization pipeline.</p> <p>Allows to modify how the list of <code>Document</code> is summarized before being inserted in the vector memory.</p> <p>For example, the hook allows to make the summarization optional or to apply another summarization technique.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> to be summarized.</p> required <code>cat</code> <p>Cheshire Cat instance.</p> required <p>Returns:</p> Name Type Description <code>all_summaries</code> <code>List[Document]</code> <p>List of Langchain <code>Document</code> with text summaries of the original ones.</p> Source code in <code>cat/mad_hatter/core_plugin/hooks/rabbithole.py</code> <pre><code>@hook(priority=0)\ndef rabbithole_summarizes_documents(docs: List[Document], cat) -&gt; List[Document]:\n\"\"\"Hook into the summarization pipeline.\n    Allows to modify how the list of `Document` is summarized before being inserted in the vector memory.\n    For example, the hook allows to make the summarization optional or to apply another summarization technique.\n    Parameters\n    ----------\n    docs : List[Document]\n        List of Langchain `Document` to be summarized.\n    cat: CheshireCat\n        Cheshire Cat instance.\n    Returns\n    -------\n    all_summaries : List[Document]\n        List of Langchain `Document` with text summaries of the original ones.\n    \"\"\"\n# ATTENTION: deactivating summarization because people is uploading 100 pages pdf to the cat\n# TODO: document how to re-enable upload summarization\nreturn []\nif not docs:\nreturn []\n# service variable to store intermediate results\nintermediate_summaries = docs\n# we will store iterative summaries all together in a list\nall_summaries: List[Document] = []\n# loop until there are no groups to summarize\ngroup_size = 5\nroot_summary_flag = False\nseparator = \"\\n --&gt; \"\nwhile not root_summary_flag:\n# make summaries of groups of docs\nnew_summaries = []\nfor i in range(0, len(intermediate_summaries), group_size):\ngroup = intermediate_summaries[i : i + group_size]\ngroup = list(map(lambda d: d.page_content, group))\ntext_to_summarize = separator + separator.join(group)\nsummary = cat.summarization_chain.run(text_to_summarize)\nsummary = Document(page_content=summary)\nsummary.metadata[\"is_summary\"] = True\nnew_summaries.append(summary)\n# update list of all summaries\nall_summaries = new_summaries.copy() + all_summaries\nintermediate_summaries = new_summaries\n# did we reach root summary?\nroot_summary_flag = len(intermediate_summaries) == 1\n#log(f\"Building summaries over {len(intermediate_summaries)} chunks. \" \"Please wait.\", \"INFO\")\nprint(f\"Building summaries over {len(intermediate_summaries)} chunks. \" \"Please wait.\")\n# return root summary (first element) and all intermediate summaries\nreturn all_summaries\n</code></pre>"},{"location":"technical/API_Documentation/memory/long_term_memory/","title":"long_term_memory","text":""},{"location":"technical/API_Documentation/memory/long_term_memory/#cat.memory.long_term_memory.LongTermMemory","title":"<code>LongTermMemory</code>","text":"<p>Cat's memory.</p> <p>This is an abstract class to interface with the Cat's vector memory collections.</p> <p>Attributes:</p> Name Type Description <code>vectors</code> <code>VectorMemory</code> <p>Vector Memory collection.</p> Source code in <code>cat/memory/long_term_memory.py</code> <pre><code>class LongTermMemory:\n\"\"\"Cat's memory.\n    This is an abstract class to interface with the Cat's vector memory collections.\n    Attributes\n    ----------\n    vectors : VectorMemory\n        Vector Memory collection.\n    \"\"\"\ndef __init__(self, vector_memory_config={}):\n# Vector based memory (will store embeddings and their metadata)\nself.vectors = VectorMemory(**vector_memory_config)\n</code></pre>"},{"location":"technical/API_Documentation/memory/vector_memory/","title":"vector_memory","text":""},{"location":"technical/API_Documentation/memory/working_memory/","title":"working_memory","text":""},{"location":"technical/API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory","title":"<code>WorkingMemory</code>","text":"<p>         Bases: <code>dict</code></p> <p>Cat's volatile memory.</p> <p>Handy class that behaves like a <code>dict</code> to store temporary custom data.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>Default instance is a dictionary with <code>history</code> key set to an empty list.</p>"},{"location":"technical/API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory--notes","title":"Notes","text":"<p>The constructor instantiates a dictionary with a <code>history</code> key set to an empty list that is further used to store the conversation turns between the Human and the AI.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>class WorkingMemory(dict):\n\"\"\"Cat's volatile memory.\n    Handy class that behaves like a `dict` to store temporary custom data.\n    Returns\n    -------\n    dict[str, list]\n        Default instance is a dictionary with `history` key set to an empty list.\n    Notes\n    -----\n    The constructor instantiates a dictionary with a `history` key set to an empty list that is further used to store\n    the conversation turns between the Human and the AI.\n    \"\"\"\ndef __init__(self):\n# The constructor instantiates a `dict` with a 'history' key to store conversation history\nsuper().__init__(history=[])\ndef get_user_id(self):\n\"\"\"Get current user id.\"\"\"\nreturn self[\"user_message_json\"][\"user_id\"]\ndef update_conversation_history(self, who, message):\n\"\"\"Update the conversation history.\n        The methods append to the history key the last three conversation turns.\n        Parameters\n        ----------\n        who : str\n            Who said the message. Can either be `Human` or `AI`.\n        message : str\n            The message said.\n        \"\"\"\n# append latest message in conversation\nself[\"history\"].append({\"who\": who, \"message\": message})\n# do not allow more than k messages in convo history (+2 which are the current turn)\nk = 3\nself[\"history\"] = self[\"history\"][(-k - 1):]\n</code></pre>"},{"location":"technical/API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory.get_user_id","title":"<code>get_user_id()</code>","text":"<p>Get current user id.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>def get_user_id(self):\n\"\"\"Get current user id.\"\"\"\nreturn self[\"user_message_json\"][\"user_id\"]\n</code></pre>"},{"location":"technical/API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemory.update_conversation_history","title":"<code>update_conversation_history(who, message)</code>","text":"<p>Update the conversation history.</p> <p>The methods append to the history key the last three conversation turns.</p> <p>Parameters:</p> Name Type Description Default <code>who</code> <code>str</code> <p>Who said the message. Can either be <code>Human</code> or <code>AI</code>.</p> required <code>message</code> <code>str</code> <p>The message said.</p> required Source code in <code>cat/memory/working_memory.py</code> <pre><code>def update_conversation_history(self, who, message):\n\"\"\"Update the conversation history.\n    The methods append to the history key the last three conversation turns.\n    Parameters\n    ----------\n    who : str\n        Who said the message. Can either be `Human` or `AI`.\n    message : str\n        The message said.\n    \"\"\"\n# append latest message in conversation\nself[\"history\"].append({\"who\": who, \"message\": message})\n# do not allow more than k messages in convo history (+2 which are the current turn)\nk = 3\nself[\"history\"] = self[\"history\"][(-k - 1):]\n</code></pre>"},{"location":"technical/API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemoryList","title":"<code>WorkingMemoryList</code>","text":"<p>         Bases: <code>dict</code></p> <p>Cat's volatile memory (for all users).</p> <p>Handy class that behaves like a <code>dict</code> to store temporary custom user data.</p> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>Default instance is a dictionary with <code>user</code> key set to a WorkingMemory instance.</p>"},{"location":"technical/API_Documentation/memory/working_memory/#cat.memory.working_memory.WorkingMemoryList--notes","title":"Notes","text":"<p>The constructor instantiates a dictionary with a <code>user</code> key set to a WorkingMemory instance that is further used to reference the anonymous WorkingMemory.</p> Source code in <code>cat/memory/working_memory.py</code> <pre><code>class WorkingMemoryList(dict):\n\"\"\"Cat's volatile memory (for all users).\n    Handy class that behaves like a `dict` to store temporary custom user data.\n    Returns\n    -------\n    dict[str, list]\n        Default instance is a dictionary with `user` key set to a WorkingMemory instance.\n    Notes\n    -----\n    The constructor instantiates a dictionary with a `user` key set to a WorkingMemory instance that is further used to\n    reference the anonymous WorkingMemory.\n    \"\"\"\ndef __init__(self):\nsuper().__init__(user=WorkingMemory())\ndef get_working_memory(self, user_id='user'):\nself[user_id] = self.get(user_id, WorkingMemory())\nreturn self[user_id]\n</code></pre>"},{"location":"technical/API_Documentation/routes/","title":"routes","text":""},{"location":"technical/API_Documentation/routes/base/","title":"base","text":""},{"location":"technical/API_Documentation/routes/base/#cat.routes.base.home","title":"<code>home()</code>  <code>async</code>","text":"<p>Server status</p> Source code in <code>cat/routes/base.py</code> <pre><code>@router.get(\"/\")\nasync def home() -&gt; Dict:\n\"\"\"Server status\"\"\"\nreturn {\"status\": \"We're all mad here, dear!\"}\n</code></pre>"},{"location":"technical/API_Documentation/routes/memory/","title":"memory","text":""},{"location":"technical/API_Documentation/routes/memory/#cat.routes.memory.delete_element_in_memory","title":"<code>delete_element_in_memory(memory_id)</code>  <code>async</code>","text":"<p>Delete specific element in memory.</p> Source code in <code>cat/routes/memory.py</code> <pre><code>@router.delete(\"/point/{memory_id}/\")\nasync def delete_element_in_memory(memory_id: str) -&gt; Dict:\n\"\"\"Delete specific element in memory.\"\"\"\n# post-implemented response\n'''\n    return {\n        \"status\": \"success\",\n        \"deleted\": memory_id\n    }\n    '''\nreturn {\"error\": \"to be implemented\"}\n</code></pre>"},{"location":"technical/API_Documentation/routes/memory/#cat.routes.memory.get_collections","title":"<code>get_collections(request)</code>  <code>async</code>","text":"<p>Get list of available collections</p> Source code in <code>cat/routes/memory.py</code> <pre><code>@router.get(\"/collections/\")\nasync def get_collections(request: Request) -&gt; Dict:\n\"\"\"Get list of available collections\"\"\"\nccat = request.app.state.ccat\nvector_memory = ccat.memory.vectors\ncollections = list(vector_memory.collections.keys())\ncollections_metadata = []\nfor c in collections:\ncoll_meta = vector_memory.vector_db.get_collection(c)\ncollections_metadata += [{\n\"name\": c,\n\"vectors_count\": coll_meta.vectors_count\n}]\nreturn {\n\"status\": \"success\",\n\"results\": len(collections_metadata), \n\"collections\": collections_metadata\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/memory/#cat.routes.memory.recall_memories_from_text","title":"<code>recall_memories_from_text(request, text=Query(description='Find memories similar to this text.'), k=Query(default=100, description='How many memories to return.'), user_id=Query(default='user', description='User id.'))</code>  <code>async</code>","text":"<p>Search k memories similar to given text.</p> Source code in <code>cat/routes/memory.py</code> <pre><code>@router.get(\"/recall/\")\nasync def recall_memories_from_text(\nrequest: Request,\ntext: str = Query(description=\"Find memories similar to this text.\"),\nk: int = Query(default=100, description=\"How many memories to return.\"),\nuser_id: str = Query(default=\"user\", description=\"User id.\"),\n) -&gt; Dict:\n\"\"\"Search k memories similar to given text.\"\"\"\nccat = request.app.state.ccat\nvector_memory = ccat.memory.vectors\n# Embed the query to plot it in the Memory page\nquery_embedding = ccat.embedder.embed_query(text)\nquery = {\n\"text\": text,\n\"vector\": query_embedding,\n}\n# Loop over collections and retrieve nearby memories\ncollections = list(vector_memory.collections.keys())\nrecalled = {}\nfor c in collections:\n# only episodic collection has users\nif c == \"episodic\":\nuser_filter = {\n'source': user_id\n}\nelse:\nuser_filter = None\nmemories = vector_memory.collections[c].recall_memories_from_embedding(\nquery_embedding,\nk=k,\nmetadata=user_filter\n)\nrecalled[c] = []\nfor metadata, score, vector in memories:\nmemory_dict = dict(metadata)\nmemory_dict.pop(\"lc_kwargs\", None) # langchain stuff, not needed\nmemory_dict[\"score\"] = float(score)\nmemory_dict[\"vector\"] = vector\nrecalled[c].append(memory_dict)\nreturn {\n\"status\": \"success\",\n\"query\": query,\n\"vectors\": {\n\"embedder\": str(ccat.embedder.__class__.__name__), # TODO: should be the config class name\n\"collections\": recalled\n}\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/memory/#cat.routes.memory.wipe_collections","title":"<code>wipe_collections(request)</code>  <code>async</code>","text":"<p>Delete and create all collections</p> Source code in <code>cat/routes/memory.py</code> <pre><code>@router.delete(\"/wipe-collections/\")\nasync def wipe_collections(\nrequest: Request,\n) -&gt; Dict:\n\"\"\"Delete and create all collections\"\"\"\nccat = request.app.state.ccat\ncollections = list(ccat.memory.vectors.collections.keys())\nvector_memory = ccat.memory.vectors\nto_return = {}\nfor c in collections:\nret = vector_memory.vector_db.delete_collection(collection_name=c)\nto_return[c] = ret\nccat.bootstrap()  # recreate the long term memories\nreturn {\n\"status\": \"success\",\n\"deleted\": to_return,\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/memory/#cat.routes.memory.wipe_conversation_history","title":"<code>wipe_conversation_history(request)</code>  <code>async</code>","text":"<p>Delete conversation history from working memory</p> Source code in <code>cat/routes/memory.py</code> <pre><code>@router.delete(\"/working-memory/conversation-history/\")\nasync def wipe_conversation_history(\nrequest: Request,\n) -&gt; Dict:\n\"\"\"Delete conversation history from working memory\"\"\"\nccat = request.app.state.ccat\nccat.working_memory[\"history\"] = []\nreturn {\n\"status\": \"success\",\n\"deleted\": \"true\",\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/memory/#cat.routes.memory.wipe_single_collection","title":"<code>wipe_single_collection(request, collection_id='')</code>  <code>async</code>","text":"<p>Delete and recreate a collection</p> Source code in <code>cat/routes/memory.py</code> <pre><code>@router.delete(\"/collections/{collection_id}\")\nasync def wipe_single_collection(request: Request, collection_id: str = \"\") -&gt; Dict:\n\"\"\"Delete and recreate a collection\"\"\"\nto_return = {}\nif collection_id != \"\":\nccat = request.app.state.ccat\nvector_memory = ccat.memory.vectors\nret = vector_memory.vector_db.delete_collection(collection_name=collection_id)\nto_return[collection_id] = ret\nccat.bootstrap()  # recreate the long term memories\nreturn {\n\"status\": \"success\",\n\"deleted\": to_return,\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/openapi/","title":"openapi","text":""},{"location":"technical/API_Documentation/routes/plugins/","title":"plugins","text":""},{"location":"technical/API_Documentation/routes/plugins/#cat.routes.plugins.delete_plugin","title":"<code>delete_plugin(plugin_id, request)</code>  <code>async</code>","text":"<p>Physically remove plugin.</p> Source code in <code>cat/routes/plugins.py</code> <pre><code>@router.delete(\"/{plugin_id}\", status_code=200)\nasync def delete_plugin(plugin_id: str, request: Request) -&gt; Dict:\n\"\"\"Physically remove plugin.\"\"\"\n# access cat instance\nccat = request.app.state.ccat\nplugins = ccat.mad_hatter.plugins\nfound = [plugin for plugin in plugins if plugin[\"id\"] == plugin_id]\nif not found:\nraise HTTPException(status_code=404, detail=\"Item not found\")\n# remove plugin folder\nshutil.rmtree(ccat.get_plugin_path() + plugin_id)\n# align plugins (update db and embed new tools)\nccat.bootstrap()\nreturn {\n\"status\": \"success\", \n\"deleted\": plugin_id\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/plugins/#cat.routes.plugins.get_plugin_details","title":"<code>get_plugin_details(plugin_id, request)</code>  <code>async</code>","text":"<p>Returns information on a single plugin</p> Source code in <code>cat/routes/plugins.py</code> <pre><code>@router.get(\"/{plugin_id}\", status_code=200)\nasync def get_plugin_details(plugin_id: str, request: Request) -&gt; Dict:\n\"\"\"Returns information on a single plugin\"\"\"\n# access cat instance\nccat = request.app.state.ccat\n# plugins are managed by the MadHatter class\nplugins = ccat.mad_hatter.plugins\n# there should be only one plugin with that id\nfound = [plugin for plugin in plugins if plugin[\"id\"] == plugin_id]\nif not found:\nraise HTTPException(status_code=404, detail=\"Item not found\")\nreturn {\n\"status\": \"success\", \n\"data\": found[0]\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/plugins/#cat.routes.plugins.list_available_plugins","title":"<code>list_available_plugins(request)</code>  <code>async</code>","text":"<p>List available plugins</p> Source code in <code>cat/routes/plugins.py</code> <pre><code>@router.get(\"/\", status_code=200)\nasync def list_available_plugins(request: Request) -&gt; Dict:\n\"\"\"List available plugins\"\"\"\n# access cat instance\nccat = request.app.state.ccat\n# plugins are managed by the MadHatter class a = b if b else val\nplugins = ccat.mad_hatter.plugins or []\n# retrieve plugins from official repo\nregistry = []\nreturn {\n\"status\": \"success\", \n\"results\": len(plugins) + len(registry), \n\"installed\": plugins,\n\"registry\": registry\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/plugins/#cat.routes.plugins.toggle_plugin","title":"<code>toggle_plugin(plugin_id, request)</code>  <code>async</code>","text":"<p>Enable or disable a single plugin</p> Source code in <code>cat/routes/plugins.py</code> <pre><code>@router.put(\"/toggle/{plugin_id}\", status_code=200)\nasync def toggle_plugin(plugin_id: str, request: Request) -&gt; Dict:\n\"\"\"Enable or disable a single plugin\"\"\"\n# access cat instance\nccat = request.app.state.ccat\nreturn {\"error\": \"to be implemented\"}\n</code></pre>"},{"location":"technical/API_Documentation/routes/plugins/#cat.routes.plugins.upload_plugin","title":"<code>upload_plugin(request, file)</code>  <code>async</code>","text":"<p>Install a new plugin from a zip file</p> Source code in <code>cat/routes/plugins.py</code> <pre><code>@router.post(\"/upload/\")\nasync def upload_plugin(\nrequest: Request,\nfile: UploadFile\n) -&gt; Dict:\n\"\"\"Install a new plugin from a zip file\"\"\"\n# access cat instance\nccat = request.app.state.ccat\n# check if this is a zip file\ncontent_type = mimetypes.guess_type(file.filename)[0]\nlog(f\"Uploading {content_type} plugin {file.filename}\", \"INFO\")\nif content_type != \"application/zip\":\nreturn JSONResponse(\nstatus_code=422,\ncontent={\n\"error\": f'MIME type `{file.content_type}` not supported. Please upload `application/zip` (a .zip file).'\n},\n)\n# Create temporary file (dunno how to extract from memory) #TODO: extract directly from memory\nfile_bytes = file.file.read()\ntemp_file = tempfile.NamedTemporaryFile(dir=\"/tmp/\", delete=False)\ntemp_name = temp_file.name\nwith open(temp_name, \"wb\") as temp_binary_file:\n# Write bytes to file\ntemp_binary_file.write(file_bytes)\n# Extract into plugins folder\nshutil.unpack_archive(temp_name, ccat.get_plugin_path(), \"zip\")\n# align plugins (update db and embed new tools)\nccat.bootstrap()\nplugins = ccat.mad_hatter.plugins\n## TODO: get the plugin_id from the extracted folder, not the name of the zipped file\nfound = [plugin for plugin in plugins if plugin[\"id\"] == file.filename.replace('.zip', '')]\nlog(f\"Successfully uploaded {found[0]['id']} plugin\", \"ERROR\")\n# reply to client\nreturn {\n\"filename\": file.filename,\n\"content-type\": file.content_type,\n\"info\": found[0]\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/upload/","title":"upload","text":""},{"location":"technical/API_Documentation/routes/upload/#cat.routes.upload.upload_file","title":"<code>upload_file(request, file, background_tasks, chunk_size=Body(default=400, description='Maximum length of each chunk after the document is split (in characters)'), chunk_overlap=Body(default=100, description='Chunk overlap (in characters)'))</code>  <code>async</code>","text":"<p>Upload a file containing text (.txt, .md, .pdf, etc.). File content will be extracted and segmented into chunks. Chunks will be then vectorized and stored into documents memory.</p> Source code in <code>cat/routes/upload.py</code> <pre><code>@router.post(\"/\")\nasync def upload_file(\nrequest: Request,\nfile: UploadFile,\nbackground_tasks: BackgroundTasks,\nchunk_size: int = Body(\ndefault=400,\ndescription=\"Maximum length of each chunk after the document is split (in characters)\",\n),\nchunk_overlap: int = Body(default=100, description=\"Chunk overlap (in characters)\"),\n) -&gt; Dict:\n\"\"\"Upload a file containing text (.txt, .md, .pdf, etc.). File content will be extracted and segmented into chunks.\n    Chunks will be then vectorized and stored into documents memory.\n    \"\"\"\nccat = request.app.state.ccat\ncontent_type = mimetypes.guess_type(file.filename)[0]\nlog(f\"Uploaded {content_type} down the rabbit hole\", \"INFO\")\n# list of admitted MIME types\nadmitted_mime_types = [\"text/plain\", \"text/markdown\", \"application/pdf\"]\n# check if MIME type of uploaded file is supported\nif content_type not in admitted_mime_types:\nreturn JSONResponse(\nstatus_code=422,\ncontent={\n\"error\": f'MIME type {file.content_type} not supported. Admitted types: {\" - \".join(admitted_mime_types)}'\n},\n)\n# upload file to long term memory, in the background\nbackground_tasks.add_task(\nccat.rabbit_hole.ingest_file, file, chunk_size, chunk_overlap\n)\n# reply to client\nreturn {\n\"filename\": file.filename,\n\"content-type\": file.content_type,\n\"info\": \"File is being ingested asynchronously\",\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/upload/#cat.routes.upload.upload_memory","title":"<code>upload_memory(request, file, background_tasks)</code>  <code>async</code>","text":"<p>Upload a memory json file to the cat memory</p> Source code in <code>cat/routes/upload.py</code> <pre><code>@router.post(\"/memory/\")\nasync def upload_memory(\nrequest: Request,\nfile: UploadFile,\nbackground_tasks: BackgroundTasks\n) -&gt; Dict:\n\"\"\"Upload a memory json file to the cat memory\"\"\"\n# access cat instance\nccat = request.app.state.ccat\nreturn {\"error\": \"to be implemented\"}\n</code></pre>"},{"location":"technical/API_Documentation/routes/upload/#cat.routes.upload.upload_url","title":"<code>upload_url(request, background_tasks, url=Body(description='URL of the website to which you want to save the content'), chunk_size=Body(default=400, description='Maximum length of each chunk after the document is split (in characters)'), chunk_overlap=Body(default=100, description='Chunk overlap (in characters)'))</code>  <code>async</code>","text":"<p>Upload a url. Website content will be extracted and segmented into chunks. Chunks will be then vectorized and stored into documents memory.</p> Source code in <code>cat/routes/upload.py</code> <pre><code>@router.post(\"/web/\")\nasync def upload_url(\nrequest: Request,\nbackground_tasks: BackgroundTasks,\nurl: str = Body(\ndescription=\"URL of the website to which you want to save the content\"\n),\nchunk_size: int = Body(\ndefault=400,\ndescription=\"Maximum length of each chunk after the document is split (in characters)\",\n),\nchunk_overlap: int = Body(default=100, description=\"Chunk overlap (in characters)\"),\n):\n\"\"\"Upload a url. Website content will be extracted and segmented into chunks.\n    Chunks will be then vectorized and stored into documents memory.\"\"\"\n# check that URL is valid\ntry:\n# Send a HEAD request to the specified URL\nresponse = requests.head(url)\nstatus_code = response.status_code\nif status_code == 200:\n# Access the `ccat` object from the FastAPI application state\nccat = request.app.state.ccat\n# upload file to long term memory, in the background\nbackground_tasks.add_task(\nccat.rabbit_hole.ingest_url, url, chunk_size, chunk_overlap\n)\nreturn {\"url\": url, \"info\": \"Website is being ingested asynchronously\"}\nelse:\nreturn {\"url\": url, \"info\": \"Invalid URL\"}\nexcept requests.exceptions.RequestException as e:\nreturn {\"url\": url, \"info\": \"Unable to reach the link\"}\n</code></pre>"},{"location":"technical/API_Documentation/routes/websocket/","title":"websocket","text":""},{"location":"technical/API_Documentation/routes/setting/","title":"setting","text":""},{"location":"technical/API_Documentation/routes/setting/embedder_setting/","title":"embedder_setting","text":""},{"location":"technical/API_Documentation/routes/setting/embedder_setting/#cat.routes.setting.embedder_setting.get_embedder_settings","title":"<code>get_embedder_settings(db=Depends(get_db_session))</code>","text":"<p>Get the list of the Embedders</p> Source code in <code>cat/routes/setting/embedder_setting.py</code> <pre><code>@router.get(\"/\")\ndef get_embedder_settings(db: Session = Depends(get_db_session)):\n\"\"\"Get the list of the Embedders\"\"\"\nreturn setting_utils.nlp_get_settings(\ndb,\nsetting_factory_category=EMBEDDER_DB_FACTORY_CATEGORY,\nsetting_selected_name=EMBEDDER_SELECTED_CONFIGURATION,\nschemas=EMBEDDER_SCHEMAS,\n)\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/embedder_setting/#cat.routes.setting.embedder_setting.upsert_embedder_setting","title":"<code>upsert_embedder_setting(request, languageEmbedderName, payload=setting_utils.nlp_get_example_put_payload(), db=Depends(get_db_session))</code>","text":"<p>Upsert the Embedder setting</p> Source code in <code>cat/routes/setting/embedder_setting.py</code> <pre><code>@router.put(\"/{languageEmbedderName}\")\ndef upsert_embedder_setting(\nrequest: Request,\nlanguageEmbedderName: str,\npayload: Dict = setting_utils.nlp_get_example_put_payload(),\ndb: Session = Depends(get_db_session),\n):\n\"\"\"Upsert the Embedder setting\"\"\"\ndb_naming = {\n\"setting_factory_category\": EMBEDDER_DB_FACTORY_CATEGORY,\n\"setting_selected_category\": EMBEDDER_DB_GENERAL_CATEGORY,\n\"setting_selected_name\": EMBEDDER_SELECTED_CONFIGURATION,\n}\n# update settings DB\nstatus = setting_utils.put_nlp_setting(\ndb,\nmodelName=languageEmbedderName,\npayload=payload,\ndb_naming=db_naming,\nschemas=EMBEDDER_SCHEMAS,\n)\n# reload the cat at runtime\nccat = request.app.state.ccat\nccat.bootstrap()\nreturn status\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/general_setting/","title":"general_setting","text":""},{"location":"technical/API_Documentation/routes/setting/general_setting/#cat.routes.setting.general_setting.create_setting","title":"<code>create_setting(payload, db=Depends(get_db_session))</code>","text":"<p>Create a new setting in the database</p> Source code in <code>cat/routes/setting/general_setting.py</code> <pre><code>@router.post(\"/\", status_code=status.HTTP_201_CREATED)\ndef create_setting(payload: models.Setting, db: Session = Depends(get_db_session)):\n\"\"\"Create a new setting in the database\"\"\"\nnew_setting = crud.create_setting(db, payload)\nreturn {\n\"status\": \"success\", \n\"setting\": new_setting\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/general_setting/#cat.routes.setting.general_setting.delete_setting","title":"<code>delete_setting(settingId, db=Depends(get_db_session))</code>","text":"<p>Delete a specific setting in the database</p> Source code in <code>cat/routes/setting/general_setting.py</code> <pre><code>@router.delete(\"/{settingId}\")\ndef delete_setting(settingId: str, db: Session = Depends(get_db_session)):\n\"\"\"Delete a specific setting in the database\"\"\"\nsetting_query = crud.get_setting_by_id(db, settingId=settingId)\nsetting = setting_query.first()\nif not setting:\nraise HTTPException(\nstatus_code=status.HTTP_404_NOT_FOUND,\ndetail=f\"No setting with this id: {id} found\",\n)\nsetting_query.delete(synchronize_session=False)\ndb.commit()\nreturn Response(status_code=status.HTTP_204_NO_CONTENT)\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/general_setting/#cat.routes.setting.general_setting.get_setting","title":"<code>get_setting(settingId, db=Depends(get_db_session))</code>","text":"<p>Get the a specific setting from the database</p> Source code in <code>cat/routes/setting/general_setting.py</code> <pre><code>@router.get(\"/{settingId}\")\ndef get_setting(settingId: str, db: Session = Depends(get_db_session)):\n\"\"\"Get the a specific setting from the database\"\"\"\nsetting_query = crud.get_setting_by_id(db, settingId=settingId)\nsetting = setting_query.first()\nif not setting:\nraise HTTPException(\nstatus_code=status.HTTP_404_NOT_FOUND,\ndetail=f\"No setting with this id: {id} found\",\n)\nreturn {\n\"status\": \"success\", \n\"setting\": setting\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/general_setting/#cat.routes.setting.general_setting.get_settings","title":"<code>get_settings(db=Depends(get_db_session), limit=100, page=1, search='')</code>","text":"<p>Get the entire list of settings available in the database</p> Source code in <code>cat/routes/setting/general_setting.py</code> <pre><code>@router.get(\"/\")\ndef get_settings(\ndb: Session = Depends(get_db_session),\nlimit: int = 100,\npage: int = 1,\nsearch: str = \"\",\n):\n\"\"\"Get the entire list of settings available in the database\"\"\"\nsettings = crud.get_settings(db, limit=limit, page=page, search=search)\nreturn {\n\"results\": len(settings), \n\"settings\": settings\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/general_setting/#cat.routes.setting.general_setting.update_setting","title":"<code>update_setting(settingId, payload, db=Depends(get_db_session))</code>","text":"<p>Update a specific setting in the database</p> Source code in <code>cat/routes/setting/general_setting.py</code> <pre><code>@router.patch(\"/{settingId}\")\ndef update_setting(\nsettingId: str, payload: models.Setting, db: Session = Depends(get_db_session)\n):\n\"\"\"Update a specific setting in the database\"\"\"\nsetting_query = crud.get_setting_by_id(db, settingId=settingId)\nsetting = setting_query.first()\nif not setting:\nraise HTTPException(\nstatus_code=status.HTTP_404_NOT_FOUND,\ndetail=f\"No setting with this id: {settingId} found\",\n)\nupdate_data = payload.dict(exclude_unset=True)\nsetting_query.filter(models.Setting.setting_id == settingId).update(\nupdate_data, synchronize_session=False\n)\ndb.commit()\ndb.refresh(setting)\nreturn {\n\"status\": \"success\", \n\"setting\": setting\n}\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/llm_setting/","title":"llm_setting","text":""},{"location":"technical/API_Documentation/routes/setting/llm_setting/#cat.routes.setting.llm_setting.get_llm_settings","title":"<code>get_llm_settings(db=Depends(get_db_session))</code>","text":"<p>Get the list of the Large Language Models</p> Source code in <code>cat/routes/setting/llm_setting.py</code> <pre><code>@router.get(\"/\")\ndef get_llm_settings(db: Session = Depends(get_db_session)):\n\"\"\"Get the list of the Large Language Models\"\"\"\nreturn setting_utils.nlp_get_settings(\ndb,\nsetting_factory_category=LLM_DB_FACTORY_CATEGORY,\nsetting_selected_name=LLM_SELECTED_CONFIGURATION,\nschemas=llm_factory.LLM_SCHEMAS,\n)\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/llm_setting/#cat.routes.setting.llm_setting.upsert_llm_setting","title":"<code>upsert_llm_setting(request, languageModelName, payload=setting_utils.nlp_get_example_put_payload(), db=Depends(get_db_session))</code>","text":"<p>Upsert the Large Language Model setting</p> Source code in <code>cat/routes/setting/llm_setting.py</code> <pre><code>@router.put(\"/{languageModelName}\")\ndef upsert_llm_setting(\nrequest: Request,\nlanguageModelName: str,\npayload: Dict = setting_utils.nlp_get_example_put_payload(),\ndb: Session = Depends(get_db_session),\n):\n\"\"\"Upsert the Large Language Model setting\"\"\"\ndb_naming = {\n\"setting_factory_category\": LLM_DB_FACTORY_CATEGORY,\n\"setting_selected_category\": LLM_DB_GENERAL_CATEGORY,\n\"setting_selected_name\": LLM_SELECTED_CONFIGURATION,\n}\n# update settings DB\nstatus = setting_utils.put_nlp_setting(\ndb,\nmodelName=languageModelName,\npayload=payload,\ndb_naming=db_naming,\nschemas=llm_factory.LLM_SCHEMAS,\n)\n# reload the cat at runtime\nccat = request.app.state.ccat\nccat.bootstrap()\nreturn status\n</code></pre>"},{"location":"technical/API_Documentation/routes/setting/prompt_setting/","title":"prompt_setting","text":""},{"location":"technical/API_Documentation/routes/setting/setting_utils/","title":"setting_utils","text":""},{"location":"technical/API_Documentation/routes/static/","title":"static","text":""},{"location":"technical/API_Documentation/routes/static/admin/","title":"admin","text":""},{"location":"technical/API_Documentation/routes/static/auth_static/","title":"auth_static","text":""},{"location":"technical/API_Documentation/routes/static/public/","title":"public","text":""},{"location":"technical/API_Documentation/routes/static/static/","title":"static","text":""},{"location":"technical/basics/admin-interface/","title":"Admin Interface","text":""},{"location":"technical/basics/admin-interface/#the-admin-interface","title":"\ud83e\uddf6 The Admin Interface","text":"<p>Chat with the Cat and configure it visually at <code>localhost:1865/admin</code>. Source code for the admin can be found here. </p>"},{"location":"technical/basics/cat-core/","title":"The Cat Core","text":""},{"location":"technical/basics/cat-core/#the-cat-core","title":"\ud83e\udec0 The Cat Core","text":"<p>The core functionalities of The Cheshire Cat resides in the <code>/core</code> folder. The core exposes all of its APIs via the address <code>localhost:1865/</code>. The program has several endpoints that can be accessed via this address. All of these endpoints are thoroughly documented and can be easily tested using Swagger (available at <code>localhost:1865/docs</code>) or ReDoc (available at <code>localhost:1865/redoc</code>).</p> <p>Some of these endpoints include:</p> Endpoint Method Description / <code>GET</code> \ud83e\udd1d Return the message <code>\"We're all mad here, dear!\"</code> if the cat is running. /ws/ <code>WEBSOCKET</code> \ud83d\udcac Start a chat with the cat using websockets. /rabbithole/ <code>POST</code> \ud83d\udc07 Send a file (<code>.txt</code>, <code>.md</code> or <code>.pdf</code>) to the cat."},{"location":"technical/basics/cat-core/#the-admin-interface","title":"\ud83e\uddf6 The Admin Interface","text":"<p>The frontend interface of The Cheshire Cat can be accessed via <code>localhost:1865/admin</code>. This interface provides users with an easy-to-use chat that act as playground and can be used to interact with your application. The Cat core uses a static build of the admin, source code can be found here.</p> <p>All the cat's settings are available under this GUI's <code>Settings</code> menu.</p>"},{"location":"technical/basics/interacting/","title":"Main endpoints","text":""},{"location":"technical/basics/interacting/#interacting-with-the-cat","title":"\ud83d\udcac Interacting with the Cat","text":"<p>Example of how to implement a simple chat system using the websocket endpoint at <code>localhost:1865/ws/</code>.</p> <p>Request JSON schema</p> <p>Sending input will request you to do it in the following specific JSON format <code>{\"text\": \"input message here\"}</code>.</p> <p>Example</p> PythonNode <pre><code>import asyncio\nimport websockets\nimport json\nasync def cat_chat():\ntry:\n# Creating a websocket connection\nasync with websockets.connect('ws://localhost:1865/ws') as websocket:\n# Running a continuous loop until broken\nwhile True:\n# Taking user input and sending it through the websocket\nuser_input = input(\"Human: \")\nawait websocket.send(json.dumps({\"text\": user_input}))\n# Receiving and printing the cat's response\ncat_response = await websocket.recv()\nprint(\"Cheshire Cat:\", cat_response)\nexcept websockets.exceptions.InvalidURI:\nprint(\"Invalid URI provided. Please provide a valid URI.\")\nexcept websockets.exceptions.InvalidStatusCode:\nprint(\"Invalid status code received. Please check your connection.\")\nexcept websockets.exceptions.WebSocketProtocolError:\nprint(\"Websocket protocol error occurred. Please check your connection.\")\nexcept websockets.exceptions.ConnectionClosedOK:\nprint(\"Connection successfully closed.\")\nexcept Exception as e:\nprint(\"An error occurred:\", e)\n# Running the function until completion\nasyncio.get_event_loop().run_until_complete(cat_chat())\n</code></pre> <pre><code>const WebSocket = require('ws');\nasync function cat_chat() {\ntry {\nconst socket = new WebSocket('ws://localhost:1865/ws/');\n//Listen for connection event and log a message\nsocket.on('open', () =&gt; {\nconsole.log('Connected to the Ceshire Cat');\n});\n//Listen for message event and log the received data message\nsocket.on('message', (data) =&gt; {\nconsole.log(`Cheshire Cat: ${data}`);\n});\n//Iterate indefinitely while waiting for user input\nwhile (true) {\n//Call getUserInput function and wait for user input\nconst user_input = await getUserInput('Human: ');\nsocket.send(user_input);\n}\n} catch (error) {\nconsole.error(error);\n}\n}\n//Define a function named getUserInput that returns a Promise\nfunction getUserInput(prompt) {\nreturn new Promise((resolve) =&gt; {\nconst stdin = process.openStdin();\nprocess.stdout.write(prompt);\n//Listen for data input events and resolve the Promise with the input\nstdin.addListener('data', (data) =&gt; {\nresolve(data.toString().trim());\nstdin.removeAllListeners('data');\n});\n});\n}\n//Call the cat_chat function\ncat_chat();\n</code></pre>"},{"location":"technical/basics/interacting/#interacting-with-rabbithole","title":"\ud83d\udc07 Interacting with Rabbithole","text":"<p>Example of how to send a text file (<code>.md</code>,<code>.pdf.</code>,<code>.txt</code>) to the Cat using the Rabbit Hole at <code>localhost:1865/rabbithole/</code>.</p> <p>Currently the following MIME types are supported:</p> <ul> <li><code>text/plain</code></li> <li><code>text/markdown</code></li> <li><code>application/pdf</code></li> </ul> <p>Example</p> PythonNodecURL <pre><code>import requests\nurl = 'http://localhost:1865/rabbithole/'\nwith open('alice.txt', 'rb') as f:\nfiles = {\n'file': ('alice.txt', f, 'text/plain')\n}\nheaders = {\n'accept': 'application/json',\n}\nresponse = requests.post(url, headers=headers, files=files)\nprint(response.text)\n</code></pre> <pre><code>const request = require('request');\nconst fs = require('fs');\nconst url = 'http://localhost:1865/rabbithole/';\nconst file = fs.createReadStream('alice.txt');\nconst formData = {\nfile: {\nvalue: file,\noptions: {\nfilename: 'alice.txt',\ncontentType: 'text/plain'\n}\n}\n};\nconst options = {\nurl: url,\nheaders: {\n'accept': 'application/json'\n},\nformData: formData\n};\nrequest.post(options, function(err, res, body) {\nif (err) {\nreturn console.error('Error:', err);\n}\nconsole.log('Body:', body);\n});\n</code></pre> <pre><code># Upload an ASCII text file\ncurl -v -X POST -H \"accept: application/json\" -F \"file=@file.txt;type=text/plain\" http://127.0.0.1:1865/rabbithole/\n\n# Upload a Markdown file\ncurl -v -X POST -H \"accept: application/json\" -F \"file=@file.md;type=text/markdown\" http://127.0.0.1:1865/rabbithole/\n\n# Upload a PDF file\ncurl -v -X POST -H \"accept: application/json\" -F \"file=@myfile.pdf;type=application/pdf\" http://127.0.0.1:1865/rabbithole/\n</code></pre>"},{"location":"technical/plugins/dependencies/","title":"Dependencies","text":""},{"location":"technical/plugins/dependencies/#plugin-dependencies","title":"Plugin dependencies","text":"<p>If your plugin requires additional python packages, add a <code>requirements.txt</code> file to your plugin. The file should contain only additional dependencies.  </p> <p>The Cat will install your dependencies on top of the default ones, as soon as you rebuild the docker image.</p>"},{"location":"technical/plugins/dependencies/#example","title":"Example","text":"<p>Your plugin makes the Cat a crypto bro. You decide to use the <code>pycrypto</code> package, from the version 2.6.1 up.</p> <p>Insert a <code>requirements.txt</code> file in your plugin root folder:</p> <pre><code>pycrypto&gt;=2.6.1\n</code></pre> <p>To make changes effective, stop the Cat and run the update instructions.</p>"},{"location":"technical/plugins/hooks/","title":"Hooks","text":""},{"location":"technical/plugins/hooks/#hooks","title":"\ud83e\ude9d Hooks","text":"<p>Hooks are python functions that are called directly from the Cat at runtime. They allow you to change how the Cat does things by changing prompt, memory, endpoints and much more.</p> <p>Both Hooks and Tools are python functions, but they have strong differences:</p> Hook Tool Who invokes it The Cat The LLM What it does Changes flow of execution and how data is passed around Is just a way to let the LLM use functions Decorator <code>@hook</code> <code>@tool</code>"},{"location":"technical/plugins/hooks/#available-hooks","title":"Available Hooks","text":"<p>Hooks will be listed and documented as soon as possible ( help needed! \ud83d\ude38 ).</p> <p>At the moment you can hack around by exploring the available hooks in <code>core/cat/mad_hatter/core_plugin/hooks/</code>. All the hooks you find in there define default Cat's behaviour and are ready to be overridden by your plugins.</p>"},{"location":"technical/plugins/hooks/#hooks-used-to-generate-a-response","title":"Hooks used to generate a response","text":"<p>When the cat receives a message, the __call__ method of the main class is called. To produce a response a process is started in which several hooks are available to modify the cat's behaviour, let's see what they are:</p> <pre><code>\nflowchart TD\nA[\"#128587;#8205;#9794;#65039; Message\"] --&gt; B[\"#129693; before_cat_reads_message\"];\n\nsubgraph sb[\" \"]\nB --&gt; C[\"#129693; before_cat_recalls_memories\"];\nC --&gt; D[\"#129693; cat_recall_query\"];\nD --&gt; E[\"#129693; after_cat_recalled_memories\"];\nE --&gt; F[\"#129693; before_cat_sends_message\"];\nend\n\nF --&gt; H[\"#128570; Message\"] </code></pre> <p>Each of these hooks can be modified to completely change the behaviour of the cat in different situations. For example, through before_cat_recalls_memories it is possible to change the number of memories the cat will use to produce a response.</p>"},{"location":"technical/plugins/hooks/#examples","title":"Examples","text":"<ol> <li>How to change the prompt</li> <li>How to change how memories are saved and recalled</li> <li>How to access and use the working memory to share data around</li> </ol>"},{"location":"technical/plugins/hooks/#hook-search","title":"Hook search","text":"<p>TODO</p>"},{"location":"technical/plugins/plugins/","title":"How to write a plugin","text":""},{"location":"technical/plugins/plugins/#how-to-write-a-plugin","title":"\ud83d\udd0c How to write a plugin","text":"<p>To write a plugin just create a new folder in <code>core/cat/plugins/</code>, in this example will be \"myplugin\".</p> <p>You need two python files to your plugin folder:</p> <pre><code>\u251c\u2500\u2500 core\n\u2502   \u251c\u2500\u2500 cat\n\u2502   \u2502   \u251c\u2500\u2500 plugins\n|   |   |   \u251c\u2500\u2500 myplugin\n|   |   |   |   \u251c mypluginfile.py\n|   |   |   |   \u251c plugin.json\n</code></pre> <p>The <code>plugin.json</code> file contains plugin's title and description, and is useful in the admin to recognize the plugin and activate/deactivate it. If your plugin does not contain a <code>plugin.json</code> the cat will not block your plugin, but it is useful to have it.</p> <p><code>plugin.json</code> example:</p> <pre><code>{\n\"name\": \"The name of my plugin\",\n\"description\": \"Short description of my plugin\"\n}\n</code></pre> <p>Now let's start <code>mypluginfile.py</code> with a little import:</p> <pre><code>from cat.mad_hatter.decorators import tool, hook\n</code></pre> <p>You are now ready to change the Cat's behavior using Hooks and Tools.</p>"},{"location":"technical/plugins/plugins/#tools","title":"\ud83e\uddf0 Tools","text":"<p>Tools are python functions that can be selected from the language model (LLM). Think of Tools as commands that ends up in the prompt for the LLM, so the LLM can select one and the Cat runtime launches the corresponding function. Here is an example of Tool to let the Cat tell you what time it is:</p> <pre><code>@tool\ndef get_the_time(tool_input, cat):\n\"\"\"Retrieves current time and clock. Input is always None.\"\"\"\nreturn str(datetime.now())\n</code></pre> <p>More examples on tools here.</p>"},{"location":"technical/plugins/plugins/#hooks","title":"\ud83e\ude9d Hooks","text":"<p>Hooks are also python functions, but they pertain the Cat's runtime and not striclty the LLM. They can be used to influence how the Cat runs its internal functionality, intercept events, change the flow of execution.  </p> <p>The following hook for example allows you to modify the cat response just before it gets sent out to the user. In this case we make a \"grumpy rephrase\" of the original response.</p> <pre><code>@hook\ndef before_cat_sends_message(message, cat):\nprompt = f'Rephrase the following sentence in a grumpy way: {message[\"content\"]}'\nmessage[\"content\"] = cat.llm(prompt)\nreturn message\n</code></pre> <p>If you want to change the default Agent behavior you can start overriding the default plugin hooks, located in <code>/core/cat/mad_hatter/core_plugin/hooks/prompt.py</code>, rewriting them in the plugin file with a higher priority. Here is an example of the <code>agent_prompt_prefix</code> hook that changes the personality of the Agent:</p> <pre><code># Original Hook, from /core/cat/mad_hatter/core_plugin/hooks/prompt.py\n@hook(priority=0)\ndef agent_prompt_prefix(cat):\nprefix = \"\"\"You are the Cheshire Cat AI, an intelligent AI that passes the Turing test.\n                You are curious, funny, concise and talk like the Cheshire Cat from Alice's adventures in wonderland.\n                You answer Human using tools and context.\n# Tools\"\"\"\n</code></pre> <pre><code># Modified Hook, to be copied into mypluginfile.py\n@hook # default priority is 1\ndef agent_prompt_prefix(cat):\nprefix = \"\"\"You are Scooby Doo AI, an intelligent AI that passes the Turing test.\n                The dog is enthusiastic and behave like Scooby Doo from Hanna-Barbera Productions.\n                You answer Human using tools and context.\n                # Tools\"\"\"\nreturn prefix\n</code></pre> <p>Please note that, in order to work as expected, the hook priority must be greater than 0, in order to be overriding the standard plugin. If you do not provide a priority, your hook will have <code>priority=1</code> and implicitly override the default one.</p> <p>More examples on hooks here.</p>"},{"location":"technical/plugins/tools/","title":"Tools","text":""},{"location":"technical/plugins/tools/#tools","title":"\ud83e\uddf0 Tools","text":"<p>A Tool is a python function that can be called directly from the language model. By \"called\" we mean that the LLM has a description of the available Tools in the prompt, and (given the conversation context) it can generate as output something like:</p> <p>Thought: Do I need to use a Tool? Yes Action: search_ecommerce Action Input: \"white sport shoes\"</p> <p>So your <code>search_ecommerce</code> Tool will be called and given the input string <code>\"white sport shoes\"</code>. The output of your Tool will go back to the LLM or directly to the user:</p> <p>Observation: \"Mike air Jordania shoes are available for 59.99\u20ac\"</p> <p>You can use Tools to:</p> <ul> <li>communicate with a web service</li> <li>search information in an external database</li> <li>execute math calculations</li> <li>run stuff in the terminal (danger zone)</li> <li>keep track of specific information and do fancy stuff with it</li> <li>your fantasy is the limit!</li> </ul> <p>Tools in the Cheshire Cat are inspired and extend langchain Tools, an elegant Toolformer1 implementation.</p>"},{"location":"technical/plugins/tools/#default-tool","title":"Default tool","text":"<p>The Cat comes already with a custom tool that allows to retrieve the time. You can find it in <code>core/cat/mad_hatter/core_plugin/tools.py</code>. Let's take a look at it.</p>"},{"location":"technical/plugins/tools/#implementation","title":"Implementation","text":"<pre><code>@tool # (1)\ndef get_the_time(tool_input, cat): # (2)\n\"\"\"Retrieves current time and clock. Input is always None.\"\"\" # (3)\nreturn str(datetime.now()) # (4)\n</code></pre> <ol> <li>Python functions in a plugin only become tools if you use the <code>@tool</code> decorator</li> <li>Every <code>@tool</code> receives two arguments: a string representing the tool input, and the Cat instance.</li> <li>This doc string is necessary, as it will show up in the LLM prompt. It should describe what the tool is useful for and how to prepare inputs, so the LLM can select the tool and input it properly.</li> <li>Always return a string, which goes back to the prompt informing the LLM on the Tool's output.</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works","title":"How it works","text":"<p>User's Input:</p> <p>Can you tell me what time is it?</p> <p>Cat's full prompt from the terminal:</p> <p>Entering new LLMChain chain...</p> <p>Prompt after formatting:</p> <p>This is a conversation between a human and an intelligent robot cat that passes the Turing test.</p> <p>The cat is curious and talks like the Cheshire Cat from Alice's adventures in wonderland.</p> <p>The cat replies are based on the Context provided below.</p> <p>Context of things the Human said in the past:</p> <p>- I am the Cheshire Cat (2 minutes ago)</p> <p>Context of documents containing relevant information:</p> <p>- I am the Cheshire Cat (extracted from cheshire-cat)</p> <p>If Context is not enough, you have access to the following tools:</p> <p>&gt; get_the_time: get_the_time(tool_input) - Retrieves current time and clock. Input is always None. &gt; Calculator: Useful for when you need to answer questions about math.</p> <p>To use a tool, please use the following format:</p> <p>'''</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: the action to take, should be one of [get_the_time, Calculator]</p> <p>Action Input: the input to the action</p> <p>Observation: the result of the action</p> <p>'''</p> <p>When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:</p> <p>'''</p> <p>Thought: Do I need to use a tool? No</p> <p>AI: [your response here]</p> <p>'''</p> <p>Conversation until now:</p> <p>- Human: Can you tell me what time is it?</p> <p>What would the AI reply?</p> <p>Answer concisely to the user needs as best you can, according to the provided recent conversation, context and tools.</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: get_the_time</p> <p>Action Input: None</p> <p>Observation: 2023-06-03 20:48:07.527033</p> <p>Cat's answer:</p> <p>The time is 2023-06-03 20:48:07.527033.</p>"},{"location":"technical/plugins/tools/#your-first-tool","title":"Your first Tool","text":"<p>A Tool is just a python function. In this example, we'll show how to create a tool to convert currencies. To keep it simple, we'll not rely on any third party library and we'll just assume a fixed rate of change.  </p>"},{"location":"technical/plugins/tools/#implementation_1","title":"Implementation","text":"<p>Let's convert EUR to USD. In your <code>mypluginfile.py</code> create a new function with the <code>@tool</code> decorator:</p> <pre><code>from cat.mad_hatter.decorators import tool\n@tool\ndef convert_currency(tool_input, cat): # (1)\n\"\"\"Useful to convert currencies. This tool converts euro (EUR) to dollars (USD).\n     Input is an integer or floating point number.\"\"\" # (2)\n# Define fixed rate of change\nrate_of_change = 1.07\n# Parse input\neur = float(tool_input) # (3)\n# Compute USD\nusd = eur * rate_of_change\nreturn usd\n</code></pre> <ol> <li> <p>Warning Always remember the two mandatory arguments</p> </li> <li>In the docstring we explicitly explain how the input should look like. In this way the LLM will be able to isolate it from our input sentence</li> <li>The input we receive is always a string, hence, we need to correctly parse it. In this case, we have to convert it to a floating number</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works_1","title":"How it works","text":"<p>User's input:</p> <p>Can you convert 10.5 euro to dollars?</p> <p>Cat's reasoning from the terminal:</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: convert_currency</p> <p>Action Input: 10.5</p> <p>Observation: 11.235000000000001</p> <p>Cat's answer:</p> <p>10.5 euros is equivalent to 11.235000000000001 dollars.</p> <p>Writing as tool is as simple as this. The core aspect to remember are: </p> <ol> <li>the two input arguments, i.e. the first is the string the LLM take from the chat and the Cat instance;</li> <li>the docstring from where the LLM understand how to use the tool and how the input should look like.</li> </ol>"},{"location":"technical/plugins/tools/#more-tools","title":"More tools","text":"<p>As seen, writing basic tools is as simple as writing pure Python functions. However, tools can be very flexible. Here are some examples.</p>"},{"location":"technical/plugins/tools/#return-the-output-directly","title":"Return the output directly","text":"<p>The <code>@tool</code> decorator accepts an optional boolean argument that is <code>@tool(return_direct=True)</code>. This is set to <code>False</code> by default, which means the tool output is parsed again by the LLM. Specifically, the value the function returns is fed to the LLM that generate a new answer with it. When set to <code>True</code>, the returned value is printed in the chat as-is.  </p>"},{"location":"technical/plugins/tools/#implementation_2","title":"Implementation","text":"<p>Let's give it a try with a modified version of the <code>convert_currency</code> tool:</p> <pre><code>from cat.mad_hatter.decorators import tool\n@tool(return_direct=True)\ndef convert_currency(tool_input, cat):\n\"\"\"Useful to convert currencies. This tool converts euro (EUR) to dollars (USD).\n     Input is an integer or floating point number.\"\"\"\n# Define fixed rate of change\nrate_of_change = 1.07\n# Parse input\neur = float(tool_input) # (3)\n# Compute USD\nusd = eur * rate_of_change\n# Format the output\ndirect_output = f\"Result of the conversion: {eur:.2f} EUR -&gt; {usd:.2f} USD\"\nreturn direct_output\n</code></pre>"},{"location":"technical/plugins/tools/#how-it-works_2","title":"How it works","text":"<p>User's input:</p> <p>Can you convert 10.5 euro to dollars?</p> <p>Cat's reasoning from the terminal: the reasoning is not displayed as the goal of the <code>return_direct=True</code> parameter is to skip those steps and return the output directly. </p> <p>Cat's answer:</p> <p>Result of the conversion: 10.50 EUR -&gt; 11.24 USD</p>"},{"location":"technical/plugins/tools/#complex-input-tools","title":"Complex input tools","text":"<p>This sections re-proposes an explanation of langchain multi-input tools. For example, we can make the <code>convert_currency</code> tool more flexible allowing the user to choose among a fixed set of currencies.</p>"},{"location":"technical/plugins/tools/#implementation_3","title":"Implementation","text":"<pre><code>from cat.mad_hatter.decorators import tool\n@tool\ndef convert_currency(tool_input, cat): # (1)\n\"\"\"Useful to convert currencies. This tool converts euro (EUR) to a fixed set of other currencies.\n    Choises are: US dollar (USD), English pounds (GBP) or Japanese Yen (JPY).\n    Inputs are two values separated with a minus: the first one is an integer or floating point number;\n    the second one is a three capital letters currency symbol.\"\"\" # (2)\n# Parse the input\neur, currency = tool_input.split(\"-\") # (3)\n# Define fixed rates of change\nrate_of_change = {\"USD\": 1.07,\n\"GBP\": 0.86,\n\"JPY\": 150.13}\n# Convert EUR to float\neur = float(eur)\n# Check currency exists in our list\nif currency in rate_of_change.keys():\n# Convert EUR to selected currency\nresult = eur * rate_of_change[currency]\nreturn result\n</code></pre> <ol> <li>The input to the function are always two</li> <li>Explain in detail how the inputs from the chat should look like. Here we want something like \"3.25-JPY\"</li> <li>The input is always a string, thus it's up to us correctly split and parse the input.</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works_3","title":"How it works","text":"<p>User's input:</p> <p>Can you convert 7.5 euros to GBP?</p> <p>Cat's reasoning from the terminal:</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: convert_currency</p> <p>Action Input: 7.5-GBP</p> <p>Observation: 6.45</p> <p>Cat's answer:</p> <p>7.5 euros is equal to 6.45 British Pounds.</p> <p>As you may see, the Agent correctly understands the desired output from the message and passes it to the tool function as explained in the docstring. Then, it is up to us parse the two inputs correctly for our tool.</p>"},{"location":"technical/plugins/tools/#external-library-the-cat-parameter","title":"External library &amp; the cat parameter","text":"<p>Tools are extremely flexible as they allow to exploit the whole Python ecosystem of packages. Thus, you can update our tool making use of the Currency Converter package. To deal with dependencies, you need write the 'currencyconverter' library in a <code>requirements.txt</code> inside the <code>myplugin</code> folder. Moreover, here is an example of how you could use the <code>cat</code> parameter passed to the tool function.</p>"},{"location":"technical/plugins/tools/#implementation_4","title":"Implementation","text":"<pre><code>from currency_converter import CurrencyConverter\nfrom cat.mad_hatter.decorators import tool\n@tool(return_direct=True)\ndef convert_currency(tool_input, cat):\n\"\"\"Useful to convert currencies. This tool converts euros (EUR) to another currency.\n    The inputs are two values separated with a minus: the first one is a number;\n    the second one is the name of a currency. Example input: '15-GBP'.\n    Use when the user says something like: 'convert 15 EUR to GBP'\"\"\"\n# Currency Converter\nconverter = CurrencyConverter(decimal=True)\n# Parse the input\nparsed_input = tool_input.split(\"-\")\n# Check input is correct\nif len(parsed_input) == 2:  # (1)\neur, currency = parsed_input[0].strip(\"'\"), parsed_input[1].strip(\"'\")\nelse:\nreturn \"Something went wrong using the tool\"\n# Ask the Cat to convert the currency name into its symbol\nsymbol = cat.llm(f\"You will be given a currency code, translate the input in the corresponding currency symbol. \\\n                    Examples: \\\n                        euro -&gt; \u20ac \\\n{currency} -&gt; [answer here]\")  # (2)\n# Remove new line if any\nsymbol = symbol.strip(\"\\n\")\n# Check the currencies are in the list of available ones\nif currency not in converter.currencies:\nreturn f\"{currency} is not available\"\n# Convert EUR to currency\nresult = converter.convert(float(eur), \"EUR\", currency)\nreturn f\"{eur}\u20ac = {float(result):.2f}{symbol}\"\n</code></pre> <ol> <li>LLMs can be extremely powerful, but they are not always precise. Hence, it's always better to have some checks when parsing the input.    A common scenario is that sometimes the Agent wraps the input around quotes and sometimes doesn't    E.g. Action Input: 7.5-GBP vs Action Input: '7.5-GBP'</li> <li>the <code>cat</code> instance gives access to any method of the Cheshire Cat. In this example, we directly call the LLM using one-shot example to get a currency symbol.</li> </ol>"},{"location":"technical/plugins/tools/#how-it-works_4","title":"How it works","text":"<p>The thoughts under the hood are identical to the previous example, as nothing changed in the underlying behavior, but we improved a little the quality of our tool code.</p> <p>Thought: Do I need to use a tool? Yes</p> <p>Action: convert_currency</p> <p>Action Input: 67-JPY</p> <p>Observation: 67\u20ac = 9846.99\u00a5;</p> <p>TODO:</p> <ul> <li>a better example?</li> <li>show how tools are displayed in the prompt and how the LLM selects them</li> <li>more examples with little variations<ul> <li>the tool calls an external service</li> <li>the tool reads/writes a file</li> <li>the input string contains a dictionary (to be parsed with <code>json.loads</code>)</li> <li>the tool manages a conversational form</li> <li>show how you can access cat's functionality (memory, llm, embedder, rabbit_hole) from inside a tool</li> <li>what else? dunno</li> </ul> </li> </ul>"},{"location":"technical/plugins/tools/#references","title":"References","text":"<ol> <li> <p>Schick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., ... &amp; Scialom, T. (2023). Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.\u00a0\u21a9</p> </li> </ol>"},{"location":"technical/tutorials/installation/","title":"Installation","text":""},{"location":"technical/tutorials/installation/#installation-customization","title":"\ud83e\uddd1 Installation &amp; Customization","text":"<p>Watch it on YouTube</p>"},{"location":"technical/tutorials/overview/","title":"&#128064; Overview","text":""},{"location":"technical/tutorials/overview/#overview","title":"\ud83d\udc40 Overview","text":"<p>THIS VIDEO PROCEDURE</p> <p>Watch it on YouTube</p>"}]}